% Section 6.1 – The Pumping Lemma for Context‑Free Languages
%
% This fragment uses the Wildcat Beamer macros provided in the user's template
% to produce a coherent sequence of frames covering Martin's Section 6.1.
% Each frame paraphrases one or more paragraphs of the textbook and uses
% tblock, talert, texample and other macros to structure the material for
% undergraduate students.  Labels increment sequentially for easy cross‑
% reference.  A figure from the original text (Figure 6.2) is included and
% stored in the project directory as fig‑6‑2.png.

\section{The Pumping Lemma for CFLs}

%-----------------------------------------------------------------------------
% Frame 1: Motivation – distinguishing finite and pushdown memory
\begin{frame}[t]{Motivation: finite vs. stack memory}
  \begin{tblock}{Core idea}
    Finite automata have a fixed number of states and cannot remember
    arbitrarily many symbols.  Pushdown automata extend this by adding a
    stack, enabling them to recognise languages like $a^n b^n$ because
    they can count the $a$’s and then match them with $b$’s.  Yet even
    a stack has limitations: it cannot easily compare three counts or
    match widely separated substrings.
  \end{tblock}
  \begin{tblock}{Proof idea}
    To show a language is not context‑free, we must prove that no
    pushdown automaton (or, equivalently, no context‑free grammar) can
    recognise it.  The pumping lemma for CFLs gives a necessary
    condition that all context‑free languages satisfy.  Violating this
    condition leads to a contradiction.
  \end{tblock}
  \begin{talert}{Pitfalls / Quantifiers}
    Do not assume that a single example of a long string failing to be
    recognised is enough; the lemma quantifies over \emph{all}
    sufficiently long strings and \emph{all} decompositions satisfying
    certain constraints.
  \end{talert}
  \note{Remind students that $a^n b^n$ is context‑free but not
    regular, whereas more complex patterns like $a^n b^n c^n$ and
    duplicated strings present challenges even for PDAs.}
  \label{fr:6.1-01}
\end{frame}

%-----------------------------------------------------------------------------
% Section 7.8 – Universal Turing Machines
%
% This section explains how to encode Turing machines and inputs as
% strings so that a single universal Turing machine can simulate any
% other Turing machine on any input.  It introduces a specific
% encoding scheme, works through an example, and outlines the design
% of a universal machine.

\section{Universal Turing Machines}

% Frame 1: Motivation for a universal machine
\begin{frame}[t]{Why universal machines?}
  \begin{tblock}{Core idea}
    A universal Turing machine (UTM) is a single machine capable of
    simulating any other Turing machine on any input when provided
    with an encoding of that machine and its input.  This concept
    illustrates the flexibility of TMs and underlies modern notions of
    stored‑program computers.
  \end{tblock}
  \begin{tblock}{Key observations}
    \begin{itemize}
      \item We can represent the description of a machine as a string
        over a fixed alphabet.
      \item A UTM takes two inputs: a code $e(T)$ for a TM $T$ and a
        code $e(z)$ for the input $z$ to $T$; it simulates $T$ on $z$.
      \item This shows that programs can be manipulated as data.
    \end{itemize}
  \end{tblock}
  \note{The notion of universality is fundamental to programmable
    computing devices and underpins undecidability proofs.}
  \label{fr:7.8-01}
\end{frame}

% Frame 2: Encoding conventions (Def 7.33)
\begin{frame}[t]{Encoding Turing machines}
  \begin{tblock}{Core idea}
    To feed a Turing machine description into another machine, we need
    to encode its components numerically.  Definition 7.33 assigns
    numbers to states, tape symbols and directions, then encodes each
    transition as a block of unary strings separated by zeros.
  \end{tblock}
  \begin{tblock}{Key steps}
    \begin{itemize}
      \item Fix an ordering of states $q_1,\dots,q_m$ and symbols
        $\gamma_1,\dots,\gamma_n$; assign directions L=1, S=2, R=3.
      \item Encode a move $(q,a)\mapsto (q',a',d)$ as the concatenation
        $0^{i}\,1\,0^{j}\,1\,0^{k}\,1\,0^{\ell}\,1\,0^{d}$ where
        indices $i,j,k,\ell$ correspond to the numbers of $q,q',a,a'$.
      \item Concatenate the codes of all transitions and terminate
        with “11” to mark the end; the resulting binary string is
        $e(T)$.
    \end{itemize}
  \end{tblock}
  \note{The specific encoding uses unary chunks to simplify parsing.
    Other injective encodings are possible; the choice is arbitrary so
    long as it is reversible.}
  \label{fr:7.8-02}
\end{frame}

% Frame 3: Encoding inputs and uniqueness
\begin{frame}[t]{Encoding inputs}
  \begin{tblock}{Core idea}
    Inputs to a machine are also encoded in unary so that they can be
    concatenated with the machine code.  Definition 7.33 denotes by
    $e(z)$ the code of an input string $z$ with respect to the same
    alphabet ordering used for $T$.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item Each symbol $\gamma_i$ in the input is encoded as $0^i 1$;
        the input string is the concatenation of these codes.
      \item The concatenation $e(T)\,e(z)$ serves as the input to the
        universal machine.  Different machines or inputs yield distinct
        codes.
      \item The encoding must be injective: no two distinct pairs
        $(T,z)$ map to the same string.  Decoding must also be
        effectively computable.
    \end{itemize}
  \end{tblock}
  \note{Emphasise that the universal machine needs to parse both
    components to begin the simulation.}
  \label{fr:7.8-03}
\end{frame}

% Frame 4: Example 7.34 – encoding a simple TM
\begin{frame}[t]{\texample{Encoding a simple machine}}
  \begin{tblock}{Setup}
    Consider a TM $T$ that, on input $a\,b^*$, overwrites the first
    $a$ with $b$ and halts.  We enumerate its states and symbols and
    encode its single transition according to the scheme.
  \end{tblock}
  \begin{tblock}{Computation of $e(T)$}
    \begin{itemize}
      \item Let $q_1=q_0$ be the start state and $q_2=h_a$ be the
        accept state; assign $q_1=1$, $q_2=2$.
      \item Let $\gamma_1=a$, $\gamma_2=b$, $\gamma_3=\Box$.
      \item The only move is $(q_1,a)\mapsto(q_2,b,S)$.  Encode this
        as $0^1 1 0^2 1 0^1 1 0^2 1 0^2$:
        $0 1$ (for $q_1$), $00 1$ ($q_2$), $0 1$ ($a$), $00 1$ ($b$),
        $00$ ($S$).  Append “11” to mark the end.
      \item Thus $e(T) = 0 1 00 1 0 1 00 1 00 11$.
    \end{itemize}
  \end{tblock}
  \note{Students should verify each segment corresponds to the
    appropriate indices.  Encourage them to encode more transitions.}
  \label{fr:7.8-04}
\end{frame}

% Frame 5: Figure 7.35 placeholder
\begin{frame}[t]{Visualising the encoding}
  \centering
  \includegraphics[width=.75\linewidth]{fig-7-35.png} % TODO: p. 255
  \begin{tblock}{Interpretation}
    Figure 7.35 shows how the unary encoding segments line up for the
    simple machine in Example 7.34.  Each group of zeros followed by
    a one represents an index in the enumeration of states or symbols.
  \end{tblock}
  \note{Use the figure to reinforce the mapping between machine
    components and their unary codes.}
  \label{fr:7.8-05}
\end{frame}

% Frame 6: Decoding a code
\begin{frame}[t]{Decoding an encoded machine}
  \begin{tblock}{Core idea}
    To simulate a machine given its encoding, the universal TM must be
    able to parse $e(T)$ and reconstruct $T$’s transition function.
    Decoding proceeds by reading unary blocks separated by ones and
    converting them back to indices of states, symbols and directions.
  \end{tblock}
  \begin{tblock}{Key steps}
    \begin{itemize}
      \item Read $0^i 1$ and interpret $i$ as the index of the current
        state $q_i$.
      \item Read $0^j 1$ to obtain the index of the next state; read
        $0^k 1$ and $0^\ell 1$ for the input and output symbols.
      \item Read $0^d$ (without a trailing 1) for the direction:
        $d=1$ corresponds to L, $d=2$ to S and $d=3$ to R.
      \item Repeat until the terminating “11” is encountered.
    \end{itemize}
  \end{tblock}
  \note{Decoding is mechanical and can itself be carried out by a
    Turing machine, highlighting the programmability of TMs.}
  \label{fr:7.8-06}
\end{frame}

% Frame 7: Building a universal TM
\begin{frame}[t]{Designing a universal machine}
  \begin{tblock}{Core idea}
    A universal TM $U$ takes an input of the form $e(T)\,e(z)$ and
    simulates $T$ on $z$.  $U$ must parse $e(T)$, store the transition
    table, maintain a simulation tape for $T$’s tape and follow $T$’s
    moves one by one.
  \end{tblock}
  \begin{tblock}{High‑level roadmap}
    \begin{enumerate}
      \item Read $e(T)$ from the input and decode each transition; store
        them in an internal table or on a second tape.
      \item Locate $e(z)$, decode it into an initial tape for $T$ and
        initialise $T$’s simulated head position and state.
      \item Repeat: given the current state and scanned symbol,
        consult the stored transition table to find $(q',a',d)$;
        rewrite the simulated tape accordingly; update the head
        position; update the simulated state.
      \item If the simulated state is $h_a$ or $h_r$, $U$ halts with
        the same outcome and outputs the simulated tape contents.
    \end{enumerate}
  \end{tblock}
  \note{The complexity of $U$ comes from decoding and lookup, but
    these are mechanical processes expressible as TM routines.}
  \label{fr:7.8-07}
\end{frame}

% Frame 8: Significance of universal machines
\begin{frame}[t]{Why universality matters}
  \begin{tblock}{Core idea}
    Universal Turing machines demonstrate that a single device can
    execute any algorithm when given a program and its input.  This
    principle underlies general‑purpose computers and leads to deep
    results such as undecidability and self‑reference.
  \end{tblock}
  \begin{tblock}{Key implications}
    \begin{itemize}
      \item The Halting Problem: given $e(T)$ and $e(z)$, decide whether
        $T$ halts on $z$; the universal machine enables the formal
        statement of this problem.
      \item Self‑replication: machines can take their own encoding as
        input, enabling diagonal arguments and fixed‑point theorems.
      \item Programming languages: high‑level languages compile to
        machine code, which is ultimately executed by a universal
        machine.
    \end{itemize}
  \end{tblock}
  \note{This frame connects universality to broader themes in
    theoretical and practical computer science.}
  \label{fr:7.8-08}
\end{frame}

% Frame 9: Flexibility of encodings
\begin{frame}[t]{Encodings are not unique}
  \begin{tblock}{Core idea}
    The specific encoding described in Definition 7.33 is one of many
    possible ways to represent Turing machines and inputs.  Any
    injective, effectively decodable encoding suffices for constructing
    a universal machine.
  \end{tblock}
  \begin{tblock}{Discussion}
    \begin{itemize}
      \item One could use binary numerals, Gödel numbering or other
        schemes; the essential requirement is that each machine and
        input has a unique code that can be decoded.
      \item The universal machine must be modified to match the
        encoding; once decoding is done, simulation proceeds as
        described.
      \item When proving undecidability results, the choice of
        encoding affects details but not the overall argument.
    \end{itemize}
  \end{tblock}
  \note{Highlight that the power of universality lies in the
    existence of an encoding, not in any particular one.}
  \label{fr:7.8-09}
\end{frame}

% Frame 10: Exercise – decode an encoding
\begin{frame}[t]{Exercise: decoding practice}
  \begin{tblock}{Task}
    Given the code $0 1\,0 1\,0 1\,0 1\,0 1\,11$, assume the
    enumeration assigns the single state $q_1$ and the symbol $\gamma_1$.
    Decode this code according to Definition 7.33.  What Turing
    machine does it represent?
  \end{tblock}
  \begin{talert}{Solution}
    Each $0 1$ represents index 1.  Reading five such pairs yields
    $(q_1,\gamma_1)\mapsto(q_1,\gamma_1,L)$: the machine stays in its
    only state, rewrites nothing and moves left forever.  The
    terminating “11” ends the description.
  \end{talert}
  \note{This exercise helps students practise decoding small codes and
    understanding the resulting machine behaviour.}
  \label{fr:7.8-10}
\end{frame}

% Frame 11: Exercise – encode a given machine
\begin{frame}[t]{Exercise: encoding practice}
  \begin{tblock}{Task}
    Let $T$ be a TM with states $\{q_0,q_1,h_a,h_r\}$ and symbols
    $\{a,b,\Box\}$ with transitions: $(q_0,a)\mapsto(q_1,b,R)$,
    $(q_1,b)\mapsto(h_a,b,S)$ and all other moves undefined.  Using
    the ordering $q_0=1,q_1=2,h_a=3,h_r=4$ and $a=1,b=2,\Box=3$, write
    down $e(T)$.
  \end{tblock}
  \begin{talert}{Hint}
    Encode each move as in Definition 7.33 and concatenate them,
    appending “11” at the end.  Remember to encode the direction
    symbols: R=3, S=2.
  \end{talert}
  \note{This exercise reinforces the mechanics of the encoding
    scheme.}
  \label{fr:7.8-11}
\end{frame}

% Frame 12: Exercise – designing a universal machine sketch
\begin{frame}[t]{Exercise: sketch a universal TM}
  \begin{tblock}{Task}
    Outline how you would modify the four‑tape simulation used for
    nondeterministic machines to build a universal deterministic TM
    that, given $e(T)$ and $e(z)$, simulates $T$ on $z$.  Identify the
    roles of the tapes.
  \end{tblock}
  \begin{talert}{Hint}
    Use one tape for the input code, one to store the decoded
    transition table, one for $T$’s simulated tape and one for
    bookkeeping (current state and position).  The universal machine
    repeatedly looks up the next transition and updates the simulated
    tape accordingly.
  \end{talert}
  \note{This exercise connects the ideas of encoding and simulation.}
  \label{fr:7.8-12}
\end{frame}

% Frame 13: Summary of Section 7.8
\begin{frame}[t]{Summary of Section 7.8}
  \begin{tblock}{Core idea}
    Universal Turing machines demonstrate that the description of a
    machine can itself be encoded as data and processed by another
    machine.  By fixing an encoding of states, symbols and directions,
    every TM and its input can be represented as a binary string.  A
    universal machine parses this code and faithfully simulates the
    original machine.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item Definition 7.33 introduces a concrete encoding scheme using
        unary blocks separated by ones.
      \item Example 7.34 illustrates encoding a simple machine; Figure 7.35
        visualises the segments.
      \item Decoding transforms the code back into transition rules,
        enabling simulation.
      \item A universal TM executes any encoded machine on any encoded
        input, establishing the programmability of TMs and paving the
        way for undecidability results.
    \end{itemize}
  \end{tblock}
  \note{Encourage students to reflect on how universality leads to
    self‑reference and the Halting Problem, topics explored in the
    next chapter.}
  \label{fr:7.8-13}
\end{frame}

% Frame 14: Importance of injective and decodable encodings
\begin{frame}[t]{Encoding requirements}
  \begin{tblock}{Core idea}
    For a universal Turing machine to function correctly, the encoding
    of machines and inputs must satisfy two essential properties:
    injectivity and effective decodability.  Without these, different
    machines might share the same code or the universal machine might
    fail to reconstruct the original machine.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item \textbf{Injectivity}: The mapping $(T,z)\mapsto e(T)\,e(z)$ must
        be one‑to‑one.  Otherwise, the universal machine could not
        determine which machine and input were intended.
      \item \textbf{Decodability}: There must exist a TM that, given
        $e(T)\,e(z)$, recovers $T$ and $z$.  This ensures that $U$ can
        parse its input and simulate the correct machine on the correct
        input.
      \item Any reasonable encoding used in computability theory
        satisfies these properties; differences lie in convenience and
        length of the codes.
    \end{itemize}
  \end{tblock}
  \note{Emphasise that these properties are essential when designing
    self‑reference arguments such as the proof of the Halting Problem.}
  \label{fr:7.8-14}
\end{frame}

%-----------------------------------------------------------------------------
% Section 7.7 – Nondeterministic Turing Machines
%
% This section introduces Turing machines whose transition function
% allows multiple possible moves.  Acceptance is defined if any branch
% leads to the accept state.  Nondeterminism does not add power to
% Turing machines for recognising languages, but it can simplify
% designs.  A standard simulation uses breadth‑first search with
% multiple tapes.

\section{Nondeterministic Turing Machines}

% Frame 1: Definition of an NTM
\begin{frame}[t]{What is a nondeterministic TM?}
  \begin{tblock}{Core idea}
    A nondeterministic Turing machine (NTM) generalises the TM
    transition function to allow multiple possible moves from a single
    state and scanned symbol.  Formally, $\delta(q,a)$ is a finite set
    of triples $(q',a',d)$ instead of a single triple.  In one step the
    machine may nondeterministically choose any of these moves.
  \end{tblock}
  \begin{tblock}{Acceptance semantics}
    \begin{itemize}
      \item An NTM accepts an input $w$ if at least one computation
        path starting from $q_0\Box w$ reaches the accept state $h_a$.
      \item It rejects $w$ if 
        ​
        no path leads to $h_a$ and at least one path leads to $h_r$.
      \item It loops on $w$ if every path either loops or crashes,
        never reaching $h_a$ or $h_r$.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls}
    Acceptance is existential: only one accepting branch is needed.
    Rejection is universal: every branch must either reject or loop
    without acceptance.  Looping branches do not invalidate acceptance.
  \end{talert}
  \note{Draw parallels to nondeterministic finite automata.  Here the
    branching happens on a TM with an unbounded tape.}
  \label{fr:7.7-01}
\end{frame}

% Frame 2: Visualising computation trees
\begin{frame}[t]{Computation trees and paths}
  \begin{tblock}{Core idea}
    The set of possible computations of an NTM on input $w$ forms a
    tree.  Each node is a configuration, and each child corresponds
    to one possible nondeterministic move.  Acceptance corresponds to
    the existence of a path from the root to a leaf containing $h_a$.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item The tree may be infinite if some branches loop.
      \item Branching can happen at every step; the number of paths
        grows exponentially in the worst case.
      \item We do not require exploring all branches to accept; it
        suffices to find one accepting path.
    \end{itemize}
  \end{tblock}
  \note{Encourage students to sketch simple computation trees for
    small NTMs to build intuition about branching.}
  \label{fr:7.7-02}
\end{frame}

% Frame 3: Example – recognising composite numbers
\begin{frame}[t]{\texample{Recognising composite numbers}}
  \begin{tblock}{Core idea}
    Consider the language $L = \{1^n \mid n \text{ is composite}\}$.
    An NTM can recognise $L$ by nondeterministically guessing two
    integers $p,q \ge 2$ such that $p \times q = n$ and verifying the
    product.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item On input $1^n$, guess $p$ by placing a marker after the
        $p$‑th $1$ on the tape; ensure $p \ge 2$ by forcing at least
        two $1$’s before the marker.
      \item Guess $q$ similarly, marking another position; ensure
        $q \ge 2$.
      \item Multiply $p$ and $q$ by repeated addition on a work tape
        and compare the result to $n$; if equal, accept; otherwise
        reject.
    \end{enumerate}
  \end{tblock}
  \note{This NTM may have many branches, each corresponding to a
    different guess for $p$ and $q$.  Only one correct factorisation
    needs to succeed.}
  \label{fr:7.7-03}
\end{frame}

% Frame 4: Details of the composite recogniser
\begin{frame}[t]{Composite recogniser details}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Use two tapes: one holds the input $1^n$, and the other
        holds a running tally of the product $p \times q$.
      \item The machine nondeterministically selects cut points on
        tape 1 to mark $p$ and $q$.  If the cuts leave fewer than two
        $1$’s in a segment, reject on that branch.
      \item Implement multiplication by adding $p$ to an accumulator
        $q$ times; compare the accumulator to the original input.
      \item Acceptance is reached if the product equals the input; all
        other branches reject.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls}
    Ensure that the machine does not accidentally accept $1^1$ or
        $1^0$ (which are not composite) by forcing $p,q\ge 2$.
  \end{talert}
  \note{Although slower than a deterministic primality test, this
    NTM illustrates how nondeterminism guesses witnesses that are
    expensive to find deterministically.}
  \label{fr:7.7-04}
\end{frame}

% Frame 5: Example – prefix‑closure and generation
\begin{frame}[t]{\texample{Prefix closure of a language}}
  \begin{tblock}{Core idea}
    Let $G$ be a language generated by some machine.  The prefix
    closure of $G$ consists of all prefixes of strings in $G$.  An
    NTM can recognise the prefix closure by nondeterministically
    generating a string in $G$ and checking whether the input is a
    prefix of that string.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Nondeterministically choose a continuation string $y$ and
        simulate the generator $M$ on $xy$ (where $x$ is the input).
      \item If $M$ accepts $xy$, then $x$ is a prefix of some string
        in $G$, so accept.
      \item If no extension $y$ leads to acceptance, the machine
        rejects.
    \end{enumerate}
  \end{tblock}
  \note{This example shows that nondeterminism can guess the
    necessary completion to witness membership in a larger language.}
  \label{fr:7.7-05}
\end{frame}

% Frame 6: Theorem 7.31 – power equivalence
\begin{frame}[t]{Theorem 7.31: NTM vs DTM}
  \begin{tblock}{Statement}
    Every language recognised by a nondeterministic Turing machine is
    also recognised by some deterministic Turing machine.  Thus, NTMs
    do not recognise any languages beyond those recognised by DTMs.
  \end{tblock}
  \begin{tblock}{Implications}
    \begin{itemize}
      \item Nondeterminism is a convenience for designing machines,
        not a source of additional power for language recognition.
      \item Acceptance by an NTM can be simulated deterministically by
        systematically exploring its computation tree.
    \end{itemize}
  \end{tblock}
  \note{This result is analogous to the equivalence of deterministic
    and nondeterministic finite automata for regular languages, but the
    simulation for TMs is more involved.}
  \label{fr:7.7-06}
\end{frame}

% Frame 7: Proof idea – breadth‑first search
\begin{frame}[t]{Proof idea: breadth‑first simulation}
  \begin{tblock}{Core idea}
    To simulate an NTM deterministically, perform a breadth‑first
    search (BFS) of its computation tree.  Level by level, generate
    all possible nondeterministic choices up to that depth and check
    whether any path has reached $h_a$.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Maintain a queue of descriptions of partial computation
        paths (choice strings).
      \item For the next choice string in the queue, simulate the
        corresponding path of the NTM up to the length of the choice.
      \item If the simulation reaches $h_a$, accept; otherwise, if
        further nondeterministic choices are possible, extend the
        choice string and enqueue the extensions.
      \item Repeat until acceptance or until all paths have been
        explored (in which case reject).
    \end{enumerate}
  \end{tblock}
  \note{BFS ensures that if an accepting path exists at any finite
    depth, it will eventually be found, avoiding infinite descent down
    a single branch.}
  \label{fr:7.7-07}
\end{frame}

% Frame 8: Four‑tape simulation of an NTM
\begin{frame}[t]{Four‑tape deterministic simulation}
  \begin{tblock}{Core idea}
    One concrete simulation uses four tapes: tape 1 holds the input,
    tape 2 holds the choice string (encoding the sequence of
    nondeterministic choices), tape 3 is a work tape to simulate the
    NTM’s moves, and tape 4 stores a counter to bound the simulation
    depth.  The simulation enumerates all choice strings in
    lexicographic order.
  \end{tblock}
  \begin{tblock}{Key steps}
    \begin{itemize}
      \item Initialise tape 2 to the empty choice string and tape 4 to 1.
      \item Repeat: simulate the NTM using the current choice string
        for up to the length stored on tape 4; if acceptance occurs,
        halt.  Otherwise, increment the choice string, increase tape 4
        appropriately and continue.
      \item If the NTM has no more choices at some point, backtrack by
        incrementing the choice string as needed.
    \end{itemize}
  \end{tblock}
  \note{This simulation may be extremely slow, but it shows that NTM
    languages are deterministic TM languages.}
  \label{fr:7.7-08}
\end{frame}

% Frame 9: Efficiency caveats
\begin{frame}[t]{Efficiency and practicality}
  \begin{tblock}{Core idea}
    Although nondeterministic Turing machines do not add expressive
    power, their deterministic simulations can be exponentially slower.
    Nondeterminism remains valuable as a conceptual tool for describing
    algorithms succinctly.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item The BFS simulation explores all branches in increasing
        depth, which may require exponential time.
      \item In complexity theory, the class $\mathsf{NP}$ captures
        languages recognised by polynomial‑time NTMs; whether these
        languages can be recognised in polynomial time by DTMs is the
        famous $\mathsf{P}$ vs $\mathsf{NP}$ question.
      \item For computability, however, this overhead is irrelevant:
        both models recognise the same languages.
    \end{itemize}
  \end{tblock}
  \note{Distinguish between computability equivalence and complexity
    differences.}
  \label{fr:7.7-09}
\end{frame}

% Frame 10: Pitfalls in understanding NTMs
\begin{frame}[t]{Pitfalls and misconceptions}
  \begin{tblock}{Core idea}
    It is tempting to think of an NTM as “magically” guessing the
    correct path.  In reality, nondeterminism is a mathematical
    abstraction: acceptance means that some path exists, not that the
    machine can find it efficiently.
  \end{tblock}
  \begin{tblock}{Clarifications}
    \begin{itemize}
      \item The NTM does not explore paths sequentially; rather, the
        acceptance definition quantifies over all paths.
      \item A single looping path does not cause rejection if another
        path accepts.  Looping branches can be ignored for acceptance.
      \item Designing NTMs requires thinking about existential and
        universal quantifiers over branches.
    \end{itemize}
  \end{tblock}
  \note{This frame aims to correct intuitive but incorrect notions
    about “nondeterministic magic”.}
  \label{fr:7.7-10}
\end{frame}

% Frame 11: Exercise – find an accepting branch
\begin{frame}[t]{Exercise: accepting branch search}
  \begin{tblock}{Task}
    Design an NTM that recognises strings over $\{a,b\}$ that contain
    the substring $aba$.  Describe the nondeterministic choices and
    explain why the machine accepts exactly those strings.
  \end{tblock}
  \begin{talert}{Hint}
    Nondeterministically guess a position in the input as the start of
    the substring and check whether the next three symbols are $a$,
    $b$ and $a$.  Acceptance is existential: any matching position
    suffices.
  \end{talert}
  \note{This exercise shows how nondeterminism can guess witnesses
    quickly.}
  \label{fr:7.7-11}
\end{frame}

% Frame 12: Exercise – identifying looping branches
\begin{frame}[t]{Exercise: looping behaviour}
  \begin{tblock}{Task}
    Suppose an NTM has two possible moves in state $q$ on symbol $a$:
    one branch rewrites $a$ to $b$ and moves right into $q$, and the
    other rewrites $a$ to $a$ and moves left into $q$.  Does this NTM
    accept any input?  What happens on input $aaa$?  Discuss the
    behaviour of each branch.
  \end{tblock}
  \begin{talert}{Hint}
    One branch moves right forever on the rightmost blank and loops;
    the other branch moves left until it crashes on the left edge.  No
    accept state is ever reached.
  \end{talert}
  \note{This exercise highlights that looping branches do not lead to
    acceptance and that acceptance requires an explicit accepting
    branch.}
  \label{fr:7.7-12}
\end{frame}

% Frame 13: Exercise – deterministic simulation sketch
\begin{frame}[t]{Exercise: simulate a simple NTM}
  \begin{tblock}{Task}
    Given an NTM that, on input $w$, nondeterministically deletes
    either the first or last symbol of $w$ and accepts if the result
    is $ab$, sketch how a deterministic TM would simulate this NTM
    using breadth‑first search.  Outline how the simulation keeps
    track of the different deletion choices.
  \end{tblock}
  \begin{talert}{Hint}
    Represent choices as binary strings: 0 means delete the first
    symbol; 1 means delete the last symbol.  Enumerate choice strings
    and apply deletions accordingly until $ab$ is produced or all
    possibilities are exhausted.
  \end{talert}
  \note{This exercise connects the BFS simulation to a concrete
    example.}
  \label{fr:7.7-13}
\end{frame}

% Frame 14: Exercise – language equality
\begin{frame}[t]{Exercise: comparing DTMs and NTMs}
  \begin{tblock}{Task}
    Let $L$ be a language recognised by some NTM in which every
    computation path halts.  Show that $L$ is decidable by a DTM
    without the need for breadth‑first search.  Outline the argument.
  \end{tblock}
  \begin{talert}{Hint}
    Because every branch halts, one can simulate the NTM by exploring
    branches depth‑first: there is no risk of an infinite descent into
    a looping branch.  Use this to build a decider.
  \end{talert}
  \note{This exercise illustrates how halting behaviour affects the
    choice of simulation strategy.}
  \label{fr:7.7-14}
\end{frame}

% Frame 16: Simulation strategies – BFS vs DFS
\begin{frame}[t]{Simulation strategies: BFS vs DFS}
  \begin{tblock}{Core idea}
    When simulating an NTM deterministically, one may choose between
    breadth‑first search (BFS) and depth‑first search (DFS) of the
    computation tree.  BFS avoids getting stuck in an infinite branch
    by exploring all nodes at a given depth before descending deeper.
    DFS may be simpler but requires care to avoid infinite descent.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item BFS guarantees that if there is an accepting path of
        finite length, it will eventually be found.  It is the
        standard method used in Theorem 7.31.
      \item DFS can be used if every branch of the NTM halts (no
        infinite paths).  In that case, exploring one branch to the end
        before backtracking will still find acceptance if it exists.
      \item If some branches loop, DFS may never return from an
        infinite descent and hence fail to discover accepting branches.
    \end{itemize}
  \end{tblock}
  \note{Encourage students to consider the structure of the NTM when
    choosing a simulation strategy.}
  \label{fr:7.7-16}
\end{frame}

% Frame 17: Example – guessing equal numbers of $a$ and $b$
\begin{frame}[t]{\texample{Equal numbers of $a$ and $b$}}
  \begin{tblock}{Core idea}
    The language $\{ a^n b^n \mid n \ge 0 \}$ can be recognised by an
    NTM that guesses a split point between the $a$’s and $b$’s and
    verifies that all symbols before the split are $a$ and all symbols
    after are $b$ with equal lengths.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item On input $w$, nondeterministically choose a cut point
        between two positions.
      \item Check that all symbols to the left of the cut are $a$ and
        all symbols to the right are $b$.
      \item Verify that the number of $a$’s equals the number of $b$’s
        by erasing one from each side in tandem; accept if both sides
        empty simultaneously.
    \end{enumerate}
  \end{tblock}
  \begin{talert}{Discussion}
    Although $a^n b^n$ is deterministic context‑free, this NTM
    exemplifies nondeterminism: it guesses the split instead of
    counting deterministically.
  \end{talert}
  \note{This example shows how nondeterminism can simplify the design
    of machines for context‑free languages.}
  \label{fr:7.7-17}
\end{frame}

% Frame 18: Quick check – interpreting acceptance semantics
\begin{frame}[t]{Exercise: acceptance semantics}
  \begin{tblock}{Task}
    Consider an NTM $N$ with two possible moves from its start state
    on input $w$: one branch immediately enters $h_a$, and the other
    branch loops forever.  According to the definition of acceptance,
    does $N$ accept $w$?  What if both branches loop forever?
  \end{tblock}
  \begin{talert}{Solution}
    \begin{itemize}
      \item In the first scenario, $N$ ​
        does accept $w$ because there exists at least
        one path that reaches $h_a$; the looping branch does not
        matter.
      \item If both branches loop, no path reaches $h_a$ or $h_r$;
        thus, $N$ loops on $w$ and neither accepts nor rejects.
    \end{itemize}
  \end{talert}
  \note{This quick check reinforces the existential nature of
    nondeterministic acceptance.}
  \label{fr:7.7-18}
\end{frame}

% Frame 15: Summary of Section 7.7
\begin{frame}[t]{Summary of Section 7.7}
  \begin{tblock}{Core idea}
    Nondeterministic Turing machines expand the transition relation to
    allow multiple possible moves.  Acceptance is defined by the
    existence of an accepting path.  Although NTMs do not recognise
    more languages than deterministic TMs, they provide a powerful
    conceptual tool for expressing algorithms succinctly.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item Definition of NTMs and acceptance semantics via
        computation trees.
      \item Examples: recognising composite numbers and prefix closures.
      \item Theorem 7.31 and its proof idea using breadth‑first
        simulation on multiple tapes.
      \item Efficiency caveats: nondeterministic and deterministic
        computations may differ dramatically in running time.
      \item Exercises reinforce understanding of branches, loops and
        deterministic simulation.
    \end{itemize}
  \end{tblock}
  \note{Encourage students to appreciate nondeterminism as a design
    convenience while recognising its limitations in terms of power.}
  \label{fr:7.7-15}
\end{frame}

%-----------------------------------------------------------------------------
% Section 7.6 – The Church–Turing Thesis
%
% This section discusses the informal hypothesis that the Turing
% machine model captures all effectively computable functions.  It is
% supported by several lines of evidence but cannot be proved as a
% mathematical theorem.

\section{The Church–Turing Thesis}

% Frame 1: Statement of the thesis
\begin{frame}[t]{The Church–Turing thesis}
  \begin{tblock}{Core idea}
    The Church–Turing thesis asserts that every effectively computable
    function (i.e., every function that can be computed by a real
    algorithm) is computable by a Turing machine.  In other words,
    Turing machines capture the intuitive notion of an algorithm.
  \end{tblock}
  \begin{talert}{Important}
    The thesis is not a formal theorem; it is a philosophical claim
    grounded in the convergence of multiple models of computation and
    the absence of counterexamples.  It cannot be proved within
    mathematics alone.
  \end{talert}
  \note{Emphasise that “effectively computable” is an informal notion;
    the thesis bridges intuitive algorithms and formal models.}
  \label{fr:7.6-01}
\end{frame}

% Frame 2: Evidence (1) – Modelling human computation
\begin{frame}[t]{Evidence: modelling human computers}
  \begin{tblock}{Core idea}
    Turing’s model was designed to reflect the way a human computes
    with pencil and paper: the person follows a finite set of rules,
    manipulates symbols one at a time and has unlimited paper for
    scratch work.  No extra insights are needed beyond what a
    Turing machine can do.
  \end{tblock}
  \begin{tblock}{Key observations}
    \begin{itemize}
      \item Human computers perform primitive operations analogous to
        examining, rewriting and moving, just as Turing machines do.
      \item The finite control captures the finite memory of the
        person; the tape models unbounded scratch space.
      \item Any effectively described mechanical procedure can be
        translated into TM instructions.
    \end{itemize}
  \end{tblock}
  \note{This argument anchors the thesis in the historical context
    from which Turing developed his model.}
  \label{fr:7.6-02}
\end{frame}

% Frame 3: Evidence (2) – Robustness to model variations
\begin{frame}[t]{Evidence: robustness to enhancements}
  \begin{tblock}{Core idea}
    Many seemingly stronger models of computation – multi‑tape TMs,
    TMs with two‑dimensional tapes, register machines – have been
    shown to be equivalent to the basic single‑tape Turing machine.
    Adding extra tapes or more complex memory structures does not
    enable the computation of functions beyond those computable by a
    standard TM.
  \end{tblock}
  \begin{tblock}{Key examples}
    \begin{itemize}
      \item Theorem 7.26 shows that multi‑tape machines can be
        simulated by single‑tape machines.
      \item Random‑access machines, λ‑calculus and partial recursive
        functions all coincide with Turing computability.
      \item Modern digital computers, with finite memory, can be
        simulated by TMs augmented with unbounded memory.
    \end{itemize}
  \end{tblock}
  \note{This robustness suggests that the notion of computability is
    insensitive to the particular details of the machine model.}
  \label{fr:7.6-03}
\end{frame}

% Frame 4: Evidence (3) – Equivalence of independent models
\begin{frame}[t]{Evidence: convergence of models}
  \begin{tblock}{Core idea}
    Several independently proposed models of computation – Turing
    machines (Turing), λ‑calculus (Church), μ‑recursive functions
    (Kleene), Post systems and others – all define the same class of
    functions.  Their equivalence supports the idea that they capture
    the intuitive notion of computability.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item Church’s thesis originally proposed that λ‑definable
        functions coincide with effectively calculable functions; Turing
        showed that his machines compute the same functions.
      \item The coincidence of these diverse formalisms lends
        credibility to the thesis.
      \item No proposed model has yet been shown to compute a
        function beyond those computed by TMs without being
        equivalent in power.
    \end{itemize}
  \end{tblock}
  \note{Highlight the historical collaboration and mutual
    reinforcement between Church and Turing.}
  \label{fr:7.6-04}
\end{frame}

% Frame 5: Evidence (4) – Absence of counterexamples
\begin{frame}[t]{Evidence: no known counterexamples}
  \begin{tblock}{Core idea}
    Despite extensive study of algorithms and computation for over
    eighty years, no function that is intuitively computable has
    been found that cannot be computed by a Turing machine.  Attempts
    to formalise new models have always led to equivalence with TMs.
  \end{tblock}
  \begin{tblock}{Discussion}
    \begin{itemize}
      \item Physical analogues such as quantum or analog computers have
        not yet produced a clear counterexample; known quantum
        algorithms still fall within Turing computability.
      \item Hypercomputation proposals (e.g., machines performing
        supertasks) remain speculative and lack physical realisability.
      \item This absence of counterexamples bolsters confidence in
        the thesis, though it cannot provide a proof.
    \end{itemize}
  \end{tblock}
  \note{Mention that the thesis speaks about what is computable in
    principle, not about efficiency or feasibility.}
  \label{fr:7.6-05}
\end{frame}

% Frame 6: Uses of the thesis
\begin{frame}[t]{Applications of the thesis}
  \begin{tblock}{Core idea}
    The Church–Turing thesis allows computer scientists to use Turing
    machines as a yardstick for algorithmic computability without
    specifying low‑level details.  High‑level descriptions of
    algorithms are accepted as valid, provided they can be simulated
    by a TM.
  \end{tblock}
  \begin{tblock}{Key uses}
    \begin{itemize}
      \item Defining the notion of an algorithm: an “effective
        procedure” is one that a TM can carry out.
      \item Justifying high‑level language descriptions in proofs:
        instead of constructing explicit TMs, one sketches an algorithm
        and appeals to the thesis for its realisability.
      \item Guiding research: the thesis suggests that new models of
        computation should be compared to TMs to determine whether
        they truly add power.
    \end{itemize}
  \end{tblock}
  \note{This pragmatic view underpins much of theoretical computer
    science; it also allows us to work with more natural models
    without constantly reverting to TMs.}
  \label{fr:7.6-06}
\end{frame}

% Frame 7: Pitfalls and misconceptions
\begin{frame}[t]{Pitfalls and misconceptions}
  \begin{tblock}{Core idea}
    The Church–Turing thesis is sometimes misinterpreted as a claim
    about efficiency or as a mathematical theorem.  It is neither.  It
    does not assert that Turing machines are the most efficient model
    of computation, nor does it rule out physical processes beyond
    our current understanding.
  \end{tblock}
  \begin{tblock}{Clarifications}
    \begin{itemize}
      \item The thesis concerns what functions are computable, not how
        quickly they can be computed; complexity theory addresses
        efficiency.
      \item It is a hypothesis about the nature of computation in our
        universe; new physics could conceivably invalidate it.
      \item Saying “by the Church–Turing thesis this is computable” is
        shorthand for providing a convincing argument that the
        algorithm can be implemented mechanically.
    \end{itemize}
  \end{tblock}
  \note{Ensure students understand the scope of the thesis and avoid
    overstating its conclusions.}
  \label{fr:7.6-07}
\end{frame}

% Frame 8: Exercise – informal algorithm to TM
\begin{frame}[t]{Exercise: from algorithm to TM}
  \begin{tblock}{Task}
    Consider the following informal algorithm on input $n$: “If $n$ is
    even, output $n/2$; otherwise output $(3n+1)/2$.”  Explain why,
    under the Church–Turing thesis, there exists a Turing machine that
    computes this function.  Sketch how you would encode $n$ and
    perform the arithmetic on a TM.
  \end{tblock}
  \begin{talert}{Hint}
    Use unary or binary encoding of $n$ and design subroutines for
    addition, multiplication and division by two.  The thesis tells us
    that any clearly defined mechanical procedure can be implemented.
  \end{talert}
  \note{This exercise links the thesis to practical algorithm design.}
  \label{fr:7.6-08}
\end{frame}

% Frame 9: Summary of Section 7.6
\begin{frame}[t]{Summary of Section 7.6}
  \begin{tblock}{Core idea}
    The Church–Turing thesis posits that Turing machines capture all
    effective computation.  Although not formally provable, it is
    supported by multiple converging lines of evidence: modelling of
    human computation, robustness to enhancements, equivalence of
    independent models and absence of counterexamples.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item Statement of the thesis and its status as a philosophical
        claim.
      \item Evidence from human computation and equivalent models.
      \item Practical uses of the thesis in defining algorithms and
        simplifying proofs.
      \item Clarifications and misconceptions: efficiency vs
        computability, physical realisability.
    \end{itemize}
  \end{tblock}
  \note{Encourage students to think critically about the thesis and
    recognise its foundational role in computability theory.}
  \label{fr:7.6-09}
\end{frame}

% Frame 14: Complexity overhead of the simulation
\begin{frame}[t]{Complexity considerations}
  \begin{tblock}{Core idea}
    While multi‑tape TMs do not compute more functions than single‑tape
    TMs, they can be significantly faster.  The simulation of a
    two‑tape machine by a single‑tape machine introduces at least a
    quadratic slowdown due to repeated scanning of the tape.
  \end{tblock}
  \begin{tblock}{Discussion}
    \begin{itemize}
      \item Each simulated move scans the entire encoded tape to
        locate primed symbols and update them.  If the original
        machine runs in $T(n)$ steps, the simulator may run in
        $O(T(n)^2)$ time.
      \item In complexity theory, multi‑tape models form the basis for
        definitions of time‑bounded complexity classes (e.g., $\mathsf{P}$).
      \item For now, we focus on computability: time overhead does not
        affect whether a function is computable, only how efficiently it
        can be computed.
    \end{itemize}
  \end{tblock}
  \note{This frame hints at the later study of complexity while
    reassuring students that the simulation preserves computability.}
  \label{fr:7.5-14}
\end{frame}

%-----------------------------------------------------------------------------
% Section 7.5 – Multitape Turing Machines
%
% This section introduces Turing machines with multiple tapes and shows
% that they do not add computational power beyond the single‑tape
% model.  The simulation of a two‑tape machine by a single‑tape machine
% is described in detail, and the complexity overhead is noted.

\section{Multitape Turing Machines}

% Frame 1: Definition of a 2‑tape TM
\begin{frame}[t]{Two‑tape Turing machines}
  \begin{tblock}{Core idea}
    A two‑tape Turing machine has two independent tapes, each with its
    own head.  In each move the machine reads the symbols under both
    heads, writes new symbols and moves each head independently.  A
    configuration is a triple $(q,x_1 a_1 y_1,x_2 a_2 y_2)$ where
    $q\in Q$, $x_i$ and $y_i$ are the left and right parts of tape $i$,
    and $a_i$ is the scanned symbol on tape $i$.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item The transition function $\delta$ maps $(q,a_1,a_2)$ to
        $(q',a'_1,a'_2,d_1,d_2)$, where $d_i \in \{L,R,S\}$ specifies the
        movement of head $i$.
      \item The machine halts when $q'$ is $h_a$ or $h_r$.
      \item Blank symbols on each tape are handled independently;
        crashing occurs if a head moves left of its leftmost square.
    \end{itemize}
  \end{tblock}
  \note{Emphasise that the tapes share the same finite control; the
    machine’s behaviour depends on both scanned symbols simultaneously.}
  \label{fr:7.5-01}
\end{frame}

% Frame 2: Why consider multiple tapes?
\begin{frame}[t]{Motivation for multitape machines}
  \begin{tblock}{Core idea}
    Multiple tapes simplify algorithms by providing extra work space
    without the need to move back and forth on a single tape.  For
    example, copying a string or implementing two counters can be done
    more directly on separate tapes.
  \end{tblock}
  \begin{tblock}{Comparison to single tape}
    \begin{itemize}
      \item On a single tape, copying requires many passes to move the
        head between the source and target regions; on two tapes, one
        head can read while the other writes.
      \item Additional tapes do not increase the class of languages
        recognised or functions computed; they only reduce the
        overhead of moving the head.
    \end{itemize}
  \end{tblock}
  \note{This mirrors the role of registers and scratch memory in real
    computers: they speed up computation but do not increase what is
    computable.}
  \label{fr:7.5-02}
\end{frame}

% Frame 3: Theorem 7.26 – simulation by a single tape
\begin{frame}[t]{Theorem 7.26: equivalence of one and two tapes}
  \begin{tblock}{Statement}
    For any two‑tape Turing machine $T$, there exists a single‑tape
    Turing machine $S$ such that for every input $w$, $S$ halts with
    the same output as $T$ on $w$ (or recognises the same language).
  \end{tblock}
  \begin{tblock}{Implications}
    \begin{itemize}
      \item Extra tapes do not increase the computational power of
        Turing machines: whatever can be computed on multiple tapes can
        also be computed on a single tape.
      \item The simulation may be slower – typically polynomially
        slower – but the existence of a simulation suffices for
        computability.
    \end{itemize}
  \end{tblock}
  \note{This theorem parallels the result that multitape register
    machines or random access machines can be simulated by single
    memory models.}
  \label{fr:7.5-03}
\end{frame}

% Frame 4: Idea of the simulation
\begin{frame}[t]{Simulation overview}
  \begin{tblock}{Core idea}
    To simulate two tapes on one tape, we interleave the contents of
    tape 1 and tape 2 on a single track separated by special markers.
    Each cell on the single tape encodes two symbols, one from each
    virtual tape.  Head positions are indicated by primed symbols.
  \end{tblock}
  \begin{tblock}{Encoding scheme}
    \begin{itemize}
      \item Represent the single tape as a sequence of pairs
        $(a,b)$ where $a$ is the symbol from tape 1 and $b$ is the
        symbol from tape 2.  A marker $\$$ separates these pairs.
      \item Use primed symbols (e.g., $a'$) to mark the pair where the
        head of tape 1 or tape 2 currently sits.
      \item Use a delimiter $\#$ to mark the end of the finite portion
        of the tape; to the right, the tape is implicitly blank.
    \end{itemize}
  \end{tblock}
  \note{This encoding allows the single‑tape simulator to access and
    update both virtual heads in one pass over the encoded pairs.}
  \label{fr:7.5-04}
\end{frame}

% Frame 5: Initialisation of the simulator
\begin{frame}[t]{Simulation steps 1–2}
  \begin{tblock}{Core idea}
    Before the simulation can begin, the single tape must be set up
    with the encoded representation of the two‑tape machine’s input.
    The simulator then locates the primed symbols that mark the head
    positions.
  \end{tblock}
  \begin{tblock}{Step 1}
    \begin{itemize}
      \item Copy the input string $w$ onto both tracks: the first track
        holds $w$ on the left and blanks on the right; the second
        track holds the same $w$ (for machines that expect identical
        inputs on both tapes) or blanks if the second tape is empty.
      \item Insert the marker $\$$ between each pair of symbols and
        append $\#$ to mark the end.
    \end{itemize}
  \end{tblock}
  \begin{tblock}{Step 2}
    \begin{itemize}
      \item Mark the first pair $(a,b)$ by priming $a$ and $b$ to
        indicate that both heads start at the leftmost position.
      \item Ensure no other symbols are primed initially.
    \end{itemize}
  \end{tblock}
  \note{These preparations mirror the initial configuration of the
    two‑tape machine: both heads at the beginning of the input.}
  \label{fr:7.5-05}
\end{frame}

% Frame 6: Scanning to simulate one move
\begin{frame}[t]{Simulation steps 3–4}
  \begin{tblock}{Core idea}
    To simulate a single move of the two‑tape machine, the single‑tape
    simulator scans its entire tape to find the primed symbols (the
    virtual heads) and determine the scanned symbols on each tape.
  \end{tblock}
  \begin{tblock}{Step 3: reading the heads}
    \begin{itemize}
      \item Sweep left to right across the tape; when you find $a'$ in
        the first component and $b'$ in the second, record $(a,b)$ in
        the simulator’s state.
      \item Remove the primes to remember the head positions.
    \end{itemize}
  \end{tblock}
  \begin{tblock}{Step 4: applying the transition}
    \begin{itemize}
      \item Use the two‑tape machine’s transition function to compute
        $(q',a_1',a_2',d_1,d_2)$ from $(q,a,b)$.
      \item Overwrite the recorded positions with $a_1'$ and $a_2'$.
    \end{itemize}
  \end{tblock}
  \note{The simulator must encode the finite control of $T$ within its
    own state so that it can look up the appropriate transition.}
  \label{fr:7.5-06}
\end{frame}

% Frame 7: Updating primed positions
\begin{frame}[t]{Simulation steps 5–6}
  \begin{tblock}{Core idea}
    After writing the new symbols, the simulator must move the primes
    left or right to reflect the virtual head movements $d_1$ and
    $d_2$.  This may require inserting or deleting blank pairs at
    the boundaries.
  \end{tblock}
  \begin{tblock}{Step 5: moving the primes}
    \begin{itemize}
      \item If $d_1=R$ (or $L$), move the prime in the first component to
        the next (or previous) pair.  If no such pair exists on the
        left, insert a blank pair to avoid crashing.
      \item Perform the analogous update for $d_2$ in the second
        component.
    \end{itemize}
  \end{tblock}
  \begin{tblock}{Step 6: re‑priming and cleanup}
    \begin{itemize}
      \item Reattach the primes to the symbols now under the virtual
        heads.  Remove any temporary markers used during the scan.
      \item If blanks were inserted on the left, ensure they are
        properly separated by $\$$ markers.
    \end{itemize}
  \end{tblock}
  \note{These updates simulate the head motions of the two‑tape
    machine within the encoded single‑tape representation.}
  \label{fr:7.5-07}
\end{frame}

% Frame 8: Finishing a simulation cycle
\begin{frame}[t]{Simulation steps 7–8}
  \begin{tblock}{Core idea}
    Completing one move of the two‑tape machine may involve additional
    cleanup.  After updating the primes, the simulator resets to the
    left end of the tape in preparation for the next cycle.
  \end{tblock}
  \begin{tblock}{Step 7}
    \begin{itemize}
      \item Sweep leftward to the first pair (bounded by a leftmost
        marker) to reposition the head of the simulator.
      \item This ensures that the next scanning pass begins at the
        beginning of the encoded tape.
    \end{itemize}
  \end{tblock}
  \begin{tblock}{Step 8}
    \begin{itemize}
      \item If the two‑tape machine enters $h_a$ or $h_r$, the simulator
        halts with the same outcome.  Otherwise, return to Step 3 and
        repeat.
    \end{itemize}
  \end{tblock}
  \note{The simulator uses these steps to sequentially emulate each
    move of the original machine.  The overhead arises from scanning
    the entire tape each cycle.}
  \label{fr:7.5-08}
\end{frame}

% Frame 9: Visualising the encoding
\begin{frame}[t]{Two tracks on one tape}
  \centering
  \includegraphics[width=.7\linewidth]{sec7-5-tracks.png} % TODO: p. 245–246 schematic
  \begin{tblock}{Interpretation}
    Each rectangle in the schematic corresponds to a pair of symbols
    $(a,b)$ from the two tapes.  Primed symbols mark the virtual head
    positions.  The separator $\$$ partitions pairs, and $\#$ marks the
    end of the finite data.  This visual aids understanding of the
    simulation algorithm.
  \end{tblock}
  \note{Use this diagram to step through an example simulation, such
    as copying a single symbol from tape 1 to tape 2.}
  \label{fr:7.5-09}
\end{frame}

% Frame 10: Corollary 7.27 – no extra power
\begin{frame}[t]{Corollary 7.27: same power}
  \begin{tblock}{Core idea}
    As a consequence of Theorem 7.26, multi‑tape Turing machines are no
    more powerful than single‑tape machines: they recognise exactly the
    same languages and compute the same functions.  They merely offer a
    more convenient programming model.
  \end{tblock}
  \begin{tblock}{Implications}
    \begin{itemize}
      \item When proving computability, it suffices to describe a
        multi‑tape machine and invoke the theorem to conclude that a
        single‑tape machine exists.
      \item In complexity theory, however, the number of tapes matters:
        multi‑tape machines can be more time‑efficient than
        single‑tape machines by polynomial factors.
    \end{itemize}
  \end{tblock}
  \note{This corollary justifies using multi‑tape machines in
    algorithm design without affecting the set of computable functions.}
  \label{fr:7.5-10}
\end{frame}

% Frame 11: Exercise – copying with two tapes
\begin{frame}[t]{Exercise: two‑tape copying machine}
  \begin{tblock}{Task}
    Design a two‑tape Turing machine that, on input $w$ on tape 1 and
    blank tape 2, copies $w$ onto tape 2 and halts.  Outline the
    transition function.
  \end{tblock}
  \begin{talert}{Hint}
    Maintain the head on tape 1 at the current symbol and the head on
    tape 2 at the position where the copy is to be written.  On each
    move, write the symbol from tape 1 onto tape 2 and advance both
    heads to the right until the blank is encountered.
  \end{talert}
  \note{This exercise illustrates how multiple tapes simplify tasks
    that are cumbersome on a single tape.}
  \label{fr:7.5-11}
\end{frame}

% Frame 12: Exercise – simulating the copying machine
\begin{frame}[t]{Exercise: simulating two‑tape copying}
  \begin{tblock}{Task}
    Using the simulation described above, sketch how a single‑tape TM
    would simulate the two‑tape copying machine from the previous
    exercise.  Identify which steps correspond to reading, writing and
    moving the virtual heads.
  \end{tblock}
  \begin{talert}{Hint}
    Translate each move of the two‑tape machine into Steps 3–8 of the
    simulation: scan for the primed symbols, record the scanned
    symbols, write the new pair, move the primes and repeat.
  \end{talert}
  \note{This exercise reinforces the mechanics of the simulation and
    helps students understand the overhead involved.}
  \label{fr:7.5-12}
\end{frame}

% Frame 13: Summary of Section 7.5
\begin{frame}[t]{Summary of Section 7.5}
  \begin{tblock}{Core idea}
    Section 7.5 introduces multitape Turing machines and shows that
    they are no more powerful than single‑tape machines.  The
    simulation uses an encoding that pairs symbols from multiple tapes
    on a single track and updates primed markers to emulate head
    movements.  Although slower, the simulation preserves the
    computational behaviour.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item Definition of two‑tape TMs and their configurations.
      \item Theorem 7.26 establishes equivalence of one‑ and two‑tape
        machines; Corollary 7.27 extends this to any finite number of
        tapes.
      \item Simulation algorithm: encode pairs, scan to find primed
        positions, update according to the original machine’s
        transition, move primes and repeat.
      \item Multi‑tape machines simplify algorithm design but do not
        enlarge the set of computable functions or languages.
    \end{itemize}
  \end{tblock}
  \note{Encourage students to appreciate the conceptual simplicity of
    the simulation while recognising its practical overhead.}
  \label{fr:7.5-13}
\end{frame}

%-----------------------------------------------------------------------------
% Section 7.4 – Combining Turing Machines
%
% This section describes how to build more complex Turing machines by
% composing simpler ones.  Submachines are connected in sequence so
% that when one halts in $h_a$, control passes to the next.  Standard
% subroutines such as move to next blank, previous blank, copy and
% delete are introduced, culminating in a palindrome acceptor.

\section{Combining Turing Machines}

% Frame 1: Composition of machines
\begin{frame}[t]{Composing TMs}
  \begin{tblock}{Core idea}
    Complex algorithms can be implemented by composing simpler Turing
    machines.  If $T_1$ and $T_2$ are machines computing functions or
    recognising languages, their composition executes $T_1$ on the
    input; upon acceptance, it continues with $T_2$ on the resulting
    tape.  Rejecting or looping in $T_1$ propagates to the composite.
  \end{tblock}
  \begin{tblock}{State‑set stitching}
    \begin{itemize}
      \item Merge the state sets of $T_1$ and $T_2$ into a single set
        with distinct copies of $h_a$ and $h_r$.
      \item Redirect the accept state of $T_1$ to the start state of
        $T_2$: replace $\delta_{T_1}(h_a,a)$ with $\delta_{T_2}(q_0,a)$
        for all $a\in \Gamma$.
      \item Ensure that $T_2$’s reject state becomes the reject state
        of the composite.  The composite accepts only when $T_2$ accepts.
    \end{itemize}
  \end{tblock}
  \note{The composition is akin to function composition: the output
    of $T_1$ becomes the input of $T_2$.  Care must be taken to
    preserve tape conventions between components.}
  \label{fr:7.4-01}
\end{frame}

% Frame 2: Handoff semantics
\begin{frame}[t]{Handoff at the accept state}
  \begin{tblock}{Core idea}
    When $T_1$ enters its accept state $h_a$, it does not immediately
    halt; instead, the composite machine transitions into the start
    state of $T_2$ with the tape unchanged.  This handoff allows
    $T_2$ to continue processing where $T_1$ left off.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item If $\delta_{T_1}(h_a,a)$ is undefined, define it to be
        $(q_0^{T_2},a,S)$ for all $a\in\Gamma$.  This acts as an
        \emph{$\epsilon$‑move} into $T_2$.
      \item All other transitions in $T_1$ remain unchanged; $T_2$’s
        transitions operate on the same tape alphabet.
      \item If $T_1$ rejects, the composite rejects immediately; if
        $T_2$ rejects, the composite rejects; acceptance occurs only
        when $T_2$ halts in $h_a$.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls}
    Forgetting to clean up markers or align head positions between
    submachines can lead to unintended behaviour.  Design each
    component with a clear contract about tape contents and head
    position upon termination.
  \end{talert}
  \label{fr:7.4-02}
\end{frame}

% Frame 3: Mixed notation for submachines
\begin{frame}[t]{Mixed notation and submachines}
  \begin{tblock}{Core idea}
    To simplify diagrams, one can annotate transitions with the name of
    a submachine instead of drawing its entire state graph.  When
    control enters such a transition, the submachine runs to completion
    and returns when it halts.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item A transition labelled \textsc{SubTM} indicates that the
        composite machine executes that submachine on the current tape
        configuration.
      \item Upon $h_a$ in the submachine, control returns to the
        caller; upon $h_r$, the composite rejects.
      \item This notation mirrors procedure calls in programming and
        helps modularise complex TMs.
    \end{itemize}
  \end{tblock}
  \note{Mixed notation is not part of the formal TM definition but is
    a convenient shorthand when writing TM diagrams by hand.}
  \label{fr:7.4-03}
\end{frame}

% Frame 4: Subroutine NB – next blank to the right
\begin{frame}[t]{\texample{Subroutine NB: next blank}}
  \begin{tblock}{Core idea}
    The subroutine \textsc{NB} moves the head to the first blank
    square on the right of the current position.  It is useful for
    finding the end of a string or marking a place to append data.
  \end{tblock}
  \begin{tblock}{Specification}
    \begin{itemize}
      \item Starting in some state $p$, repeatedly move right until
        the blank $\Box$ is scanned.
      \item Enter state $p'$ and stay on $\Box$; do not overwrite it.
      \item Return control to the caller in state $p'$.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Usage}
    \textsc{NB} can be used after copying or deleting segments to
    reposition the head at the end of the active data region.
  \end{talert}
  \centering
  \includegraphics[width=.7\linewidth]{fig-7-19.png} % TODO: p. 241
  \note{Draw students’ attention to how \textsc{NB} leaves the tape
    unchanged except for moving the head.}
  \label{fr:7.4-04}
\end{frame}

% Frame 5: Subroutine PB – previous blank to the left
\begin{frame}[t]{\texample{Subroutine PB: previous blank}}
  \begin{tblock}{Core idea}
    The subroutine \textsc{PB} moves the head left until it encounters
    the first blank.  This is useful for returning to the beginning of
    a string or finding a separator after appending data.
  \end{tblock}
  \begin{tblock}{Specification}
    \begin{itemize}
      \item Starting at state $p$, move left while scanning non‑blank
        symbols.
      \item When $\Box$ is encountered, enter state $p'$ and remain on
        that square.
      \item Return control to the caller; the tape is unmodified.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls}
    Ensure the machine does not move left off the tape; otherwise it
    will crash into $h_r$.  Often a leftmost marker is placed on the
    tape before calling \textsc{PB}.
  \end{talert}
  \label{fr:7.4-05}
\end{frame}

% Frame 6: Subroutine Copy
\begin{frame}[t]{\texample{Subroutine Copy}}
  \begin{tblock}{Core idea}
    The subroutine \textsc{Copy} duplicates a block of data from one
    region of the tape to another.  For example, given a string
    $w\Box\Box\Box\cdots$, \textsc{Copy} produces $w\#w$ on the tape.
  \end{tblock}
  \begin{tblock}{Specification}
    \begin{itemize}
      \item Place a delimiter (e.g., $\#$) after the original string.
      \item Reset the head to the start of $w$ and scan each symbol.
      \item For each symbol, move to the end of the tape (using
        \textsc{NB}), write the symbol and return to the next symbol of
        $w$.
      \item Continue until all symbols are copied.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Gotchas}
    Avoid overwriting the original string while copying.  Use markers
    or state to remember which part of the tape you are in.
  \end{talert}
  \label{fr:7.4-06}
\end{frame}

% Frame 7: Subroutine Delete
\begin{frame}[t]{\texample{Subroutine Delete}}
  \begin{tblock}{Core idea}
    The subroutine \textsc{Delete} removes a specified substring from
    the tape by shifting the remainder left to fill the gap.  It is
    often used in combination with \textsc{Copy} to overwrite the
    original data after processing.
  \end{tblock}
  \begin{tblock}{Specification}
    \begin{itemize}
      \item Mark the beginning and end of the substring to be deleted.
      \item Starting at the left marker, move right, copying each
        subsequent symbol over the current position, until the end
        marker is reached.
      \item Fill the vacated region at the end with blanks.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls}
    Care must be taken to handle the blank at the deletion boundary; if
    the machine shifts data incorrectly, it may corrupt subsequent
    computations.
  \end{talert}
  \centering
  \includegraphics[width=.6\linewidth]{fig-7-22.png} % TODO: p. 242
  \label{fr:7.4-07}
\end{frame}

% Frame 8: Subroutine Equal
\begin{frame}[t]{\texample{Subroutine Equal}}
  \begin{tblock}{Core idea}
    The subroutine \textsc{Equal} compares two strings of equal
    length separated by a delimiter and sets a flag (or halts) if
    they are identical.  It is used, for example, after copying to
    check whether two halves of a string match.
  \end{tblock}
  \begin{tblock}{Specification}
    \begin{itemize}
      \item Place the head at the beginning of the first string and
        scan to the delimiter, pairing each symbol with its counterpart
        in the second string.
      \item If all pairs match, accept; if a mismatch is found,
        reject.
      \item Optionally unmark or clean up the delimiter afterwards.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls}
    Ensure the machine does not read past the delimiter; if the two
        strings have different lengths, reject immediately.
  \end{talert}
  \centering
  \includegraphics[width=.5\linewidth]{fig-7-23.png} % TODO: p. 243
  \label{fr:7.4-08}
\end{frame}

% Frame 9: Palindrome acceptor via composition
\begin{frame}[t]{\texample{Palindrome acceptor}}
  \begin{tblock}{Core idea}
    A palindrome is a string that reads the same forwards and backwards.
    We can recognise palindromes by composing submachines: (1) copy
    the input to create two copies, (2) move to the next blank, (3)
    reverse the second copy, (4) move back to the previous blank, and
    (5) compare the two halves.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Use \textsc{Copy} to transform $w$ into $w\#w$.
      \item Call \textsc{NB} to place the head on the blank following
        $w\#w$.
      \item Call a reverse subroutine on the second copy of $w$.
      \item Use \textsc{PB} to return to the delimiter $\#$.
      \item Call \textsc{Equal} to compare $w$ with $w^R$; accept if
        they match, reject otherwise.
    \end{enumerate}
  \end{tblock}
  \begin{talert}{Pitfalls}
    Ensure that markers used by the reverse subroutine do not interfere
    with the delimiter $\#$ or the comparison.  Clean up markers
    before calling \textsc{Equal}.
  \end{talert}
  \note{This composition exemplifies the modular design of TMs and
    how subroutines can be reused to build nontrivial algorithms.}
  \label{fr:7.4-09}
\end{frame}

% Frame 10: Designing components carefully
\begin{frame}[t]{Pitfalls in combining machines}
  \begin{tblock}{Core idea}
    When combining submachines, it is crucial to design each with a
    clear contract: the format of the tape when the subroutine is
    invoked, the location of the head, and the state upon return.
    Violating these contracts leads to unpredictable behaviour.
  \end{tblock}
  \begin{tblock}{Key guidelines}
    \begin{itemize}
      \item Place explicit markers to delimit segments of the tape
        that submachines may access.
      \item Always reposition the head to a predictable location before
        returning from a subroutine.
      \item Avoid leaving stray markers or temporary symbols that
        subsequent submachines might misinterpret.
    \end{itemize}
  \end{tblock}
  \note{These guidelines mirror best practices in software
    engineering, where modules communicate through well‑defined
    interfaces.}
  \label{fr:7.4-10}
\end{frame}

% Frame 11: Exercise – compose Copy and Equal
\begin{frame}[t]{Exercise: using Copy and Equal}
  \begin{tblock}{Task}
    Suppose you have subroutines \textsc{Copy} and \textsc{Equal} as
    described above.  Sketch how to build a TM that recognises
    duplicated strings of the form $ww$ without explicitly marking the
    midpoint.  Outline the sequence of subroutine calls.
  \end{tblock}
  \begin{talert}{Hint}
    Use \textsc{Copy} to transform the input $w$ into $w\#w$.  Then
    apply \textsc{Equal} to compare the two halves.  Reject if the
    delimiter $\#$ does not occur exactly in the middle.
  \end{talert}
  \note{This exercise invites students to reuse submachines to solve
    a problem they already encountered in Section 7.2.}
  \label{fr:7.4-11}
\end{frame}

% Frame 12: Exercise – handling markers between components
\begin{frame}[t]{Exercise: marker management}
  \begin{tblock}{Task}
    In the palindrome acceptor, suppose the reverse subroutine uses a
    temporary marker $M$ to mark characters.  Explain how to ensure
    that $M$ does not interfere with the delimiter $\#$ or the
    comparison in \textsc{Equal}.  Describe a general strategy for
    cleaning up markers between submachines.
  \end{tblock}
  \begin{talert}{Hint}
    Before returning from the reverse subroutine, scan the tape and
    replace all occurrences of $M$ with the original symbols.  Use
    state to remember whether you are in the left or right copy.
  \end{talert}
  \note{Proper marker management is critical when composing TMs; this
    exercise emphasises that detail.}
  \label{fr:7.4-12}
\end{frame}

% Frame 13: Summary of Section 7.4
\begin{frame}[t]{Summary of Section 7.4}
  \begin{tblock}{Core idea}
    Section 7.4 illustrates how to build complex Turing machines by
    composing simpler ones.  Subroutines such as \textsc{NB},
    \textsc{PB}, \textsc{Copy}, \textsc{Delete} and \textsc{Equal}
    modularise common tasks.  Composition involves connecting the
    accept state of one machine to the start state of the next.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item Composition is analogous to function composition: the
        output tape of $T_1$ becomes the input tape of $T_2$.
      \item Mixed notation allows submachines to be treated as black
        boxes within state diagrams.
      \item Careful design and marker management ensure that
        submachines interact correctly.
      \item A palindrome acceptor demonstrates the power of this
        modular approach.
    \end{itemize}
  \end{tblock}
  \note{Encourage students to think of Turing machines as modular
    programs.  This perspective will be useful when simulating
    multitape machines and nondeterminism.}
  \label{fr:7.4-13}
\end{frame}

%-----------------------------------------------------------------------------
% Section 7.3 – Turing Machines That Compute Partial Functions
%
% This section shifts focus from recognising languages to computing
% (partial) functions with Turing machines.  A machine may take a
% tuple of integers or strings as input and produce an output string on
% its tape.  Examples illustrate unary encoding, string reversal and
% simple arithmetic modulo 2.  Exercises reinforce the design of
% function‑computing TMs.

\section{Turing Machines That Compute Partial Functions}

% Frame 1: Computing vs recognising
\begin{frame}[t]{Computing vs recognising}
  \begin{tblock}{Core idea}
    Until now, Turing machines have been used to recognise languages:
    accept if the input belongs to the language, reject or loop
    otherwise.  Machines can also compute functions by transforming the
    input into an output written on the tape and then halting.  Such
    machines may be undefined on inputs outside the function’s domain.
  \end{tblock}
  \begin{tblock}{Key distinctions}
    \begin{itemize}
      \item A recogniser need only distinguish members from non‑members;
        it does not produce an output.
      \item A function‑computing TM must rewrite the tape to encode the
        function value and then halt in the accept state.
      \item If the machine loops on some inputs, the function is
        \emph{partial}: it is undefined for those inputs.
    \end{itemize}
  \end{tblock}
  \note{Point out that computing encompasses recognition as a special
    case: characteristic functions of languages yield 1 for members and
    0 for non‑members.}
  \label{fr:7.3-01}
\end{frame}

% Frame 2: Definition 7.9 – computing a partial function
\begin{frame}[t]{Definition of computing a function}
  \begin{tblock}{Core idea}
    A Turing machine $T$ \emph{computes} a partial function
    $f: (\Sigma^*)^k \rightarrow \Sigma^*$ if, on any input consisting of
    $k$ strings separated by delimiters, $T$ halts in $h_a$ with
    $f(x_1,\dots,x_k)$ written on its tape whenever $f$ is defined on
    $(x_1,\dots,x_k)$, and loops or halts in $h_r$ when $f$ is
    undefined on that input.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Inputs are encoded as a single string using a fixed
        delimiter (e.g., $\#)$ between components.
      \item The output of $T$ replaces the entire input portion of
        the tape; extraneous markers are cleared before halting.
      \item $T$ must halt on all inputs in the domain of $f$, but may
        loop on inputs outside the domain (yielding a partial function).
    \end{itemize}
  \end{tblock}
  \note{Stress that the machine must clean up its workspace before
    halting, leaving only the output string on the tape.}
  \label{fr:7.3-02}
\end{frame}

% Frame 3: Unary coding for numeric functions
\begin{frame}[t]{Encoding numbers in unary}
  \begin{tblock}{Core idea}
    When computing numeric functions, it is convenient to encode
    numbers in unary: the integer $n$ is represented by a string of
    $n$ ones separated by single zeros.  For a $k$‑ary function
    $f(n_1,\dots,n_k)$, the input is $1^{n_1} 0 1^{n_2} 0 \cdots 0
    1^{n_k}$, and the output is $1^{f(n_1,\dots,n_k)}$.
  \end{tblock}
  \begin{tblock}{Key advantages}
    \begin{itemize}
      \item Unary notation enables the TM to increment or decrement
        counts by adding or removing symbols without worrying about
        positional notation.
      \item Delimiters (zeros) separate the arguments, making it easy
        to identify individual components.
      \item Although inefficient, unary representation suffices to
        demonstrate computability; more efficient binary encodings can be
        handled with additional machinery.
    \end{itemize}
  \end{tblock}
  \note{Remind students that this unary encoding is conventional in
    computability theory; it simplifies the design of example TMs.}
  \label{fr:7.3-03}
\end{frame}

% Frame 4: Example – computing the reverse of a string
\begin{frame}[t]{\texample{Reverse function}}
  \begin{tblock}{Core idea}
    Given an input string $w$ over some alphabet, the reverse function
    outputs $w^R$.  A Turing machine can compute this by repeatedly
    swapping the first and last unprocessed symbols, moving inward.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Place two markers at the first and last symbols of the
        string.
      \item Swap the marked symbols by rewriting them and moving the
        markers inward.
      \item Repeat until the markers meet or cross; then remove
        auxiliary marks and halt with the reversed string on the tape.
    \end{enumerate}
  \end{tblock}
  \note{Students may recall similar algorithms in imperative
    programming; this TM implements it at the level of individual tape
    moves.}
  \label{fr:7.3-04}
\end{frame}

% Frame 5: Reverse function – implementation details
\begin{frame}[t]{Reverse function details}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Use uppercase variants (e.g., $A$ and $B$) as temporary
        markers to remember the symbols at the ends.
      \item To swap, overwrite the left symbol with a placeholder,
        traverse to the right end, copy the last symbol over the
        placeholder, and vice versa.
      \item After each swap, convert uppercase markers back to
        lowercase and move the markers inward by one position.
      \item When the markers meet or cross, clean up any remaining
        markers and halt.
    \end{itemize}
  \end{tblock}
  \centering
  \includegraphics[width=.8\linewidth]{fig-7-11.png} % TODO: p. 236
  \begin{talert}{Pitfalls}
    Ensure that the TM correctly handles the case of odd‑length
    strings, where the middle symbol does not need to be swapped but
    must be left unchanged.
  \end{talert}
  \note{Work through a specific example, such as reversing $abc$, to
    illustrate the movement of the markers and the swap operations.}
  \label{fr:7.3-05}
\end{frame}

% Frame 6: Example – quotient and remainder modulo 2
\begin{frame}[t]{\texample{Quotient and remainder mod 2}}
  \begin{tblock}{Core idea}
    For an input of the form $1^n$ in unary, we wish to compute
    $\lfloor n/2 \rfloor$ and $n \bmod 2$ separately.  One TM computes
    the quotient by erasing every other $1$; another computes the
    remainder by scanning the tape and checking the parity of the
    length.
  \end{tblock}
  \begin{tblock}{Roadmap for quotient}
    \begin{enumerate}
      \item Starting at the leftmost $1$, mark it and skip the next
        unmarked $1$, erasing it.  Continue alternating mark and erase
        until all symbols are processed.
      \item When no more unmarked $1$’s remain, remove the markers;
        the remaining $1$’s form $1^{\lfloor n/2 \rfloor}$.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Roadmap for remainder}
    \begin{enumerate}
      \item Traverse the tape to the end, counting parity by toggling
        a state variable between “even” and “odd” each time a $1$ is
        encountered.
      \item Upon reaching blank, write $1$ if the state is “odd” and
        write nothing if “even”; erase the original input and halt.
    \end{enumerate}
  \end{tblock}
  \note{Figures 7.13a and 7.13b illustrate TMs computing quotient and
    remainder modulo 2.  These machines highlight how to use state to
    remember simple numerical information.}
  \label{fr:7.3-06}
\end{frame}

% Frame 7: Quotient mod 2 – tracing the algorithm
\begin{frame}[t]{Tracing the quotient machine}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item On input $1^5$, the machine marks the first $1$ and erases
        the second, marks the third and erases the fourth, and finally
        marks the fifth.  The three marked $1$’s are then converted
        back into unmarked $1$’s to produce $1^3$, which equals
        $\lfloor 5/2 \rfloor$.
      \item This process halves the length of the input by pairing
        adjacent symbols.
      \item The number of passes over the tape is linear in the input
        length, demonstrating that simple arithmetic is computable by
        TMs.
    \end{itemize}
  \end{tblock}
  \centering
  \includegraphics[width=.55\linewidth]{fig-7-13a.png} % TODO: p. 237
  \note{Trace a shorter example, like $1^4 \mapsto 1^2$, to ensure
    students understand the pairing strategy.}
  \label{fr:7.3-07}
\end{frame}

% Frame 8: Remainder mod 2 – tracing the algorithm
\begin{frame}[t]{Tracing the remainder machine}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item On input $1^n$, the TM scans right, flipping a state
        variable between “even” and “odd” each time it sees a $1$.
      \item Upon reaching blank, it erases the input and writes $1$ if
        the state is “odd”; otherwise it writes nothing.
      \item For $n$ even, the output is the empty string; for $n$ odd,
        the output is $1$, encoding $n \bmod 2$.
    \end{itemize}
  \end{tblock}
  \centering
  \includegraphics[width=.55\linewidth]{fig-7-13b.png} % TODO: p. 237
  \note{This machine demonstrates the use of internal state alone to
    compute a simple function without rewriting the input until the end.}
  \label{fr:7.3-08}
\end{frame}

% Frame 9: Characteristic functions vs acceptors
\begin{frame}[t]{Characteristic functions and acceptance}
  \begin{tblock}{Core idea}
    The characteristic function of a language $L$ is the function
    $\chi_L$ that returns $1$ on inputs in $L$ and $0$ otherwise.  A TM
    computing $\chi_L$ is stronger than a recogniser for $L$ because it
    must halt and output a bit on every input, not merely accept
    members and loop on non‑members.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item If $L$ is decidable, its characteristic function is
        computable: use a decider for $L$ to output $1$ on acceptance
        and $0$ on rejection.
      \item If $\chi_L$ is computable, then $L$ is decidable, because
        we can run the function and accept when it outputs $1$ and
        reject when it outputs $0$.
      \item If $L$ is recognisable but not decidable, then $\chi_L$
        cannot be computed by any TM, since it would yield a decider.
    \end{itemize}
  \end{tblock}
  \note{Explain the interplay between computing characteristic
    functions and deciding languages; this will recur in undecidability
    proofs.}
  \label{fr:7.3-09}
\end{frame}

% Frame 10: Partial vs total functions
\begin{frame}[t]{Partiality and domains}
  \begin{tblock}{Core idea}
    A function computed by a TM may be undefined on some inputs; the
    machine loops on those inputs.  Such functions are called partial
    functions.  A total function is defined on all possible inputs and
    requires the computing TM to halt on every input.
  \end{tblock}
  \begin{tblock}{Implications}
    \begin{itemize}
      \item Partial computable functions model algorithms that may not
        produce output for some inputs (e.g., searching for a witness that
        might not exist).
      \item Total computable functions correspond to algorithms that
        always terminate with a result.
      \item Proving totality often requires a termination argument,
        similar to those used for language deciders.
    \end{itemize}
  \end{tblock}
  \note{Emphasise that partial functions are ubiquitous in computability
    theory; many natural problems lack total algorithms.}
  \label{fr:7.3-10}
\end{frame}

% Frame 11: Exercise – computing the length of a unary string
\begin{frame}[t]{Exercise: compute the length}
  \begin{tblock}{Task}
    Design a Turing machine that, on input $1^n$ in unary, outputs
    $1^n$ on its tape (i.e., computes the identity function on natural
    numbers).  Your machine should erase the input one symbol at a
    time while building a copy to the right, then clean up and halt.
  \end{tblock}
  \begin{talert}{Hint}
    Use a two‑phase process: mark a $1$, move to the end of the tape
    and append a new $1$, then return and erase the mark.  Repeat until
    all input symbols are processed.
  \end{talert}
  \note{This exercise illustrates copying data on a TM tape, a basic
    technique for many algorithms.}
  \label{fr:7.3-11}
\end{frame}

% Frame 12: Exercise – doubling a unary number
\begin{frame}[t]{Exercise: doubling function}
  \begin{tblock}{Task}
    Construct a Turing machine that computes the function $f(n)=2n$ on
    unary input $1^n$, producing $1^{2n}$.  Describe your algorithm
    informally and outline the phases needed to copy the input twice.
  \end{tblock}
  \begin{talert}{Hint}
    One strategy is to copy the input to the right as in the previous
    exercise, then copy the original input again after the first copy.
    Clean up any markers before halting.
  \end{talert}
  \note{Ask students to think about how many passes over the tape are
    necessary and how to avoid overwriting data prematurely.}
  \label{fr:7.3-12}
\end{frame}

% Frame 13: Exercise – computing a simple characteristic function
\begin{frame}[t]{Exercise: a characteristic function}
  \begin{tblock}{Task}
    Let $L$ be the language of unary strings of even length.  Design a
    TM that computes the characteristic function $\chi_L$: it writes
    $1$ on its tape if the input has even length and $0$ otherwise.
  \end{tblock}
  \begin{talert}{Hint}
    You can adapt the remainder‑mod 2 machine: scan the input and
    toggle between two states to remember parity, then erase the input
    and write the appropriate output symbol.
  \end{talert}
  \note{Computing a characteristic function is stronger than merely
    recognising the language; ensure your machine halts on all inputs.}
  \label{fr:7.3-13}
\end{frame}

% Frame 14: Exercise – domain and looping
\begin{frame}[t]{Exercise: domain of a partial function}
  \begin{tblock}{Task}
    Suppose a TM $T$ computes a partial function $f$ on unary inputs
    such that $f(0)=0$, $f(1)=1$ and $f(n)$ is undefined for $n>1$.
    Describe how $T$ behaves on inputs $1^2$ and $1^3$.  Does $T$ loop
    or enter $h_r$?  Justify your answer.
  \end{tblock}
  \begin{talert}{Hint}
    $T$ must not falsely output a value for $n>1$, so it either loops
    forever or explicitly rejects those inputs.  Consider how you
    would implement this behaviour with a simple counting mechanism.
  \end{talert}
  \note{This exercise emphasises the difference between looping and
    explicit rejection when a function is undefined on certain inputs.}
  \label{fr:7.3-14}
\end{frame}

% Frame 15: Mini summary of Section 7.3
\begin{frame}[t]{Summary of Section 7.3}
  \begin{tblock}{Core idea}
    Section 7.3 extends Turing machines beyond language recognition to
    computing partial functions.  Inputs and outputs are encoded as
    strings (often unary for integers).  The machine must produce the
    correct output on all inputs in the function’s domain and may
    loop on others.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item Definition of computing a partial function (Definition 7.9).
      \item Unary encoding of numbers and tuples.
      \item Example machines for reversing a string and computing
        quotient and remainder modulo 2.
      \item Characteristic functions and the connection between
        computability and decidability.
      \item Exercises on copying, doubling and parity underscore
        practical TM design techniques.
    \end{itemize}
  \end{tblock}
  \note{Encourage students to attempt the exercises to gain hands‑on
    experience with designing function‑computing TMs.}
  \label{fr:7.3-15}
\end{frame}

% Additional frames for Section 7.2 to meet the depth requirement

% Frame 15: Detailing the marking phase of the duplication TM
\begin{frame}[t]{Marking phase in detail}
  \begin{tblock}{Core idea}
    During the marking phase of the $xx$ machine, the TM uses its
    ability to move freely to isolate the midpoint of the string.  It
    systematically moves the last unmarked symbol to the front,
    thereby building a mirrored structure.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Starting at the left, scan right until a blank is found.
        Move left one square to the last symbol of the string.
      \item Replace that symbol with a marker $\triangledown$, shift it
        one square left at a time by swapping with each preceding
        symbol until reaching the leftmost unmarked square.
      \item Repeat this process, thereby pushing all markers to the
        front half of the tape.  When the number of markers equals the
        number of unmarked symbols, the midpoint has been marked.
    \end{itemize}
  \end{tblock}
  \note{By rearranging the tape, the TM effectively measures the
    length of the input – something impossible for PDAs.}
  \label{fr:7.2-15}
\end{frame}

% Frame 16: Detailing the comparison phase of the duplication TM
\begin{frame}[t]{Comparison phase in detail}
  \begin{tblock}{Core idea}
    After the midpoint is identified, the TM compares each marker on
    the left with a corresponding symbol on the right.  Mismatches
    cause rejection.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Move to the leftmost marker and record its underlying symbol
        (e.g., remember whether it originally represented an $a$ or $b$).
      \item Move right past all markers and the midpoint marker to
        the first unmarked symbol on the right half; compare it to the
        remembered symbol.
      \item If they match, erase or mark both and return to the next
        marker on the left; otherwise, transition to $h_r$.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls}
    Ensure that after comparing a pair, the head returns to the
    correct left marker.  A systematic scanning strategy prevents
    missing or rechecking symbols.
  \end{talert}
  \note{These details show how the TM uses markers to store
    information across passes over the tape.}
  \label{fr:7.2-16}
\end{frame}

% Frame 17: Pairing algorithm for $i<j$ revisited
\begin{frame}[t]{Pairing algorithm in detail}
  \begin{tblock}{Core idea}
    The algorithm for $\{a^i b a^j \mid i<j\}$ incrementally removes
    one $a$ from each side of the $b$.  By tracking the removal, the
    machine determines whether the right block remains longer.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Use a special marker to denote the $b$ and distinguish
        between left and right blocks.
      \item To remove a pair: (1) move left of the $b$ to find an
        unmarked $a$; replace it with a blank; (2) move right past
        the $b$ and erase the first unmarked $a$ on the right.
      \item Maintain invariants: the number of erased symbols
        corresponds on both sides until one side empties; acceptance
        occurs only if the right has strictly more symbols.
    \end{itemize}
  \end{tblock}
  \note{Breaking the algorithm into substeps helps in constructing
    the actual transition function or diagram.}
  \label{fr:7.2-17}
\end{frame}

% Frame 18: The effect of undefined blank transitions
\begin{frame}[t]{Undefined transitions and implicit loops}
  \begin{tblock}{Core idea}
    If a Turing machine designer omits a transition for a certain
    symbol in a non‑halting state, the machine will implicitly reject
    whenever that configuration arises.  Conversely, failing to handle
    the blank symbol at the end of the input may cause the TM to loop
    forever.
  \end{tblock}
  \begin{tblock}{Examples}
    \begin{itemize}
      \item A machine that simulates a DFA but never checks for blank
        will keep moving right forever on an infinite tail of blanks,
        looping rather than rejecting.
      \item If a machine expects only $a$ and $b$ but encounters $c$,
        the undefined transition causes an immediate crash into $h_r$.
        Specifying $\delta(q,c)$ explicitly can change the outcome.
    \end{itemize}
  \end{tblock}
  \note{Highlight the importance of defining behaviour on all
    relevant symbols, especially the blank, to guarantee termination.}
  \label{fr:7.2-18}
\end{frame}

% Frame 19: Loops and partial computations
\begin{frame}[t]{Discussion: loops and partiality}
  \begin{tblock}{Core idea}
    A Turing machine that may loop on some inputs defines a partial
    function from strings to \{accept, reject\}.  Partiality is
    inherent in recognisable languages: one cannot, in general,
    transform a recogniser into a decider without additional insight.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item Recognisers may leave some inputs unresolved (looping);
        deciders must always resolve every input by halting.
      \item The halting problem shows that no general method can
        convert every recogniser into a decider.
      \item When designing TMs, strive to ensure termination on all
        cases unless non‑decidability is intended or unavoidable.
    \end{itemize}
  \end{tblock}
  \note{This discussion foreshadows later undecidability results and
    helps students appreciate the subtle difference between semi‑
    decidability and full decidability.}
  \label{fr:7.2-19}
\end{frame}

% Frame 20: Exercise – design a palindrome recogniser
\begin{frame}[t]{Exercise: designing a palindrome TM}
  \begin{tblock}{Task}
    Sketch a Turing machine that recognises palindromes over
    $\{a,b\}$.  Your machine should use markers to match the first and
    last symbols, moving inward until all pairs are checked.
  \end{tblock}
  \begin{talert}{Hint}
    Mark the leftmost symbol, move to the rightmost unmarked symbol
    and compare; if they match, mark it and return left.  Repeat
    until the markers meet or cross.  Reject on mismatch.
  \end{talert}
  \note{This exercise combines the marker technique used in the
    duplication machine with the comparison strategy for $i<j$.
    Palindromes provide a classic test for TM design.}
  \label{fr:7.2-20}
\end{frame}

%-----------------------------------------------------------------------------
% Section 7.2 – Turing Machines as Language Acceptors
%
% This section explains how Turing machines recognise languages.  It
% introduces the acceptance definition, shows how finite automata can be
% simulated by Turing machines, and presents example machines for
% specific languages.  Exercises encourage students to reason about
% accept/reject/loop outcomes.

\section{Turing Machines as Language Acceptors}

% Frame 1: Acceptance definition (Def 7.2)
\begin{frame}[t]{Acceptance of a language}
  \begin{tblock}{Core idea}
    A Turing machine $T$ \emph{accepts} a string $w$ if the initial
    configuration on $w$ reaches the accept state $h_a$ after a finite
    number of moves.  The language accepted by $T$ is denoted $L(T) =
    \{ w \mid T \text{ accepts } w \}$.  A machine may also reject or
    loop on an input.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item $T$ accepts $w$ if $q_0\Box w \Rightarrow^* C$ for some
        configuration $C$ containing $h_a$.
      \item $T$ rejects $w$ if $q_0\Box w \Rightarrow^* C$ for some
        configuration $C$ containing $h_r$ and there is no accepting
        configuration reachable.
      \item If neither halting state is ever reached, $T$ loops on $w$.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls}
    Acceptance is defined in terms of the existence of a halting path.
    A single looping branch does not invalidate acceptance if another
    branch eventually reaches $h_a$ (this becomes relevant for
    nondeterministic machines later).
  \end{talert}
  \note{This definition parallels earlier notions of recognisable
    languages: a language is Turing‑recognisable if some TM accepts
    precisely its strings.  Decidability additionally requires that
    $T$ halt on all inputs.}
  \label{fr:7.2-01}
\end{frame}

% Frame 2: Accept vs reject vs loop
\begin{frame}[t]{Accept, reject or loop?}
  \begin{tblock}{Core idea}
    A Turing machine partitions strings into three categories: those it
    accepts, those it rejects and those on which it loops.  Unlike
    finite automata and pushdown automata, which implicitly reject by
    failing to enter an accept state, Turing machines must signal
    rejection explicitly.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item For every $w \in \Sigma^*$, exactly one of three outcomes holds:
        $w\in L(T)$ if $T$ halts in $h_a$, $w$ is rejected if $T$ halts
        in $h_r$, or $T$ loops on $w$.
      \item A language is \emph{recognisable} if some TM accepts all
        strings in the language and either rejects or loops on all
        others.
      \item A language is \emph{decidable} if some TM accepts all
        strings in the language and rejects all others (no looping).
    \end{itemize}
  \end{tblock}
  \note{Underline the distinction between recognisable and decidable
    languages; this will be crucial when discussing undecidability in
    later chapters.}
  \label{fr:7.2-02}
\end{frame}

% Frame 3: Simulating finite automata with TMs
\begin{frame}[t]{From finite automata to Turing machines}
  \begin{tblock}{Core idea}
    Every deterministic finite automaton (DFA) can be simulated by a
    Turing machine that scans its input once from left to right
    without rewriting or moving backwards.  The TM uses its state to
    mimic the DFA’s control and halts on blank input.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Construct a TM whose states correspond to the DFA’s
        states plus two halting states.
      \item For each DFA transition $\delta_F(p,\sigma)=q$, define
        $\delta_T(p,\sigma)=(q,\sigma,R)$.
      \item When the TM encounters the blank symbol after the last
        input symbol, it moves into $h_a$ if in an accepting DFA state
        or $h_r$ otherwise.
    \end{enumerate}
  \end{tblock}
  \begin{talert}{Gotchas}
    A DFA implicitly rejects by ending in a non‑accepting state; the TM
    must explicitly enter $h_r$ after finishing the input.  There is no
    need to move left or rewrite symbols in this simulation.
  \end{talert}
  \note{Refer to Figure 7.4 for an illustration of the FA→TM
    translation.  The important idea is that TMs can replicate less
    powerful machines exactly.}
  \label{fr:7.2-03}
\end{frame}

% Frame 4: Diagrammatic example of FA to TM
\begin{frame}[t]{Example translation of a DFA}
  \begin{tblock}{Core idea}
    Consider a DFA recognising strings with an even number of $a$’s.
    The corresponding TM has states $q_{\mathrm{even}}$ and
    $q_{\mathrm{odd}}$ and transitions that move right on $a$ and
    other symbols.  Upon reaching blank, it enters $h_a$ from
    $q_{\mathrm{even}}$ and $h_r$ from $q_{\mathrm{odd}}$.
  \end{tblock}
  \centering
  \includegraphics[width=.8\linewidth]{fig-7-4.png} % TODO: p. 230
  \begin{talert}{Comment}
    Note how the TM never overwrites the tape and only moves right.
    Implicit rejects in the DFA are made explicit by transitions to
    $h_r$ after scanning the blank.
  \end{talert}
  \note{Ask students to verify that this TM accepts precisely the
    language of even‑$a$ strings and halts on all inputs.}
  \label{fr:7.2-04}
\end{frame}

% Frame 5: Implicit rejections and blank detection
\begin{frame}[t]{Handling implicit rejections}
  \begin{tblock}{Core idea}
    A DFA implicitly rejects by finishing in a non‑accepting state; a
    TM must explicitly transition to $h_r$ when the input is exhausted.
    It detects the end of the input by encountering the blank symbol
    and then decides whether to accept or reject.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Insert transitions of the form $\delta(q,\Box)=(h_a,\Box,S)$
        if $q$ is accepting in the DFA and
        $\delta(q,\Box)=(h_r,\Box,S)$ otherwise.
      \item Ensure all other symbols of $\Sigma$ simply move right
        without changing the state or the tape.
    \end{itemize}
  \end{tblock}
  \note{This frame clarifies how to turn a DFA into a TM that halts on
    all inputs, thereby making the implicit rejection explicit.}
  \label{fr:7.2-05}
\end{frame}

% Frame 6: Example TM for the language $XX$
\begin{frame}[t]{\texample{TM for $XX$}}
  \begin{tblock}{Core idea}
    We design a Turing machine that accepts strings of the form
    $xx$, where $x\in \{a,b\}^*$.  The machine operates in two
    phases: first it marks the midpoint, and second it compares the two
    halves symbol by symbol.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item \textbf{Marking phase}: repeatedly sweep right to find the end
        of the string, marking the last unmarked symbol with a special
        marker (e.g., $\triangledown$) and moving it to the leftmost
        unmarked position.
      \item \textbf{Comparison phase}: return to the beginning and
        compare corresponding symbols of the two halves, erasing or
        marking them as you go.
      \item If all pairs match and no unmatched symbols remain, enter
        $h_a$; otherwise enter $h_r$.
    \end{enumerate}
  \end{tblock}
  \begin{talert}{Key insight}
    The machine uses the tape as scratch space to reorganise the input.
    Unlike PDAs, it can revisit any part of the string and perform
    arbitrary rewrites.
  \end{talert}
  \note{Encourage students to visualise the marker being moved to
    identify the midpoint, then the pairwise comparison that ensures
    duplication.}
  \label{fr:7.2-06}
\end{frame}

% Frame 7: Tracing the $XX$ machine
\begin{frame}[t]{Tracing the duplication machine}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item On input $abbaab$, the TM marks the last $b$, moves it to
        the front and repeats until the midpoint marker divides the
        tape into two equal parts.
      \item In the comparison phase, it erases the marker and checks
        each pair of symbols: $a$ vs $a$, $b$ vs $b$, and so on.
      \item If at any point a mismatch occurs or one half is longer
        than the other, the machine transitions to $h_r$.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls}
    The machine must ensure that no unmarked symbols remain when
    entering the comparison phase; otherwise it might erroneously
    accept strings of odd length.  Careful bookkeeping during the
    marking phase prevents this.
  \end{talert}
  \note{Discuss how the ability to move left and right enables the
    machine to “shuffle” symbols on the tape, a power not available to
    pushdown automata.}
  \label{fr:7.2-07}
\end{frame}

% Frame 8: Example TM for $\{a^i b a^j \mid i<j\}$
\begin{frame}[t]{\texample{TM for $\{a^i b a^j \mid i<j\}$}}
  \begin{tblock}{Core idea}
    This language consists of strings with a single $b$ separating two
    blocks of $a$’s, where the block after $b$ is longer than the block
    before it.  The TM first verifies the form $a^* b a^*$ and then
    erases one $a$ from the right block for each $a$ in the left block.
    If at least one $a$ remains on the right, it accepts.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Scan to ensure exactly one $b$ and no other symbols; reject
        otherwise.
      \item Mark the $b$ and repeatedly: (a) move left to find the
        leftmost unmarked $a$; if none, move to step 3; (b) erase that
        $a$; (c) move right past the $b$ and erase the first $a$ on the
        right block; if none, reject.
      \item After pairing all left $a$’s, check if any right $a$’s
        remain; if so, accept; otherwise reject.
    \end{enumerate}
  \end{tblock}
  \note{This machine may loop if the pairing step never finishes.
    However, because each pairing erases exactly two $a$’s, the process
    must terminate when either block is exhausted.}
  \label{fr:7.2-08}
\end{frame}

% Frame 9: Looping and termination in the $i<j$ machine
\begin{frame}[t]{Termination analysis}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Each iteration of the pairing loop reduces the total number
        of $a$’s by two.  Since the input is finite, the loop must
        eventually end.
      \item If both blocks become empty simultaneously, then $i=j$ and
        the machine rejects.  If the right block becomes empty first,
        $i>j$ and the machine rejects.  If the left block empties while
        at least one $a$ remains on the right, then $i<j$ and the
        machine accepts.
      \item There is no possibility of an infinite loop, so this TM
        decides the language.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls}
    Forgetting to check the single $b$ condition or to handle empty
    blocks correctly will lead to incorrect acceptance.  Always verify
    the structure of the input before beginning the pairing loop.
  \end{talert}
  \note{Use this analysis to illustrate how to argue termination and
    correctness of a TM.}
  \label{fr:7.2-09}
\end{frame}

% Frame 10: Recognition vs decidability
\begin{frame}[t]{Recognisable vs decidable languages}
  \begin{tblock}{Core idea}
    A language is Turing‑recognisable if some TM accepts exactly its
    strings, possibly looping on others.  It is Turing‑decidable if
    some TM accepts exactly its strings and rejects all others.  Every
    decidable language is recognisable, but not vice versa.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Recognisable: $L$ is recognisable if $\exists$ TM $T$ such
        that $L(T)=L$.
      \item Decidable: $L$ is decidable if $\exists$ TM $T$ such that
        $L(T)=L$ and $T$ halts on all inputs.
      \item There exist languages that are recognisable but not
        decidable (e.g., the Halting Problem).  The difference lies in
        whether looping is allowed on non‑members.
    \end{itemize}
  \end{tblock}
  \note{Foreshadow the undecidability results to come in later
    chapters.  Students should appreciate why looping cannot simply
    be equated with rejection.}
  \label{fr:7.2-10}
\end{frame}

% Frame 11: Exercise – design a right‑moving TM
\begin{frame}[t]{Exercise: simulating a DFA}
  \begin{tblock}{Task}
    Sketch a Turing machine that recognises the language $\{ w \in
    \{a,b\}^* \mid w \text{ ends with } a \}$ by scanning its input
    once from left to right and halting upon reaching the blank.
  \end{tblock}
  \begin{talert}{Hint}
    Mimic the DFA with states representing whether the last symbol
    seen was $a$ or $b$.  On blank, move to $h_a$ if the last symbol
    was $a$ and to $h_r$ otherwise.
  \end{talert}
  \note{This exercise reinforces the FA→TM construction and shows how
    to handle final‑symbol conditions.}
  \label{fr:7.2-11}
\end{frame}

% Frame 12: Exercise – locating loops
\begin{frame}[t]{Exercise: where does it loop?}
  \begin{tblock}{Task}
    Consider a TM that on input $w$ rewrites the first symbol to a
    marker and then moves to the right searching for another marker.
    If it reaches the blank, it moves left and repeats forever.  Does
    this machine accept, reject or loop on the empty string?  On the
    string $a$?  On the string $aa$?  Explain.
  \end{tblock}
  \begin{talert}{Hint}
    Trace the head’s movement: on $\epsilon$, the machine marks
    nothing and immediately loops; on a single $a$, it marks the
    first $a$ and loops; on $aa$, it marks the first $a$, finds the
    second $a$, marks it and terminates?  Be precise.
  \end{talert}
  \note{This exercise encourages students to reason about
    non‑terminating behaviour and its impact on language recognition.}
  \label{fr:7.2-12}
\end{frame}

% Frame 13: Exercise – classify outcomes
\begin{frame}[t]{Exercise: classify accept/reject/loop}
  \begin{tblock}{Task}
    For each of the following hypothetical TMs $T$ and inputs $w$, decide
    whether $T$ accepts, rejects or loops.  Provide a brief justification.
    \begin{enumerate}
      \item $T$ checks if $w$ has equal numbers of $a$ and $b$ by
        pairing them; if it encounters an unmatched $a$ or $b$, it
        crashes.  Input $w=abbb$.
      \item $T$ simulates a DFA recognising even numbers of $a$’s
        but never transitions to $h_r$ when the blank is reached.
        Input $w=ba$.
      \item $T$ searches for a second $c$ in $w$ and halts in $h_a$ if
        found; otherwise it moves right forever.  Input $w=cc$.
    \end{enumerate}
  \end{tblock}
  \begin{talert}{Solution outline}
    \begin{itemize}
      \item[(a)] The machine will encounter an unmatched $b$ and crash,
        so it rejects.
      \item[(b)] The machine ends in a non‑accepting DFA state on
        blank but never enters $h_r$, so it loops.
      \item[(c)] The machine finds the second $c$ and halts in $h_a$,
        so it accepts.
    \end{itemize}
  \end{talert}
  \note{Encourage students to articulate why each outcome occurs.
    Distinguish between explicit halting and implicit looping.}
  \label{fr:7.2-13}
\end{frame}

% Frame 14: Mini summary of Section 7.2
\begin{frame}[t]{Summary of Section 7.2}
  \begin{tblock}{Core idea}
    Turing machines are not merely abstract calculators; they serve as
    language acceptors as well.  A TM accepts a string by halting in
    its accept state, rejects by halting in its reject state and may
    loop on inputs outside its language.  Every DFA can be simulated
    by a simple TM.  Examples show how TMs can enforce complex
    structural constraints such as duplication ($xx$) and size
    comparisons ($i<j$).
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item Definition of acceptance and the distinction between
        recognisable and decidable languages.
      \item Simulation of finite automata: right‑moving TMs with
        explicit halting behaviour.
      \item Design patterns for TMs: marking, scanning, erasing and
        comparison to implement sophisticated checks.
    \end{itemize}
  \end{tblock}
  \note{This summary consolidates the techniques introduced for
    designing language‑accepting TMs and sets the stage for machines
    that compute functions.}
  \label{fr:7.2-14}
\end{frame}

%-----------------------------------------------------------------------------
% Section 7.1 – A General Model of Computation
%
% This section introduces Turing machines, motivated by the inadequacy of
% finite automata and pushdown automata for certain languages.  It models
% computation with a tape, head and finite control.  Each frame below
% paraphrases one or more paragraphs from Martin’s Chapter 7, Section 7.1.

\section{A General Model of Computation}

% Frame 1: Limitations of FAs and PDAs
\begin{frame}[t]{Beyond finite and stack memory}
  \begin{tblock}{Core idea}
    Finite automata are restricted to a fixed number of states and thus
    cannot remember arbitrarily long substrings.  Pushdown automata add a
    stack, enabling recognition of languages like $a^n b^n$, but they still
    fail on patterns requiring more than one unbounded counter or
    non‑contiguous comparisons.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Recall the languages $a^n b^n c^n$, \\ $\{ x c x \mid x \in \{a,b\}^* \}$ and palindromes.
      \item Observe that PDAs cannot enforce three equal counts or match
        separated substrings.
      \item Introduce a more powerful model that simulates a human
        computer with unlimited scratch space.
    \end{enumerate}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    Do not conclude that every language beyond pushdown automata is
    undecidable.  A stronger model exists that can simulate any
    effective computation – the Turing machine.
  \end{talert}
  \note{Use this opening frame to remind students of earlier examples
    where PDA memory was insufficient.  Prepare them for the leap to
    Turing’s more general model.}
  \label{fr:7.1-01}
\end{frame}

% Frame 2: Motivating languages that are not context‑free
\begin{frame}[t]{Languages beyond context‑free power}
  \begin{tblock}{Core idea}
    The ternary matching language $\{a^n b^n c^n \mid n \ge 0\}$,
    the duplication language $\{ x c x \mid x \in \{a,b\}^*\}$ and
    the simple palindrome language $\{ w w^R \}$ all demand more than
    one stack.  A PDA can count one quantity but cannot compare three
    independent counts or mirror widely separated halves.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item $a^n b^n c^n$ requires two independent counters to match three
        blocks; a single stack provides only one.
      \item Palindromes with a marker $c$ in the middle require storing an
        entire substring for later comparison; a PDA cannot retrieve
        arbitrary positions once popped.
      \item Such languages motivate a model that can store and compare
        unbounded data in arbitrary positions.
    \end{itemize}
  \end{tblock}
  \note{Connect these examples to the pumping lemma for CFLs studied in
    Section 6.1.  These languages were shown to be non‑context‑free and
    now serve as test cases for a more powerful machine.}
  \label{fr:7.1-02}
\end{frame}

% Frame 3: Turing’s motivation
\begin{frame}[t]{Turing’s insight and goals}
  \begin{tblock}{Core idea}
    Alan Turing sought to formalise what it means to compute.  His
    thought experiment did not merely extend pushdown automata; it
    modelled the mechanical process carried out by a human following
    explicit rules with unlimited paper for scratch work.  The Church–
    Turing thesis asserts that this model captures all effectively
    computable procedures.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Identify primitive actions available to a human computer.
      \item Abstract those actions into a mathematical model (the TM).
      \item Argue informally that all reasonable computing devices are
        equivalent in power.
    \end{enumerate}
  \end{tblock}
  \note{Stress that Turing machines were designed to capture
    computation itself, not just to recognise languages.  This broader
    perspective motivates later discussions of algorithms and
    decidability.}
  \label{fr:7.1-03}
\end{frame}

% Frame 4: Primitive operations of a human computer
\begin{frame}[t]{Human computation primitives}
  \begin{tblock}{Core idea}
    A human computing by hand performs simple, discrete actions on a
    work tape: they \\examine the symbol under the current position,
    optionally \textit{rewrite} it with another symbol, and then \textit{move}
    the head one square to the left, to the right or keep it stationary.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item \emph{Examine}: read the symbol currently under the head to
        determine which rule applies.
      \item \emph{Rewrite}: replace that symbol with another symbol from
        the tape alphabet or leave it unchanged.
      \item \emph{Move}: shift the head one square left (L), right (R) or
        stay (S).  These three actions form the basis of Turing machine
        transitions.
    \end{itemize}
  \end{tblock}
  \note{This frame lays out the discrete building blocks from which the
    Turing machine is constructed.  Students should visualise a
    mechanical procedure executed one step at a time.}
  \label{fr:7.1-04}
\end{frame}

% Frame 5: Tape and alphabets
\begin{frame}[t]{Tape, alphabets and head}
  \begin{tblock}{Core idea}
    A Turing machine operates on a tape consisting of a doubly infinite
    strip of squares extending without bound to the right.  Two
    alphabets are distinguished: the \emph{input alphabet} $\Sigma$, which
    contains the characters that can appear in the input, and the
    \emph{tape alphabet} $\Gamma$, which contains $\Sigma$ plus a special
    blank symbol $\Box$ and any auxiliary markers used during the
    computation.  The tape head scans exactly one square at a time.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item The tape is infinite to the right but has a distinguished
        leftmost square; attempts to move left of this square cause a
        \emph{crash} into the reject state.
      \item The head always points to exactly one tape square, which
        contains a symbol from $\Gamma$; unvisited squares are implicitly
        blank.
      \item The machine begins scanning the leftmost symbol of the
        input; all squares to its right are initially blank.
    \end{itemize}
  \end{tblock}
  \note{Clarify that while the tape extends infinitely, at any
    finite time only a finite portion has been visited or rewritten.
    The blank symbol distinguishes unused areas from meaningful data.}
  \label{fr:7.1-05}
\end{frame}

% Frame 6: Single move semantics
\begin{frame}[t]{What happens in one move?}
  \begin{tblock}{Core idea}
    Each move of a Turing machine is fully determined by its current
    state and the symbol under the head.  A move comprises three
    simultaneous actions: replace the scanned symbol with another
    symbol, change to a new state, and move the head one square left,
    right or remain stationary.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item The machine reads $(q,a)$ – current state $q$ and scanned
        symbol $a$.
      \item It consults its transition function $\delta$ and writes
        $a'$ in place of $a$.
      \item It enters a new state $q'$ and moves the head according to
        $d \in \{L,R,S\}$.
    \end{enumerate}
  \end{tblock}
  \begin{talert}{Pitfalls}
    Forgetting that these three actions are atomic leads to confusion.
    For example, a machine cannot read a symbol on the left and then
    decide whether to overwrite; the replacement is part of the same
    instruction.
  \end{talert}
  \note{Stress that the transition function is partial; if no rule
    matches $(q,a)$, the machine rejects by crashing.}
  \label{fr:7.1-06}
\end{frame}

% Frame 7: Accepting, rejecting and looping
\begin{frame}[t]{Halting and looping}
  \begin{tblock}{Core idea}
    A Turing machine has two distinguished halting states: the accept
    state $h_a$ and the reject state $h_r$.  If execution ever enters
    $h_a$ or $h_r$, the machine stops.  A computation that never enters
    either halting state but continues to make moves forever is said to
    \emph{loop}.  Loops are neither accepts nor rejects.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Only two halting states are necessary because any other
        behaviour can be encoded as transitions that eventually enter
        $h_a$ or $h_r$.
      \item If $\delta$ is undefined for $(q,a)$, the machine crashes
        immediately into $h_r$; this models running off the left edge
        or encountering an unhandled symbol.
      \item A string is \emph{accepted} if the machine halts in $h_a$;
        it is \emph{rejected} if the machine halts in $h_r$; otherwise it
        \emph{loops} and the result is undefined.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls}
    It is tempting to treat looping as rejection, but for decision
    problems we require an explicit halting state to reject.  A machine
    that loops may fail to provide an answer at all.
  \end{talert}
  \note{Later sections will distinguish between languages that are
    recognisable (accepted by some TM) and those that are decidable
    (where the TM always halts).}
  \label{fr:7.1-07}
\end{frame}

% Frame 8: Formal definition of a Turing machine (Def 7.1)
\begin{frame}[t]{Definition of a Turing machine}
  \begin{tblock}{Core idea}
    A Turing machine is specified by a five‑tuple $(Q,\Gamma,\Sigma,\delta,q_0)$
    where $Q$ is a finite set of states containing two special halting
    states $h_a$ and $h_r$, $\Gamma$ is the tape alphabet, $\Sigma\subseteq
    \Gamma \setminus \{\Box\}$ is the input alphabet, $\delta$ is a partial
    transition function, and $q_0$ is the start state.  The blank
    symbol $\Box$ is in $\Gamma$ but not in $\Sigma$.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item The transition function $\delta$ maps each non‑halting
        pair $(q,a)$, where $q\in Q\setminus\{h_a,h_r\}$ and $a\in \Gamma$, to
        a triple $(q',a',d)$ where $q'\in Q$, $a'\in \Gamma$ and
        $d\in\{L,R,S\}$.
      \item If $\delta(q,a)$ is undefined, the machine crashes into
        $h_r$.  Halting states have no outgoing transitions.
    \end{itemize}
  \end{tblock}
  \note{Present this definition slowly, emphasising that the
    transition function may be partial and that the blank symbol and
    halting states are treated specially.}
  \label{fr:7.1-08}
\end{frame}

% Frame 9: Reading transitions and undefined moves
\begin{frame}[t]{Interpreting transitions}
  \begin{tblock}{Core idea}
    The triple $(q',a',d)=\delta(q,a)$ instructs the machine: while in
    state $q$ scanning symbol $a$, overwrite $a$ with $a'$, move the head
    according to $d$ and enter state $q'$.  If no such triple exists
    for $(q,a)$, the machine transitions immediately to $h_r$.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Rewriting $a$ to $a'$ allows the machine to mark or erase
        information on the tape.
      \item Movement direction $d$ may be left (L), right (R) or stay
        (S).  Attempting to move left off the leftmost square causes a
        crash into $h_r$.
      \item Halting transitions $\delta(h_a,a)$ and $\delta(h_r,a)$ are
        undefined; once a halting state is entered, the machine stops.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls}
    Forgetting to define behaviour for some $(q,a)$ will implicitly
    reject any string leading to that configuration.  When designing
    machines, take care to handle all relevant symbols or deliberately
    crash if an unexpected symbol appears.
  \end{talert}
  \label{fr:7.1-09}
\end{frame}

% Frame 10: Diagrams and left‑edge crash
\begin{frame}[t]{Diagrams and crash behaviour}
  \begin{tblock}{Core idea}
    Turing machine transitions are often drawn as labelled edges in a
    state diagram.  An edge from state $p$ to state $q$ labelled
    $a\;\rightarrow\;b,d$ means that while in $p$ scanning $a$ the machine
    writes $b$, moves the head in direction $d$ and enters $q$.
    Undefined transitions or moves that would leave the tape on the
    left crash the machine into $h_r$.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Each node in the diagram represents a state in $Q$;
        halting states are often denoted by double circles or explicit
        labels.
      \item If a transition is not drawn for a particular symbol, it is
        understood to lead to $h_r$ by default.
      \item Moving left from the leftmost tape square is prohibited;
        instead of raising an exception, the model treats this as an
        immediate reject.
    \end{itemize}
  \end{tblock}
  \centering
  \includegraphics[width=.7\linewidth]{sec7-1-schematic.png} % TODO: screenshot p. 227–228
  \note{Use the schematic to illustrate state diagrams and the crash
    behaviour.  Emphasise that the blank portion of the tape is not
    explicitly depicted.}
  \label{fr:7.1-10}
\end{frame}

% Frame 11: Configurations and representations
\begin{frame}[t]{Configuration strings}
  \begin{tblock}{Core idea}
    A \emph{configuration} captures the entire instantaneous description
    of a Turing machine: the tape contents, the head position and the
    current state.  It is often written as $x q y$, where $x$ is the
    portion of the tape to the left of the head, $q$ is the current
    state, and $y$ is the infinite suffix starting at the head.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item If the head is scanning the first symbol of $y$, then the
        configuration is $xqy$ with no separation mark; $x$ may be empty.
      \item In an alternative notation, one may write $x\sigma y$ to
        emphasise that $\sigma$ is the scanned symbol; the current state
        $q$ is then implicit in the context.
      \item The initial configuration on input string $w$ is
        $q_0\,\Box w$, with the head on the blank immediately to the
        left of $w$.
    \end{itemize}
  \end{tblock}
  \note{Help students practice reading and writing configuration
    strings.  They must be comfortable translating between diagrams and
    symbolic representations.}
  \label{fr:7.1-11}
\end{frame}

% Frame 12: The one‑step and multi‑step relations
\begin{frame}[t]{Computational relation}
  \begin{tblock}{Core idea}
    We write $C \Rightarrow D$ to mean that configuration $D$ follows
    configuration $C$ by applying one transition of the Turing machine.
    The reflexive, transitive closure of this relation is written as
    $\Rightarrow^*$ and denotes any finite sequence of moves.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item If $C = xqay$ and $\delta(q,a)=(q',a',d)$, then:
        \begin{enumerate}
          \item If $d=R$, then $C \Rightarrow x a' q' y$ (head moves right).
          \item If $d=L$ and $x=zb$, the head moves left to the last
            symbol of $x$; the configuration becomes $z q' b a' y$.
          \item If $d=S$, the head stays: $C \Rightarrow x q' a' y$.
        \end{enumerate}
      \item If $\delta(q,a)$ is undefined or $d=L$ and $x$ is empty,
        then $C \Rightarrow h_r y$; the machine crashes and the rest of
        the tape is ignored.
    \end{itemize}
  \end{tblock}
  \note{Encourage students to simulate a few steps on a simple
    machine to solidify these update rules.}
  \label{fr:7.1-12}
\end{frame}

% Frame 13: Halting semantics and looping
\begin{frame}[t]{Halting semantics revisited}
  \begin{tblock}{Core idea}
    A computation halts when a configuration containing $h_a$ or $h_r$ is
    reached; beyond this point no further transitions are defined.
    A computation that never halts is called a loop.  Halting is
    essential for decidability; a looping machine may recognise but
    cannot decide a language.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item If there exists some configuration $C$ such that
        $\text{init}(w) \Rightarrow^* C$ and $C$ contains $h_a$, then
        $w$ is accepted.
      \item If no such configuration exists but $\text{init}(w)\Rightarrow^*
        C$ for some $C$ containing $h_r$, then $w$ is rejected.
      \item If neither $h_a$ nor $h_r$ is ever reached, the machine
        loops; the decision problem remains unanswered.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls}
    Students often equate looping with rejection.  Stress that for
    decision procedures we require explicit halting behaviour: both
    acceptance and rejection must be indicated by entering $h_a$ or
    $h_r$.
  \end{talert}
  \label{fr:7.1-13}
\end{frame}

% Frame 14: Quick check – classify configurations
\begin{frame}[t]{Exercise: classify configurations}
  \begin{tblock}{Task}
    Consider the following configuration strings for some Turing machine $T$:
    \begin{enumerate}
      \item $\mathrel{\Box}\,q_1\,111\Box\Box\dots$.
      \item $1q_2\Box\Box\dots$.
      \item $101q_3 1\Box\dots$.
    \end{enumerate}
    For each configuration, decide whether it is an initial
    configuration, an intermediate configuration or a halting
    configuration.  Explain your reasoning.
  \end{tblock}
  \begin{talert}{Hint}
    Check which state symbol appears, the position of the head and
    whether a halting state $h_a$ or $h_r$ is present.  Remember that
    the blank symbol indicates unvisited squares.
  \end{talert}
  \note{This exercise helps students practise reading configuration
    strings and distinguishing different phases of a computation.}
  \label{fr:7.1-14}
\end{frame}

% Frame 15: Quick check – does this move halt?
\begin{frame}[t]{Exercise: does a move halt?}
  \begin{tblock}{Task}
    Suppose $T$ is in configuration $C = q\Box x$ (the head is on the
    blank just to the left of the input $x$) and $\delta(q,\Box) = (q',\Box,L)$.
    Does $T$ halt after this move?  If so, in which state?  If not,
    describe the resulting configuration.
  \end{tblock}
  \begin{talert}{Hint}
    Moving left from the leftmost square triggers a crash into $h_r$.
    Use the definition of the one‑step relation to justify your answer.
  \end{talert}
  \note{This question reinforces the special treatment of the left
    boundary of the tape.}
  \label{fr:7.1-15}
\end{frame}

% Frame 16: Quick check – when does $h_r$ occur?
\begin{frame}[t]{Exercise: when does $h_r$ occur?}
  \begin{tblock}{Task}
    Identify at least two distinct circumstances under which a Turing
    machine will enter the reject state $h_r$.  Provide a short example
    for each case.
  \end{tblock}
  \begin{talert}{Solution sketch}
    \begin{itemize}
      \item \emph{Undefined transition}: if $\delta(q,a)$ is not
        defined for some non‑halting state $q$ and scanned symbol $a$,
        then the machine rejects immediately.
      \item \emph{Left‑edge crash}: if the head tries to move left when
        it is already on the leftmost tape square, the machine crashes
        into $h_r$.
    \end{itemize}
    Other scenarios, such as explicit transitions to $h_r$, also
    indicate rejection.
  \end{talert}
  \note{Students should internalise these conditions to avoid
    inadvertently designing machines that loop instead of rejecting.}
  \label{fr:7.1-16}
\end{frame}

% Frame 17: Mini summary of Section 7.1
\begin{frame}[t]{Summary of Section 7.1}
  \begin{tblock}{Core idea}
    Section 7.1 introduces Turing machines as a general model of
    computation.  They are motivated by languages beyond the reach of
    pushdown automata and formalise the operations of a human computer.
    Turing machines operate on an infinite tape with a finite
    transition function and can halt in either an accept or reject
    state or loop forever.
  \end{tblock}
  \begin{tblock}{Key points}
    \begin{itemize}
      \item Primitive operations: examine, rewrite and move.
      \item Tape model: two alphabets, blank symbol and head; left
        boundary crash.
      \item Five‑tuple definition and transition semantics.
      \item Configurations, the one‑step relation and halting states.
      \item Distinction between acceptance, rejection and looping.
    \end{itemize}
  \end{tblock}
  \note{This summary emphasises the new terminology and prepares
    students for the next section, where Turing machines are used as
    language acceptors.}
  \label{fr:7.1-17}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 2: Languages outside PDAs – examples motivating the lemma
\begin{frame}[t]{Beyond the reach of PDAs}
  \begin{tblock}{Core idea}
    Several intuitive languages cannot be accepted by any pushdown
    automaton.  Among them are the ternary matching language
    $\{a^n b^n c^n \mid n\ge 0\}$, which would require the stack to
    compare three independent counts, and the duplication language
    $XX = \{xx \mid x \in \{a,b\}^*\}$, which demands that the second
    half of a string be identical to the first.  These examples
    motivate the search for a principled tool to show non‑context‑free
    behaviour.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Recall that a PDA can only compare two contiguous blocks.
      \item Observe that matching three counts or duplicating an
        arbitrary substring is beyond its capability.
      \item Introduce a lemma that every context‑free language must
        satisfy; failure of the lemma will prove a language is not
        context‑free.
    \end{enumerate}
  \end{tblock}
  \note{Encourage the audience to think of $a^n b^n c^n$ and $XX$ as
    “target” languages against which the pumping lemma will be applied.}
  \label{fr:6.1-02}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 3: Self‑embedding patterns in CFG derivations
\begin{frame}[t]{Self‑embedding in context‑free derivations}
  \begin{tblock}{Core idea}
    The analogue of “repeating a state” in a finite automaton is
    “repeating a variable” in a derivation tree.  If a derivation is
    long enough, some nonterminal must reappear along a path from the
    root to a leaf.  This repetition allows us to cut out or duplicate
    portions of the derivation, yielding an infinite family of
    derivations from a single pattern.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Consider a derivation $S \Rightarrow^* v A z$ in which
        $A$ later expands to $wAy$ and then to $x$.
      \item The overall string is $u = v w x y z$.
      \item Replacing the subtree $A \Rightarrow^* x$ by $wAy$ an
        arbitrary number of times yields strings $v w^m x y^m z$.
    \end{enumerate}
  \end{tblock}
  \note{Draw a simple tree on the board showing $A$ appearing twice on
    the same branch; emphasise that $w$ and $y$ are the portions
    derived between the two occurrences.}
  \label{fr:6.1-03}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 4: Preparing the grammar and bounding derivations
\begin{frame}[t]{Preparatory steps: CNF and tree height}
  \begin{tblock}{Core idea}
    To control the shape of derivation trees, we convert our grammar $G$
    to Chomsky normal form (CNF): each production has either a single
    terminal or a pair of variables on the right.  CNF eliminates
    $\varepsilon$‑productions and unit productions, ensuring that
    derivation trees are binary and of bounded height relative to the
    length of the derived string.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item A binary tree of height $h$ has at most $2^h$ leaves.
      \item If $u$ is derived in $G$ and its derivation tree has height
        $h$, then $|u| \le 2^h$.
      \item Let $p$ be the number of nonterminals in $G$; choose
        $n = 2^{p+1}$.  Any string $u \in L(G)$ with $|u| \ge n$ must
        have a derivation tree of height at least $p+1$, forcing a
        repeated variable along some branch.
    \end{itemize}
  \end{tblock}
  \note{Explain that $p$ counts distinct variables, so a path with
    $p+1$ interior nodes must repeat one of them by the pigeonhole
    principle.}
  \label{fr:6.1-04}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 5: Statement of Theorem 6.1 (Pumping Lemma for CFLs)
\begin{frame}[t]{Theorem 6.1 – Pumping Lemma for CFLs}
  \begin{tblock}{Core idea}
    If $L$ is a context‑free language, there exists an integer $n>0$
    (depending only on $L$) such that every string $u \in L$ with
    $|u| \ge n$ can be decomposed as $u = v w x y z$ with the following
    properties:
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{enumerate}
      \item \textbf{Non‑null pump:} $|w y| > 0$ ensures that at least
        one of $w$ or $y$ is non‑empty.
      \item \textbf{Bounded pumping length:} $|w x y| \le n$ limits the
        region that can be pumped to a substring of length at most $n$.
      \item \textbf{Pumping property:} for every $m \ge 0$, the string
        $v w^m x y^m z$ is also in $L$.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Proof idea}
    Replace $L$ by $L\setminus\{\varepsilon\}$ and choose a CNF
    grammar $G$ generating it.  A sufficiently long derivation in $G$
    must revisit some nonterminal $A$ along a branch.  Let $w$ and $y$
    be the substrings derived between the two occurrences of $A$, and let
    $x$ be the string derived from the second occurrence.  Pumping the
    subtree labelled $A$ yields all strings $v w^m x y^m z$.
  \end{tblock}
  \note{Stress that $n$ depends only on the grammar; once $n$ is fixed,
    the lemma applies uniformly to all long enough strings.}
  \label{fr:6.1-05}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 6: Condition (1) – ensuring non‑trivial pumping
\begin{frame}[t]{Interpreting condition (1)}
  \begin{tblock}{Core idea}
    The requirement $|w y| > 0$ prevents both $w$ and $y$ from being
    empty.  Without it, the pumping lemma would hold trivially for every
    language (we could take $w$ and $y$ empty and do nothing).  In
    derivation‑tree terms, $w$ and $y$ correspond to branches on either
    side of the second occurrence of $A$; at least one branch must
    produce a terminal.
  \end{tblock}
  \begin{talert}{Pitfalls / Quantifiers}
    Do not assume that exactly one of $w$ or $y$ is non‑empty; it is
    possible that both contain symbols.  When proving a language is not
    context‑free, you must consider \emph{all} valid decompositions,
    including those where $w$ and $y$ overlap different parts of the
    string.
  \end{talert}
  \note{Use this condition to eliminate decompositions where pumping
    nothing would trivially satisfy the lemma.  At least one symbol must
    participate in the pump.}
  \label{fr:6.1-06}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 7: Condition (2) – bounding the pump region
\begin{frame}[t]{Interpreting condition (2)}
  \begin{tblock}{Core idea}
    The bound $|w x y| \le n$ localises the pumping region to a
    substring of limited length.  This comes from the fact that the
    subtree rooted at the higher occurrence of $A$ has height at most
    $p+1$, so its yield cannot exceed $2^{p+1}=n$ symbols.  Hence $w x
    y$—the portion of $u$ controlled by the repeated nonterminal—must
    lie within this bound.
  \end{tblock}
  \begin{talert}{Pitfalls / Quantifiers}
    Unlike the regular pumping lemma, the pumped region need not lie
    within the prefix of length $n$; it can appear anywhere as a
    contiguous block of at most $n$ symbols.  Thus you must be prepared
    to consider decompositions where $w x y$ appears in the middle or
    toward the end of $u$.
  \end{talert}
  \note{Condition (2) often forces us to split our analysis into cases
    depending on which symbol groups $w x y$ overlaps.}
  \label{fr:6.1-07}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 8: Condition (3) – the pumping property
\begin{frame}[t]{Interpreting condition (3)}
  \begin{tblock}{Core idea}
    The most powerful part of the lemma asserts that once a valid
    decomposition is found, we can pump the substrings $w$ and $y$ any
    number of times and remain in $L$.  In particular, choosing $m=0$
    removes $w$ and $y$, choosing $m=2$ duplicates them, and so on.
    This universality allows us to derive contradictions by
    lengthening or shortening certain parts of $u$ while preserving its
    membership in $L$ according to the assumption.
  \end{tblock}
  \begin{talert}{Pitfalls / Quantifiers}
    The integer $m$ is quantified universally: for \emph{every} $m \ge
    0$, the pumped string must belong to $L$.  When constructing a
    contradiction, it is sufficient to find a single $m$ (often $0$ or
    $2$) that leads to an invalid string.
  \end{talert}
  \note{Encourage students to experiment with $m=0$ and $m=2$ when
    applying the lemma to specific languages; these cases often break
    symmetry or parity conditions.}
  \label{fr:6.1-08}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 9: Visualising the pump – Figure 6.2
\begin{frame}[t]{Figure 6.2 – Derivation tree for pumping}
  \centering
  \includegraphics[width=.8\linewidth]{fig-6-2.png}
  \begin{tblock}{Interpretation}
    The derivation tree shows a branch where the nonterminal $A$
    appears twice.  The substring $x$ is derived from the lower
    occurrence, while $w$ and $y$ come from the left and right siblings
    of the upper occurrence.  Pumping corresponds to repeating the
    pattern of subtrees beneath $A$.
  \end{tblock}
  \note{Point out the labels $v,w,x,y,z$ beneath the tree and relate
    them to the five pieces in Theorem 6.1.  Observe that $w$ and $y$
    may each be empty, but not simultaneously.}
  \label{fr:6.1-09}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 10: Strategy for using the pumping lemma
\begin{frame}[t]{Strategy for proving non‑CFLs}
  \begin{tblock}{Core idea}
    To show a language $L$ is not context‑free using Theorem 6.1, we
    argue by contradiction.  Assume $L$ is a CFL and let $n$ be the
    pumping length given by the lemma.  We then choose a string $u \in L$
    with $|u|\ge n$ expressed in terms of $n$.  The goal is to show that
    \emph{every} valid decomposition $u=v w x y z$ leads to a pumped
    string $v w^m x y^m z$ that violates membership in $L$ for some
    $m\ge 0$.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Select $u=u(n)$ that satisfies all structural conditions of
        $L$.
      \item Analyse all possible placements of $w x y$ of length at
        most $n$ within $u$; these give rise to different cases.
      \item For each case, find a specific value of $m$ (often $0$ or
        $2$) such that the pumped string is not in $L$.
    \end{enumerate}
  \end{tblock}
  \begin{talert}{Pitfalls / Quantifiers}
    Do not be tempted to pick one decomposition and pump it; the lemma
    guarantees only that some decompositions satisfy the conditions.  To
    obtain a contradiction you must show that no valid decomposition
    survives pumping.
  \end{talert}
  \note{During proofs, clearly state the cases based on where $w x y$
    lies within $u$ and justify why no other cases exist.}
  \label{fr:6.1-10}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 11: Example 6.3 – Introduction and setup
\begin{frame}[t]{Example 6.3 – $a^n b^n c^n$ is not context‑free}
  \begin{texample}{Example 6.3: setup}
    Suppose $L = \{ a^n b^n c^n \mid n \ge 0 \}$.  For contradiction,
    assume $L$ is a CFL and let $n$ be the pumping length from
    Theorem 6.1.  Choose the witness string $u = a^n b^n c^n$.  Since
    $|u| = 3n \ge n$, the lemma provides a decomposition $u = v w x y z$
    satisfying conditions (1)–(3).
  \end{texample}
  \begin{tblock}{Proof idea}
    We exploit the fact that $w x y$ contains at most two distinct
    symbols (because $|w x y| \le n$), while the full string contains
    three blocks of $a$’s, $b$’s and $c$’s.  Removing or duplicating
    $w$ and $y$ will unbalance the counts of these symbols.
  \end{tblock}
  \note{Ask students why $w x y$ cannot cover all three symbol types:
    its length is at most $n$, but each block of $a$, $b$, $c$ in $u$
    has length $n$.}
  \label{fr:6.1-11}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 12: Example 6.3 – Case analysis and contradiction
\begin{frame}[t]{Example 6.3 – Case analysis}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Since $|w y|>0$, $w x y$ must overlap at least one of the
        three blocks $a^n$, $b^n$, or $c^n$ but not all three.  Thus
        $w y$ contains symbols from at most two of these blocks.
      \item Let $\sigma_1$ be a symbol occurring in $w y$ and
        $\sigma_2$ be a symbol not occurring in $w y$.  Pumping down
        ($m=0$) deletes $w$ and $y$, producing $v w^0 x y^0 z$ with
        fewer than $n$ occurrences of $\sigma_1$ but still $n$
        occurrences of $\sigma_2$.
      \item This pumped string cannot remain in $L$, because condition
        (3) would require equal numbers of $a$, $b$, and $c$.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls / Quantifiers}
    Although pumping up ($m>1$) also gives a contradiction, pumping
    down is often simpler.  Ensure that the chosen $m$ breaks the
    equality of counts for \emph{any} valid placement of $w x y$.
  \end{talert}
  \note{Highlight that this argument simultaneously covers all cases:
    whichever two blocks $w x y$ touches, pumping alters the balance
    among the three counts.}
  \label{fr:6.1-12}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 13: Example 6.3 – Implications and generalisations
\begin{frame}[t]{Example 6.3 – Broader implications}
  \begin{tblock}{Core idea}
    The argument shows more than just $a^n b^n c^n$ is not context‑free.
    It also applies to the larger language $\{ t \in \{a,b,c\}^* \mid
    n_a(t) = n_b(t) = n_c(t) \}$, because pumping unbalances the counts
    of at least one symbol.  Whenever a language demands equal numbers
    of three distinct symbols, the pumping lemma provides a decisive
    contradiction.
  \end{tblock}
  \begin{tblock}{Proof idea}
    We did not use the fact that $u$ consists of three contiguous
    blocks—only that each symbol occurs equally often.  Therefore the
    pumped string $v w^0 x y^0 z$ always falls outside this larger set
    because at least one symbol count differs.
  \end{tblock}
  \note{Encourage students to identify other languages where three‑way
    equality is required and to apply a similar pumping argument.}
  \label{fr:6.1-13}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 14: Example 6.4 – Setup and decomposition cases
\begin{frame}[t]{Example 6.4 – Duplicated strings $XX$}
  \begin{texample}{Example 6.4: setup}
    Let $XX = \{ x x \mid x \in \{a,b\}^* \}$.  Assume $XX$ is a
    context‑free language and let $n$ be its pumping length.  Choose
    $u = a^n b^n a^n b^n$ (a string in $XX$ of length $4n$).  By the
    lemma, $u$ can be written $v w x y z$ with $|w y|>0$ and
    $|w x y| \le n$.
  \end{texample}
  \begin{tblock}{Roadmap}
    Because $|w x y| \le n$, the pumped region overlaps at most two of
    the four contiguous blocks ($a^n$, $b^n$, $a^n$, $b^n$).  We must
    examine four principal cases depending on which blocks contain
    symbols of $w y$:
    \begin{enumerate}
      \item $w y$ contains an $a$ from the first block.
      \item $w y$ contains a $b$ from the first block but no $a$ from
        that block.
      \item $w y$ lies entirely in the second half and contains at
        least one $a$.
      \item $w y$ lies entirely in the second half and contains only
        $b$’s.
    \end{enumerate}
  \end{tblock}
  \note{Emphasise that other subcases (e.g. symmetry between the
    two halves) reduce to these by relabeling $a$ and $b$.}
  \label{fr:6.1-14}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 15: Example 6.4 – Case 1: first $a$ block
\begin{frame}[t]{Example 6.4 – Case 1: first $a$ block}
  \begin{tblock}{Core idea}
    Suppose $w y$ contains at least one $a$ from the first block
    $a^n$.  Because $|w x y| \le n$, $w y$ cannot extend into the
    second half of $u$.  Pumping down ($m=0$) removes all such
    occurrences.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Write $u = a^n b^n a^n b^n$.  After deleting $w$ and $y$ the
        pumped string becomes $v x z = a^i b^j a^n b^n$ for some
        $i< n$ and $j \le n$.
      \item If $i \ne j$ or $j<n$, then $|v x z|$ is either odd or its
        midpoint falls inside the middle $a^n$ block.  In both cases
        the first half of the string ends with an $a$, so $v x z$ is
        not of the form $x x$.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls / Quantifiers}
    It is tempting to think that $m=2$ might produce an easier
    contradiction, but in this case deletion already suffices.  Always
    choose the smallest $m$ that breaks the required symmetry.
  \end{talert}
  \note{Visualise the midpoint of $v x z$; it cannot align with the
    boundary between the two halves once $w y$ is removed.}
  \label{fr:6.1-15}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 16: Example 6.4 – Case 2: first $b$ block
\begin{frame}[t]{Example 6.4 – Case 2: first $b$ block}
  \begin{tblock}{Core idea}
    Now assume $w y$ contains no $a$’s from the first block but at
    least one $b$ from the subsequent $b^n$ block.  It may also
    contain $a$’s from the second half.  Again, deletion ($m=0$)
    produces a string whose midpoint lies inside a $b^i a^j$ block and
    therefore cannot be a perfect square.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item After pumping down, the string is $a^n b^i a^j b^n$ with
        $i<n$ and $j\le n$.
      \item If $i+j$ is odd, the string has odd length and thus cannot
        be split into two equal halves.  If $i+j$ is even, the midpoint
        lies somewhere in $b^i a^j$, forcing the first half to end in a
        $b$, contradicting the requirement that both halves be equal.
    \end{itemize}
  \end{tblock}
  \note{Explain that the crucial observation is where the midpoint
    falls: it cannot match the corresponding symbol in the second half.}
  \label{fr:6.1-16}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 17: Example 6.4 – Cases 3 and 4: second half
\begin{frame}[t]{Example 6.4 – Cases 3 and 4: second half}
  \begin{tblock}{Core idea}
    Finally, suppose $w y$ lies entirely within the second half of $u$.
    There are two subcases: (3) $w y$ contains at least one $a$; (4)
    $w y$ contains only $b$’s.  In both situations, pumping down
    ($m=0$) deletes a non‑empty portion of the second half without
    altering the first half, so the two halves cannot be identical.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item In case 3, $v x z = a^n b^n a^i b^n$ with $i < n$, so the
        second half starts with $a$ while the first half ends with $b$.
      \item In case 4, $v x z = a^n b^n a^n b^i$ with $i < n$, so the
        second half is shorter and ends earlier than the first half.
    \end{itemize}
  \end{tblock}
  \note{Emphasise that symmetry is broken no matter where $w y$ sits
    within the second half.}
  \label{fr:6.1-17}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 18: Example 6.4 – Consequences
\begin{frame}[t]{Example 6.4 – Consequences}
  \begin{tblock}{Core idea}
    By exhausting all possible placements of $w x y$ within $u$, we
    conclude that for every valid decomposition, pumping down yields a
    string that is not of the form $xx$.  Therefore $XX$ is not
    context‑free.  Similar arguments apply to related languages such as
    $\{a^i b^j b^i a^j \mid i \ge 0\}$ and $\{a^i b^i a^j b^j \mid i,j \ge 0\}$,
    which require mirroring across two distinct symbol types.
  \end{tblock}
  \begin{tblock}{Proof idea}
    In each case the impossibility of aligning two halves after
    pumping arises from an imbalance in one part of the string.  The
    pumping lemma provides a uniform way to demonstrate such
    asymmetries.
  \end{tblock}
  \note{Invite the audience to test other “palindrome‑like” languages
    using similar decompositions and case analyses.}
  \label{fr:6.1-18}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 19: Example 6.5 – Inequality language
\begin{frame}[t]{Example 6.5 – Inequality language}
  \begin{texample}{Example 6.5: setup}
    Let $L = \{ x \in \{a,b,c\}^* \mid n_a(x) < n_b(x) \text{ and }
    n_a(x) < n_c(x) \}$.  Assume $L$ is context‑free and let $n$ be the
    pumping length.  Choose $u = a^n b^{n+1} c^{n+1}$, which satisfies
    $n_a(u) = n$, $n_b(u) = n+1$, and $n_c(u) = n+1$.
  \end{texample}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Suppose $u = v w x y z$ satisfies conditions (1)–(3).  If
        $w y$ contains an $a$, it cannot contain a $c$ (because $|w x
        y| \le n$).  Pumping up with $m=2$ adds an extra copy of both
        $w$ and $y$, yielding at least $n+1$ occurrences of $a$ but
        exactly $n+1$ occurrences of $c$, contradicting $n_a < n_c$.
      \item Otherwise, $w y$ contains only $b$’s or $c$’s.  Pumping
        down with $m=0$ deletes $w$ and $y$, leaving exactly $n$
        $a$’s and at most $n$ $b$’s or $c$’s.  This violates $n_a <
        n_b$ or $n_a < n_c$.
    \end{itemize}
  \end{tblock}
  \note{This example illustrates that different cases may require
    different values of $m$ to obtain a contradiction.}
  \label{fr:6.1-19}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 20: Example 6.6 – Legal C programs are not a CFL
\begin{frame}[t]{Example 6.6 – Legal C programs are not a CFL}
  \begin{texample}{Example 6.6: setup}
    Consider the language of syntactically legal C programs, which
    requires that variables be declared before use.  We show that this
    language is not context‑free by applying the pumping lemma.  Let
    $n$ be the pumping length and choose
    \[
      u = \texttt{main()\{int\ } a^{n+1} \texttt{;}\ a^{n+1}\texttt{;}\ a^{n+1}\texttt{;}\}
    \]
    where $a^{n+1}$ denotes an identifier consisting of $n+1$ $a$’s.
    This rudimentary program declares an integer variable and then
    references it twice.
  \end{texample}
  \begin{tblock}{Roadmap}
    Any valid decomposition $u=v w x y z$ must satisfy $|w x y| \le n$.
    We analyse where $w y$ can lie:
    \begin{enumerate}
      \item $w y$ overlaps the header (\texttt{main()}) or the braces.
      \item $w y$ lies in the declaration \texttt{int\ }$a^{n+1}$.
      \item $w y$ spans one or more of the semicolons separating the
        identifiers.
      \item $w y$ sits entirely within one identifier $a^{n+1}$.
    \end{enumerate}
    In every case, pumping down ($m=0$) produces an invalid program
    because either the syntax is broken or identifier matching fails.
  \end{tblock}
  \note{Stress that context‑free grammars cannot enforce the
    declaration‑before‑use constraint when the number of occurrences
    grows arbitrarily.}
  \label{fr:6.1-20}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 21: Example 6.6 – Case analysis
\begin{frame}[t]{Example 6.6 – Cases and contradictions}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item \textbf{Header/braces:} If $w y$ includes any part of
        \texttt{main()} or the opening/closing braces, then removing
        $w$ and $y$ leaves a string without a proper header or without
        matching braces, which is not a legal C program.
      \item \textbf{Declaration:} If $w y$ overlaps the declaration
        \texttt{int\ }$a^{n+1}$, then $v x z$ lacks a complete type
        declaration or leaves only a fragment of the keyword
        \texttt{int}, so the identifier is misinterpreted.
      \item \textbf{Semicolons:} If $w y$ contains a semicolon, then
        deleting it results in an extra or missing statement terminator,
        causing one identifier to be used without being declared or
        leaving an unmatched group of $a$’s.
      \item \textbf{Within an identifier:} If $w y$ sits strictly
        inside one $a^{n+1}$, pumping down shortens that identifier
        relative to the others.  The program now contains three
        different names, violating the requirement that each use matches
        the declaration.
    \end{itemize}
  \end{tblock}
  \note{This example highlights how context‑sensitive dependencies
    (matching identifiers) exceed the power of PDAs.}
  \label{fr:6.1-21}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 22: Ogden’s lemma – motivation and statement
\begin{frame}[t]{Ogden’s Lemma – pinning down the pump}
  \begin{tblock}{Motivation}
    The pumping lemma sometimes fails to give enough control over the
    location of the pumped region.  Ogden’s lemma strengthens it by
    allowing us to \emph{distinguish} certain positions in the string and
    guarantee that the pumped portion includes at least one of these
    distinguished positions.
  \end{tblock}
  \begin{tblock}{Theorem 6.7 (Ogden’s lemma)}
    Let $L$ be a context‑free language.  There exists an integer $n>0$
    such that for any string $u \in L$ with $|u| \ge n$ and any choice
    of $n$ or more distinguished positions in $u$, there is a
    decomposition $u = v w x y z$ satisfying:
    \begin{enumerate}
      \item $w y$ contains at least one distinguished position.
      \item $w x y$ contains at most $n$ distinguished positions.
      \item For every $m \ge 0$, the pumped string $v w^m x y^m z$ is in
        $L$.
    \end{enumerate}
  \end{tblock}
  \note{Interpret “distinguished positions” as pins that mark where
    pumping must occur; we control the pump’s location.}
  \label{fr:6.1-22}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 23: Ogden’s lemma – Proof sketch
\begin{frame}[t]{Ogden’s Lemma – proof idea}
  \begin{tblock}{Proof idea}
    The proof follows the same blueprint as the pumping lemma but
    counts \emph{distinguished descendants} in the derivation tree.
    Starting from the root, we follow a path to a leaf, always choosing
    the child with more distinguished descendants.  Interior nodes where
    both children have distinguished descendants are branch points.
    Because there are more than $2^p$ distinguished leaf nodes, there
    must be more than $p$ branch points on the path, ensuring that some
    variable $A$ appears twice among these branch points.  This yields
    the decomposition and conditions stated in the theorem.
  \end{tblock}
  \begin{tblock}{Highlights}
    \begin{itemize}
      \item If a path has $h$ branch points, then at most $2^h$
        distinguished leaves descend from the root.  With more than
        $2^p$ distinguished positions, there are at least $p+1$ branch
        points.
      \item The two occurrences of $A$ among these branch points
        guarantee that $w y$ includes a distinguished position while the
        subtree rooted at the upper $A$ contributes at most $n$
        distinguished positions to $w x y$.
    \end{itemize}
  \end{tblock}
  \note{Emphasise that Ogden’s lemma reduces to the pumping lemma when
    every position is distinguished.}
  \label{fr:6.1-23}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 24: Example 6.8 – Setup and intuition
\begin{frame}[t]{Example 6.8 – Language $\{a^i b^i c^j \mid j \ne i\}$}
  \begin{texample}{Example 6.8: setup}
    Consider $L = \{ a^i b^i c^j \mid j \ne i \}$.  We prove $L$ is not
    context‑free using Ogden’s lemma.  Let $n$ be the integer from
    Theorem 6.7.  Choose $u = a^n b^n c^{n + n!}$ and distinguish the
    first $n$ positions (the $a$’s).  Assume $u = v w x y z$ satisfies
    conditions (1)–(3) of Ogden’s lemma.
  \end{texample}
  \begin{tblock}{Proof idea}
    The distinguished positions force the pump to include at least one
    $a$.  Depending on the composition of $w$ and $y$ we select a
    suitable $m$ to alter the counts of $a$, $b$ and $c$ so that the
    pumped string falls into the forbidden case $j = i$.
  \end{tblock}
  \note{Recall that ordinary pumping fails here because $w y$ could
    avoid all $a$’s; distinguishing the $a$’s prevents that.}
  \label{fr:6.1-24}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 25: Example 6.8 – Case analysis with distinguished positions
\begin{frame}[t]{Example 6.8 – Case analysis}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item If $w$ or $y$ contains two distinct symbols, pumping up with
        $m=2$ introduces an imbalance among $a$’s, $b$’s and $c$’s that
        cannot satisfy $j \ne i$.
      \item Suppose neither $w$ nor $y$ contains two different symbols.
        Because of condition (1), at least one of them is $a^p$ for
        some $p>0$.  We analyse four subcases:
        \begin{enumerate}
          \item $w=a^p$, $y=a^q$.
          \item $w=a^p$, $y=b^q$ with $p>0$ and $p \ne q$.
          \item $w=a^p$, $y=c^q$ with $p>0$.
          \item $w=a^p$, $y=b^p$ (the symmetric case).
        \end{enumerate}
      \item In the first three subcases, choose $m=2$ to increase or
        decrease the number of $a$’s relative to $b$’s or $c$’s,
        yielding $j = i$ in the pumped string.
      \item In the final subcase where $w=a^p$ and $y=b^p$, choose
        $m = k+1$ with $k = n!/p$.  Then $v w^m x y^m z$ contains
        $n + n!$ $a$’s but only $n$ $b$’s and $c$’s, so $j = i$ and the
        string falls outside $L$.
    \end{itemize}
  \end{tblock}
  \note{The factorial trick ensures that the extra $p$ $a$’s added by
    pumping align exactly with the $n!$ offset in the number of $c$’s.}
  \label{fr:6.1-25}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 26: Example 6.9 – Pumping lemma fails but Ogden works
\begin{frame}[t]{Example 6.9 – Pumping lemma fails but Ogden works}
  \begin{texample}{Example 6.9: language with optional prefix}
    Define $L = \{ a^p b^q c^r d^s \mid p=0 \text{ or } q=r=s \}$.  The
    language consists of strings either starting with no $a$ or with
    equal numbers of $b$, $c$ and $d$ following at least one $a$.
  \end{texample}
  \begin{tblock}{Why the pumping lemma alone fails}
    For any fixed $n$ and any long $u \in L$, one can always choose a
    decomposition $u = v w x y z$ satisfying Theorem 6.1 such that
    pumping preserves membership in $L$.  If $p=0$, take $w$ to be the
    first symbol; if $p>0$, let $w=a$ and $y$ and $x$ be empty.  No
    matter what $m$ is chosen, $v w^m x y^m z$ still satisfies the
    condition $p=0$ or $q=r=s$.
  \end{tblock}
  \begin{talert}{Pitfalls / Quantifiers}
    This example shows that satisfying the pumping lemma does not imply
    a language is context‑free.  The lemma is a one‑way implication: all
    CFLs pump, but some non‑CFLs may also appear to pump when poorly
    chosen decompositions are allowed.
  \end{talert}
  \note{Reinforce that the pumping lemma is a necessary but not
    sufficient condition for context‑freeness.}
  \label{fr:6.1-26}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 27: Example 6.9 – Applying Ogden’s lemma successfully
\begin{frame}[t]{Example 6.9 – Ogden’s lemma to the rescue}
  \begin{tblock}{Setup}
    Let $n$ be the integer from Ogden’s lemma.  Choose
    $u = a b^n c^n d^n$ and distinguish all positions of $u$ except the
    first.  Any valid decomposition $u = v w x y z$ must satisfy
    conditions (1)–(3) of Ogden’s lemma.  In particular, $w y$ must
    contain at least one of the $b$, $c$ or $d$ positions and cannot
    contain all three symbols.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Since $w y$ includes no $a$ (the only non‑distinguished
        position), pumping up with $m=2$ changes the counts of $b$, $c$
        and $d$ while leaving the single $a$ intact.
      \item The pumped string $v w^2 x y^2 z$ still begins with $a$ but
        no longer has equal numbers of $b$, $c$ and $d$.  Thus it
        violates the requirement $q=r=s$ and is not in $L$.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Conclusion}
    Ogden’s lemma succeeds where the ordinary pumping lemma fails: by
    forcing the pumped region to touch the distinguished portion, it
    guarantees that pumping alters the crucial part of the string.
  \end{talert}
  \note{This example demonstrates the practical value of marking
    distinguished positions to control the location of the pump.}
  \label{fr:6.1-27}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 28: Summary – when to use which lemma
\begin{frame}[t]{Summary – Pumping versus Ogden}
  \begin{tblock}{Checklist for using the pumping lemma}
    \begin{itemize}
      \item Choose $u = u(n)$ ensuring $|u| \ge n$ and reflecting the
        structure of the language.
      \item Bound $|w x y| \le n$ to identify which symbol blocks $w y$
        can overlap; split the analysis accordingly.
      \item Find an $m$ (often $0$ or $2$) that breaks a defining
        equality or order in the language for all valid decompositions.
    \end{itemize}
  \end{tblock}
  \begin{tblock}{When to invoke Ogden’s lemma}
    \begin{itemize}
      \item If the ordinary pumping lemma allows decompositions that
        avoid the crucial part of $u$, mark a set of distinguished
        positions to force $w y$ to include them.
      \item Use Ogden’s lemma for languages where a specific block must
        be pumped, such as $\{a^i b^i c^j \mid j \ne i\}$.
      \item Remember that both lemmas provide necessary but not
        sufficient conditions for context‑freeness.
    \end{itemize}
  \end{tblock}
  \note{Encourage students to view these lemmas as tools in a toolbox;
    choosing the right tool requires understanding the structure of the
    language.}
  \label{fr:6.1-28}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 29: Final remarks
\begin{frame}[t]{Concluding observations}
  \begin{tblock}{Core idea}
    The pumping lemma for context‑free languages extends the finite
    automaton intuition of “repeating a state” to “repeating a
    nonterminal.”  By examining the structure of derivation trees, we
    can prove that certain languages lie beyond the power of pushdown
    automata.  Ogden’s lemma further refines this method by allowing
    us to control where the pumping must occur.
  \end{tblock}
  \begin{tblock}{Take‑home messages}
    \begin{itemize}
      \item Not every non‑CFL fails the ordinary pumping lemma; some
        require the additional flexibility of Ogden’s lemma.
      \item When applying these lemmas, carefully define the witness
        string and systematically consider all valid decompositions.
      \item Understanding the limitations of PDAs and CFGs helps
        clarify the boundary between context‑free and non‑context‑free
        languages.
    \end{itemize}
  \end{tblock}
  \note{Finish by reminding students that these techniques will be
    invaluable when analysing closure properties and decision problems
    later in the course.}
  \label{fr:6.1-29}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 30: Comparing regular and context‑free pumping lemmas
\begin{frame}[t]{Regular vs. context‑free pumping lemmas}
  \begin{tblock}{Core idea}
    The pumping lemma for regular languages states that every long
    enough string can be written as $u = x y z$ with $|x y| \le n$ and
    $|y|>0$ such that $x y^m z \in L$ for all $m \ge 0$.  The pumped
    portion $y$ lies in the \textit{prefix} of length $n$.
  \end{tblock}
  \begin{tblock}{Contrast with Theorem 6.1}
    In the context‑free setting, the pumped region $w x y$ has length
    bounded by $n$, but it may occur anywhere in the string.  The
    decomposition has five parts $v w x y z$, and both $w$ and $y$ are
    pumped together.  These differences reflect the increased power of
    PDAs (they can postpone comparison) and the more complex
    combinatorial structures of CFLs.
  \end{tblock}
  \note{Use this slide to revisit Chapter 2 and remind students how
    similar techniques arise in different settings.}
  \label{fr:6.1-30}
\end{frame}

% -----------------------------------------------------------------------------
% Section 6.2 – Intersections and Complements of CFLs
%
% This section introduces closure properties of context‑free languages and
% demonstrates how intersections and complements interact with context‑free
% languages.  Examples 6.10–6.12 show that intersection can yield a
% non‑context‑free language and that the complement of a non‑CFL can be
% context‑free.  Theorem 6.13 establishes that the intersection of a CFL with
% a regular language is always a CFL via a product construction of a PDA and
% an FA.  Throughout we follow the Wildcat template: one frame per
% paragraph, structured boxes, and succinct speaker notes.

\section{Intersections and Complements of CFLs}

%-----------------------------------------------------------------------------
% Frame 1: Closure properties recap
\begin{frame}[t]{Closure properties recap}
  \begin{tblock}{Core idea}
    Context‑free languages enjoy many of the same closure properties as
    regular languages: they are closed under union, concatenation, and
    Kleene $\ast$.  However, unlike regular languages, the family of CFLs is
    \emph{not} closed under intersection or set difference.  This section
    reviews these facts and explores what happens when we take
    intersections and complements of CFLs.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Recall positive closure results for CFLs.
      \item Examine examples where intersections lead outside the class.
      \item Explore cases where complements of non‑CFLs are themselves
        context‑free.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item $\mathrm{CFL}$ is closed under $L_1 \cup L_2$, $L_1 L_2$, and
        $L^\ast$ by constructing a new grammar or PDA that nondeterministically
        chooses which sublanguage to generate.
      \item $\mathrm{CFL}$ is \emph{not} closed under $L_1 \cap L_2$ or
        $L_1 - L_2$; counterexamples will follow.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    Do not conflate closure under union with closure under intersection.
    Simply knowing that two languages are context‑free does not imply that
    their intersection is context‑free.  Likewise, the complement of a
    non‑context‑free language may still be a CFL.
  \end{talert}
  \note{Use this slide to remind students of the main closure results
    established earlier in the course.  Highlight that the failure of
    intersection closure differentiates CFLs from regular languages.}
  \label{fr:6.2-01}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 2: Example 6.10 – Intersection expressing $a^n b^n c^n$
\begin{frame}[t]{Example 6.10 — $a^n b^n c^n$ as an intersection}
  \begin{texample}{Intersection representation}
    The ternary matching language $A^n B^n C^n = \{a^n b^n c^n \mid n\ge 0\}$ is not a
    context‑free language (Example 6.3), yet it can be expressed as the
    intersection of two simpler languages:
    \[
      A^n B^n C^n
      = \{a^i b^j c^k \mid i=j \text{ and } j=k\}
      = \{a^i b^i c^k \mid i,k\ge 0\}\;\cap\;\{a^i b^j c^j \mid i,j\ge 0\}.
    \]
    Each factor enforces one equality of counts while allowing the other
    count to vary.
  \end{texample}
  \note{Introduce the two component languages and write down their
    definitions on the board.  Ask students to verify informally that
    strings in the intersection must have equal numbers of $a$’s, $b$’s,
    and $c$’s.}
  \label{fr:6.2-02}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 3: Example 6.10 – Proof idea and highlights
\begin{frame}[t]{Example 6.10 — Why the factors are CFLs}
  \begin{tblock}{Core idea}
    Each language in the intersection can be generated by a context‑free
    grammar or recognised by a pushdown automaton.  The first factor
    matches equal numbers of $a$’s and $b$’s and then allows any number
    of trailing $c$’s.  The second factor allows any number of leading
    $a$’s and then matches equal numbers of $b$’s and $c$’s.  Their
    intersection forces both equalities and thus yields $a^n b^n c^n$.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Decompose $A^n B^n C^n$ into two languages, each enforcing
        a single equality.
      \item Construct a CFG or PDA for $\{a^i b^i c^k\}$ by matching
        $a$’s with $b$’s and ignoring $c$’s.
      \item Construct a CFG or PDA for $\{a^i b^j c^j\}$ by ignoring
        $a$’s and matching $b$’s with $c$’s.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item For $L_1 = \{a^i b^i c^k \mid i,k\ge 0\}$, a PDA pushes one
        symbol onto the stack for each $a$, pops one for each $b$, and
        then accepts regardless of the number of $c$’s.
      \item For $L_2 = \{a^i b^j c^j \mid i,j\ge 0\}$, a PDA skips the
        initial $a$’s, pushes one symbol for each $b$, pops one for
        each $c$, and accepts when the stack empties.
      \item Both $L_1$ and $L_2$ are context‑free; nevertheless,
        $L_1 \cap L_2 = A^n B^n C^n$ is not.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    This example shows that even though each component language is
    context‑free, their intersection need not be.  Do not misinterpret
    this construction as evidence that CFLs are closed under
    intersection.
  \end{talert}
  \note{Emphasise that the failure of closure arises because the two PDAs
    would need to be simulated simultaneously, effectively requiring two
    stacks.}
  \label{fr:6.2-03}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 4: Example 6.10 – Constructing PDAs
\begin{frame}[t]{Example 6.10 — Constructing the component PDAs}
  \begin{tblock}{Core idea}
    Pushdown automata for the two component languages operate in
    complementary ways.  One counts $a$’s and $b$’s and then ignores
    $c$’s, while the other ignores $a$’s and counts $b$’s and $c$’s.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item For $L_1$, on input $a^i b^i c^k$, push a stack symbol for
        each $a$, pop one for each $b$, and accept regardless of the
        number of $c$’s encountered.
      \item For $L_2$, on input $a^i b^j c^j$, ignore the prefix
        $a^i$, push a stack symbol for each $b$, pop one for each $c$, and
        accept when the stack empties.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Both automata are nondeterministic PDAs with a single stack.
      \item The stack actions ensure equality of the relevant symbol
        counts, while nondeterminism or unconditional moves handle the
        unrestricted portion of the input.
    \end{itemize}
  \end{tblock}
  \note{Sketch the state diagrams of these PDAs on the board to show how
    each uses its stack.  The key point is that a single stack suffices
    when enforcing only one equality.}
  \label{fr:6.2-04}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 5: Example 6.11 – Complement of $XX$
\begin{frame}[t]{Example 6.11 — Complement of $XX$}
  \begin{texample}{Characterising $\overline{XX}$}
    Let $XX = \{xx \mid x \in \{a,b\}^*\}$ denote the duplication language,
    which is not context‑free.  Its complement in $\{a,b\}^*$ consists
    precisely of the strings of odd length together with those even‑length
    strings whose first and second halves are not identical.  Formally,
    \[
      \overline{XX}
      = \{x \in \{a,b\}^* \mid |x| \text{ is odd}\}
        \cup \{x \in \{a,b\}^* \mid |x| = 2n,\; \exists k \le n:\, x_k \ne x_{n+k}\}.
    \]
    For even length strings, there must exist a position $k$ in the first
    half where the symbol differs from the corresponding symbol in the
    second half.
  \end{texample}
  \note{Ask students to think of simple examples: $aba$ has odd length
    and is in the complement; $abba$ has a mismatch at the second
    position ($b \ne a$).}
  \label{fr:6.2-05}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 6: Example 6.11 – Decomposing mismatches
\begin{frame}[t]{Example 6.11 — Decomposing mismatched strings}
  \begin{tblock}{Core idea}
    Any even‑length string in $\overline{XX}$ can be seen as the
    concatenation of two odd‑length strings centred on the mismatched
    symbols.  This viewpoint leads to a grammar that generates
    $\overline{XX}$.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Let $x$ be an even‑length string of length $2n$ not in $XX$.
      \item Choose the smallest index $k$ with $1 \le k \le n$ such that
        the $k$th and $(n+k)$th symbols differ (say $a$ and $b$).
      \item View $x$ as a prefix of length $k-1$, followed by $a$, a
        middle portion of length $n-1$, then $b$, and a suffix.
      \item Reassociate the $n-1$ middle symbols as $(k-1)$ symbols
        before the $b$ and $(n-k)$ after the $b$, producing two
        odd‑length substrings.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Every mismatched position yields such a decomposition, and
        conversely any concatenation of two odd‑length strings with
        different middle symbols lies in $\overline{XX}$.
    \end{itemize}
  \end{tblock}
  \note{Draw two parallel halves and indicate the mismatch position.
    Point out how the middle symbols define two smaller odd‑length
    pieces.}
  \label{fr:6.2-06}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 7: Example 6.11 – Grammar for the complement
\begin{frame}[t]{Example 6.11 — A CFG for $\overline{XX}$}
  \begin{tblock}{Core idea}
    The complement of $XX$ is generated by a simple grammar that
    produces all odd‑length strings and their concatenations.  There are
    two nonterminals $A$ and $B$ generating odd strings with middle
    symbol $a$ or $b$, and a helper $E$ generating single symbols.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Productions: $S \rightarrow A \mid B \mid AB \mid BA$.
      \item $A \rightarrow E A E \mid a$ generates odd‑length strings
        whose middle symbol is $a$; $B \rightarrow E B E \mid b$ does the
        same with $b$.
      \item $E \rightarrow a \mid b$ generates any single terminal.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    This example shows that the complement of a non‑CFL may be a CFL.
    Do not infer from this that CFLs are closed under complement; they
    are not in general.  Complementation works in this particular case
    because we can describe $\overline{XX}$ with a CFG.
  \end{talert}
  \note{Go through the productions and generate a few short strings.
    Emphasise that $A$ and $B$ enforce a fixed middle symbol while
    growing outwards symmetrically.}
  \label{fr:6.2-07}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 8: Example 6.12 – Expressing $a^n b^n c^n$ with inequalities
\begin{frame}[t]{Example 6.12 — Three languages enforcing inequalities}
  \begin{texample}{Intersection via inequalities}
    Define three languages over $\{a,b,c\}$ as follows:
    \[
      L_1 = \{a^i b^j c^k \mid i \le j\},\quad
      L_2 = \{a^i b^j c^k \mid j \le k\},\quad
      L_3 = \{a^i b^j c^k \mid k \le i\}.
    \]
    Each $L_i$ is context‑free (pushdown automata can compare two
    counts), and their intersection is
    \[
      L_1 \cap L_2 \cap L_3 = \{a^n b^n c^n \mid n\ge 0\},
    \]
    which we know is not a CFL.  This presents another way of writing
    $a^n b^n c^n$ as the intersection of CFLs.
  \end{texample}
  \note{Ask students why each inequality can be enforced with a PDA: in
    $L_1$, push $a$’s and pop for $b$’s; in $L_2$, push $b$’s and pop
    for $c$’s; in $L_3$, push $c$’s and pop for $a$’s.}
  \label{fr:6.2-08}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 9: Example 6.12 – Union of complements
\begin{frame}[t]{Example 6.12 — Union of complements is a CFL}
  \begin{tblock}{Core idea}
    To see why the intersection of the $L_i$ is not context‑free, we
    rewrite each $L_i$ as a union of a regular language with a simpler
    CFL.  The complements of the $L_i$ then combine to give a CFL
    whose complement is not.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Let $R = a^* b^* c^*$; $R$ is regular.
      \item Show that $L_1' = R \cup \{a^i b^j c^k \mid i>j\}$, and
        similarly define $L_2'$ and $L_3'$.
      \item Each set $\{a^i b^j c^k \mid i>j\}$ is context‑free, so
        $L_i'$ is a CFL.
      \item The union $L_1' \cup L_2' \cup L_3'$ is a CFL whose
        complement equals $L_1 \cap L_2 \cap L_3$ and is not context‑free.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item By De Morgan’s laws, $(L_1 \cap L_2 \cap L_3)' = L_1' \cup
        L_2' \cup L_3'$.
      \item Closure of CFLs under union and under concatenation with a
        regular filter implies $L_1' \cup L_2' \cup L_3'$ is a CFL.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    Even though $L_1' \cup L_2' \cup L_3'$ is a CFL, its complement
    $L_1 \cap L_2 \cap L_3$ is not.  Do not mistake the closure of
    complements of sublanguages for closure of the original languages.
  \end{talert}
  \note{Draw a Venn diagram illustrating the relationship between
    intersections and unions of complements.}
  \label{fr:6.2-09}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 10: Example 6.12 – Alternate formulation via $A_i$
\begin{frame}[t]{Example 6.12 — Alternate formulation without $R$}
  \begin{tblock}{Core idea}
    Another way to express the same phenomenon is to drop the regular
    language $R$ and work directly with inequalities on symbol counts.
    Define
    \[
      A_1 = \{x \mid n_a(x) \le n_b(x)\},\quad
      A_2 = \{x \mid n_b(x) \le n_c(x)\},\quad
      A_3 = \{x \mid n_c(x) \le n_a(x)\}.
    \]
    The intersection $A_1 \cap A_2 \cap A_3$ consists of all strings
    with equal numbers of $a$’s, $b$’s, and $c$’s, which is not a CFL.
    Therefore the union $A_1 \cup A_2 \cup A_3$ is a CFL whose
    complement is not.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Each $A_i$ is context‑free: a PDA can compare the two
        relevant counts by pushing and popping.
      \item By closure under union, $A_1 \cup A_2 \cup A_3$ is a CFL.
      \item Its complement $A_1^c \cap A_2^c \cap A_3^c$ enforces
        simultaneous equalities and is not context‑free.
    \end{itemize}
  \end{tblock}
  \note{This version avoids the regular filter $R$ and deals purely
    with comparisons of counts.  It highlights how a union of
    inequalities can be context‑free even when the common equality is
    not.}
  \label{fr:6.2-10}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 11: Product construction for FAs
\begin{frame}[t]{Product construction for finite automata}
  \begin{tblock}{Core idea}
    Finite automata are closed under union, intersection, and difference
    via the Cartesian‑product construction.  Given FAs $M_1$ and $M_2$, we
    construct a new FA whose states are pairs $(p,q)$ and whose
    acceptance condition is defined according to the desired operation.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item For union, a state $(p,q)$ is accepting if $p$ or $q$ is
        accepting.
      \item For intersection, $(p,q)$ is accepting if both $p$ and $q$
        are accepting.
      \item For difference $L(M_1) - L(M_2)$, $(p,q)$ is accepting if
        $p$ is accepting and $q$ is not.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item The transition function on input symbol $\sigma$ maps
        $(p,q)$ to $(\delta_1(p,\sigma),\delta_2(q,\sigma))$.
      \item Because finite automata have no auxiliary memory besides
        their state, the pair construction suffices to simulate both
        machines simultaneously.
    \end{itemize}
  \end{tblock}
  \note{Refer back to Section 2.2 where this construction was first
    introduced.  This slide sets up the contrast with PDAs.}
  \label{fr:6.2-11}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 12: Why the product of two PDAs fails
\begin{frame}[t]{Why a product of two PDAs fails}
  \begin{tblock}{Core idea}
    A pushdown automaton’s configuration consists of a state and a stack
    contents.  Attempting to simulate two independent PDAs by storing
    both states in a pair $(p_1,p_2)$ does not solve the problem of
    managing two stacks.  A single stack cannot faithfully emulate two
    independent stacks; this is why CFLs are not closed under
    intersection in general.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Observe that the state of a PDA encodes only finite
        information; the unbounded information is in the stack.
      \item When simulating two PDAs, we would need to keep track of
        two stacks’ contents — impossible with just one stack.
      \item Hence the cross‑product construction does not extend from
        finite automata to arbitrary PDAs.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item The configuration space of the product of two PDAs requires
        pairs of stacks $(\alpha,\beta)$; but a PDA only has a single
        stack.
      \item There is no general way to interleave stack operations on
        $\alpha$ and $\beta$ without losing the ability to recover each
        independently.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    Do not attempt to simulate two stacks by encoding one on top of
    another or by using special markers; such tricks cannot in general
    maintain both stacks’ LIFO order.  This is why the intersection of
    CFLs may be non‑context‑free.
  \end{talert}
  \note{Relate this insight to Example 6.10: simulating both PDAs for
    $\{a^i b^i c^k\}$ and $\{a^i b^j c^j\}$ would require two stacks.}
  \label{fr:6.2-12}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 13: Product of an FA and a PDA
\begin{frame}[t]{Product construction: FA × PDA}
  \begin{tblock}{Core idea}
    If one of the two machines is a finite automaton (which has no
    stack), we can form the Cartesian product of its states with the
    states of a PDA to recognise the intersection with a regular
    language.  The resulting machine uses the single stack of the PDA
    component, and the FA component merely records which state we
    should be in after reading the input.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Let $M_1$ be a PDA for $L_1$ and $M_2$ an FA for $L_2$.
      \item Build a new PDA $M$ whose states are pairs $(p,q)$ with
        $p \in Q_1$ and $q \in Q_2$.
      \item On each input symbol, update both components: follow
        $M_1$’s transition and update the FA state using its transition
        function.
      \item Use the stack actions of $M_1$ unchanged; ignore stack in
        the FA component.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item The starting state is $(q_1,q_2)$; accepting states are
        $A_1 \times A_2$.
      \item When $M_1$ makes an $\varepsilon$‑move, $M$ does likewise
        without changing the FA state.
      \item This construction underlies Theorem 6.13.
    \end{itemize}
  \end{tblock}
  \note{Emphasise that the FA component contributes no stack, so one
    stack suffices.  Contrast with the failed attempt to simulate two
    PDAs.}
  \label{fr:6.2-13}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 14: Theorem 6.13 — Statement
\begin{frame}[t]{Theorem 6.13 — CFL ∩ Regular is CFL}
  \begin{tblock}{Core idea}
    If $L_1$ is a context‑free language and $L_2$ is a regular language,
    then their intersection $L_1 \cap L_2$ is also a context‑free
    language.  The proof constructs a new PDA that simultaneously
    simulates a PDA for $L_1$ and an FA for $L_2$, using only one
    stack.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Let $M_1$ be a PDA accepting $L_1$ and $M_2$ an FA
        accepting $L_2$.
      \item Build $M$ whose state set is the Cartesian product of
        $M_1$’s and $M_2$’s state sets.
      \item $M$ accepts an input iff both $M_1$ and $M_2$ accept it.
    \end{itemize}
  \end{tblock}
  \note{State the theorem clearly before delving into the details of the
    construction.  Point out that regular languages serve as filters on
    CFLs.}
  \label{fr:6.2-14}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 15: Theorem 6.13 — Construction details
\begin{frame}[t]{Theorem 6.13 — Construction details}
  \begin{tblock}{Core idea}
    The transitions of the product PDA reflect those of the original PDA
    and the FA.  On an input symbol, $M$ updates both components; on an
    $\varepsilon$‑move of the PDA, $M$ updates only the PDA component.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item States: $Q = Q_1 \times Q_2$.
      \item Start state: $q_0 = (q_1,q_2)$.
      \item Accepting states: $A = A_1 \times A_2$.
      \item Define the transition relation $\delta$ as follows:
        \begin{enumerate}
          \item For each input symbol $\sigma$ and stack symbol $Z$,
            $\delta((p,q),\sigma,Z)$ contains $((p',q'),\alpha)$ for
            each $(p',\alpha) \in \delta_1(p,\sigma,Z)$ with
            $\delta_2(q,\sigma) = q'$.
          \item For each stack symbol $Z$, $\delta((p,q),\varepsilon,Z)$
            contains $((p',q),\alpha)$ for each $(p',\alpha) \in
            \delta_1(p,\varepsilon,Z)$.
        \end{enumerate}
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item $M$ uses the same stack symbols and operations as $M_1$;
        the FA component does not affect the stack.
      \item $M$ is nondeterministic if $M_1$ is; its nondeterminism is
        confined to the PDA component.
    \end{itemize}
  \end{tblock}
  \note{Encourage students to trace a short input through $M$ and
    verify that both components advance correctly.  Highlight how
    $\varepsilon$‑moves in $M_1$ translate into $\varepsilon$‑moves in
    $M$ that do not change the FA state.}
  \label{fr:6.2-15}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 16: Theorem 6.13 — Invariant and proof sketch
\begin{frame}[t]{Theorem 6.13 — Invariant and proof sketch}
  \begin{tblock}{Core idea}
    The correctness of the product construction rests on an invariant
    relating runs of $M_1$ and $M$.  Informally, $M$ reaches state
    $(p,q)$ with stack $\alpha$ after reading a prefix $y$ of the input
    precisely when $M_1$ reaches state $p$ with the same stack after
    reading $y$, and the FA $M_2$ reaches state $q$ after reading $y$.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Let $y$ and $z$ be strings with $y z$ the input.
      \item Prove by induction on the number of moves that
        $(q_1, y z, Z_0) \Rightarrow^n_{M_1} (p, z, \alpha)$ and
        $\delta_2^\ast(q_2, y) = q$ hold if and only if
        $((q_1,q_2), y z, Z_0) \Rightarrow^n_M ((p,q), z, \alpha)$.
      \item The base case is immediate; the induction step splits into
        cases depending on whether the last move is an input move or an
        $\varepsilon$‑move.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item For input moves, synchronise the PDA’s consumption of a
        symbol with the FA’s transition on the same symbol.
      \item For $\varepsilon$‑moves, update only the PDA component while
        leaving the FA state unchanged.
      \item Conclude that $M$ accepts exactly those strings accepted by
        both $M_1$ and $M_2$.
    \end{itemize}
  \end{tblock}
  \note{Sketch the induction argument informally.  The detailed proof
    appears in the textbook; students should focus on understanding the
    high‑level idea rather than replicating every line of the proof.}
  \label{fr:6.2-16}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 17: Nondeterminism and union versus intersection
\begin{frame}[t]{Nondeterminism: union versus intersection}
  \begin{tblock}{Core idea}
    Nondeterminism allows a PDA to recognise the union of two CFLs by
    guessing which component to simulate.  By contrast, recognising the
    intersection of two CFLs requires simulating both components
    simultaneously, which is beyond the capability of a single stack.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item To accept $L_1 \cup L_2$, a PDA can nondeterministically
        choose to simulate either $M_1$ or $M_2$ at the start.
      \item To accept $L_1 \cap L_2$, a PDA would need to ensure that
        \emph{both} $M_1$ and $M_2$ accept; this requires two stacks or
        an equivalent mechanism.
      \item This explains why union is easy but intersection is hard.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item The union construction uses an initial $\varepsilon$‑move
        into either PDA; acceptance is inherited from the chosen PDA.
      \item The absence of closure under intersection follows from the
        inability to enforce acceptance by both PDAs without simulating
        two stacks.
    \end{itemize}
  \end{tblock}
  \note{This frame ties together Example 6.10 and the product
    construction.  Students should appreciate the role of nondeterminism in
    closure properties.}
  \label{fr:6.2-17}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 18: Deterministic PDAs and complementation
\begin{frame}[t]{Deterministic PDAs and complementation}
  \begin{tblock}{Core idea}
    Deterministic pushdown automata (DPDAs) behave differently under
    complementation than general PDAs.  For a DPDA with no
    $\varepsilon$‑transitions, swapping the accepting and nonaccepting
    states yields a DPDA that accepts the complement language.  More
    generally, every language accepted by a DPDA has a DPDA that
    accepts its complement.  However, not every CFL is accepted by a
    DPDA.
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item If a DPDA has no $\varepsilon$‑moves, acceptance depends
        solely on whether it ends in an accepting state after reading
        the input.  Reversing accepting and rejecting states therefore
        complements the language.
      \item Even with $\varepsilon$‑moves, one can construct a DPDA
        accepting the complement of a language recognised by a DPDA;
        this requires more care to avoid infinite $\varepsilon$‑loops.
      \item If $L$ is a CFL such that $\overline{L}$ is not a CFL,
        then $L$ cannot be accepted by a DPDA.  Palindromes provide an
        example where both $L$ and $\overline{L}$ are CFLs but $L$ is
        not deterministic (Theorem 5.1).
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    Do not assume that complementation works for arbitrary PDAs.  The
    DPDA property (determinism and appropriately restricted
    $\varepsilon$‑moves) is essential.  Moreover, a language may be
    context‑free without being deterministic context‑free.
  \end{talert}
  \note{Link this discussion back to Example 6.11: $XX$ is not a CFL,
    but $\overline{XX}$ is.  This complement is accepted by a PDA but
    not necessarily by a DPDA.}
  \label{fr:6.2-18}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 19: Practical insights for intersection and complement
\begin{frame}[t]{Practical insights for intersections and complements}
  \begin{tblock}{Core idea}
    When analysing whether a language is context‑free, decompose it
    strategically.  If the language involves multiple independent
    constraints (such as matching three counts), try to express it as an
    intersection and then demonstrate that each component is context‑free.
    Conversely, sometimes it is easier to show that a language is a CFL
    by writing it as a union of complements of simpler languages.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Identify independent counting or ordering constraints and
        separate them into individual languages.
      \item Use PDAs or CFGs to show each component is context‑free.
      \item Apply the pumping lemma or closure arguments to reason about
        the intersection or complement.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Use Theorem 6.13 when intersecting a CFL with a regular
        filter; the result will still be a CFL.
      \item Use De Morgan’s laws to relate intersections and unions of
        complements when exploring non‑closure results.
    \end{itemize}
  \end{tblock}
  \note{This slide emphasises strategy rather than theory.  Encourage
    students to practise decomposing languages in homework exercises.}
  \label{fr:6.2-19}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 20: Exercise – Identify closure properties
\begin{frame}[t]{Exercise: Identify closure properties}
  \begin{tblock}{Core idea}
    Determine whether the following operations preserve being a
    context‑free language.  Justify your answers informally.
  \end{tblock}
  \begin{tblock}{Exercises}
    \begin{enumerate}
      \item Union: If $L_1$ and $L_2$ are CFLs, is $L_1 \cup L_2$ a CFL?
      \item Concatenation and Kleene $\ast$: Are CFLs closed under
        concatenation and star?  Explain why.
      \item Intersection: Give a pair of CFLs whose intersection is not
        a CFL (beyond $a^n b^n c^n$).
      \item Complement: Find a language $L$ that is not context‑free but
        whose complement \emph{is} context‑free.
      \item Difference: If $L_1$ and $L_2$ are CFLs, is $L_1 - L_2$
        necessarily a CFL?  Provide a counterexample or proof.
      \item Intersection with a regular language: Explain why
        $L_1 \cap R$ is a CFL when $L_1$ is a CFL and $R$ is regular.
    \end{enumerate}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    Be careful to justify each answer.  For operations that do not
    preserve context‑freeness, provide an explicit counterexample or
    reference to an earlier example from the text.
  \end{talert}
  \note{Encourage students to use the pumping lemma and closure
    constructions learned so far to answer these questions.}
  \label{fr:6.2-20}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 21: Exercise – Construct an FA×PDA machine
\begin{frame}[t]{Exercise: Construct an FA×PDA}
  \begin{tblock}{Core idea}
    Practise the construction of Theorem 6.13.  Let $L_1$ be the set of
    balanced parentheses over $\{(,)\}$ and let $L_2$ be the regular
    language of all strings ending with the substring $()$.  Design a
    PDA $M_1$ for $L_1$, an FA $M_2$ for $L_2$, and then form their
    product PDA $M$ recognising $L_1 \cap L_2$.
  \end{tblock}
  \begin{tblock}{Exercises}
    \begin{enumerate}
      \item Draw or describe a PDA that recognises balanced parentheses.
      \item Draw or describe a finite automaton that recognises strings
        ending in $()$.
      \item Using the product construction, define the states and
        transitions of a PDA that recognises strings of balanced
        parentheses that end in $()$.
      \item Explain why your machine uses only one stack.
    \end{enumerate}
  \end{tblock}
  \note{This exercise reinforces the product construction and shows how
    to intersect a CFL with a simple regular filter.}
  \label{fr:6.2-21}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 22: Exercise – Difference of CFLs
\begin{frame}[t]{Exercise: Provide a difference counterexample}
  \begin{tblock}{Core idea}
    Provide two context‑free languages $L_1$ and $L_2$ such that the
    difference $L_1 - L_2$ is \emph{not} context‑free.  Outline a proof
    using the pumping lemma for CFLs.
  \end{tblock}
  \begin{tblock}{Exercises}
    \begin{enumerate}
      \item Let $L_1 = \{a^n b^n c^m \mid n,m \ge 0\}$ and $L_2 = \{a^n b^m
        c^m \mid n,m \ge 0\}$.  Show that both $L_1$ and $L_2$ are
        context‑free.
      \item Show that $L_1 - L_2 = \{a^n b^n c^m \mid n \ne m\}$ is not a
        context‑free language by applying the pumping lemma.
      \item Conclude that the class of CFLs is not closed under
        difference.
    \end{enumerate}
  \end{tblock}
  \note{This exercise parallels Example 6.10 but focuses on set
    difference rather than intersection.}
  \label{fr:6.2-22}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 23: Exercise – Spot the pitfall
\begin{frame}[t]{Exercise: Spot the pitfall}
  \begin{tblock}{Core idea}
    Identify and correct the following erroneous reasoning: “Since the
    complement of the non‑context‑free language $XX$ is context‑free,
    the complement of any non‑context‑free language must be context‑free.”
  \end{tblock}
  \begin{tblock}{Exercises}
    \begin{enumerate}
      \item Explain why the statement above does not follow.
      \item Provide a non‑context‑free language whose complement is also
        non‑context‑free.
      \item Comment on the role of deterministic PDAs in complementation.
    \end{enumerate}
  \end{tblock}
  \note{Encourage students to recall Example 6.3 and the discussion on
    deterministic context‑free languages.  Not all complementation
    arguments go through.}
  \label{fr:6.2-23}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 24: Schematic of the FA×PDA product
\begin{frame}[t]{Schematic of FA×PDA product}
  \begin{tblock}{Core idea}
    A picture helps to cement the idea of the product construction.
    Below is a high‑level schematic showing how the PDA component and
    the FA component cooperate on each input symbol.
  \end{tblock}
  \begin{center}
    \includegraphics[width=.7\linewidth]{sec6-2-schematic.png} % TODO: screenshot or sketch
  \end{center}
  \note{Replace this placeholder with a schematic of two automata
    feeding into a combined state $(p,q)$ and sharing one stack.  Use
    arrows to indicate how input symbols are processed and how the FA
    state updates in tandem with the PDA state.}
  \label{fr:6.2-24}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 25: Section 6.2 summary
\begin{frame}[t]{Summary of Section 6.2}
  \begin{tblock}{Core idea}
    Intersections and complements of context‑free languages behave
    differently from those of regular languages.  While CFLs are closed
    under union, concatenation, and star, they are not closed under
    intersection or set difference.  Strategic decompositions and
    complement operations reveal both positive and negative closure
    phenomena.
  \end{tblock}
  \begin{tblock}{Roadmap recap}
    \begin{enumerate}
      \item Expressing $a^n b^n c^n$ as an intersection of CFLs shows
        non‑closure under intersection (Example 6.10).
      \item The complement of a non‑CFL can be a CFL (Example 6.11).
      \item Intersections of inequalities illustrate further examples of
        non‑closure (Example 6.12).
      \item The product construction shows that intersecting a CFL with
        a regular language is always a CFL (Theorem 6.13).
      \item Nondeterminism explains why union is easy and intersection
        hard for PDAs.
    \end{enumerate}
  \end{tblock}
  \note{Conclude by encouraging students to try the exercises and to
    recognise when to use the pumping lemma versus closure properties.}
  \label{fr:6.2-25}
\end{frame}

% -----------------------------------------------------------------------------
% Section 6.3 – Decision Problems Involving Context‑Free Languages
%
% This section surveys basic decision problems for context‑free languages,
% including membership, emptiness, infiniteness, intersection non‑emptiness,
% and language inclusion.  It outlines algorithms (CYK, Earley) where they
% exist and notes undecidability results.  One frame per paragraph,
% following the Wildcat style.

\section{Decision Problems Involving Context‑Free Languages}

%-----------------------------------------------------------------------------
% Frame 1: Introduction to decision problems
\begin{frame}[t]{Decision problems for CFLs}
  \begin{tblock}{Core idea}
    Because context‑free grammars and pushdown automata are equivalent,
    decision problems for CFLs can be formulated in either representation.
    Standard algorithms convert CFGs to PDAs and vice versa, so we can
    choose the representation that makes a given problem more tractable.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Review the basic decision problems: membership, emptiness,
        infiniteness, intersection non‑emptiness, and inclusion.
      \item Discuss algorithms for the decidable problems and their
        limitations.
      \item Briefly indicate which problems are undecidable.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Conversions between CFGs and PDAs are effective; thus
        membership and emptiness problems can be posed using either.
      \item Some problems have polynomial‑time algorithms (CYK,
        Earley), while others are undecidable.
    \end{itemize}
  \end{tblock}
  \note{Orient the class: we will now leave closure properties and
    explore algorithmic questions about CFLs.  Underline that not every
    natural question admits a decidable answer.}
  \label{fr:6.3-01}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 2: Defining the membership problem
\begin{frame}[t]{Membership problem definition}
  \begin{tblock}{Core idea}
    The membership problem for context‑free languages asks: given a
    context‑free grammar $G$ and a string $x$, does $G$ derive $x$?
    Equivalently, given a PDA $M$ and a string $x$, does $M$ accept $x$?
    This is the analogue of testing whether a finite automaton accepts a
    string.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Formally state the problem: determine whether $x \in L(G)$.
      \item Note special cases such as $x = \varepsilon$ (nullability of
        the start symbol).
      \item Set up algorithms to answer this question for arbitrary
        grammars.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item When $x = \varepsilon$, membership reduces to checking
        whether the start variable can derive the empty string.
      \item When $|x| > 0$, we may first eliminate $\varepsilon$‑rules
        and unit productions before applying a membership test.
    \end{itemize}
  \end{tblock}
  \note{Stress that membership is a “first question” one asks about
    programming‑language grammars: does a given program conform to the
    syntax?}
  \label{fr:6.3-02}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 3: Naïve membership via exhaustive derivations
\begin{frame}[t]{Naïve membership via exhaustive derivations}
  \begin{tblock}{Core idea}
    A simple but impractical method to decide membership is to try all
    possible derivations in the grammar and see if any derive the
    target string.  When the grammar has been cleaned of
    $\varepsilon$‑productions and unit productions, any derivation of a
    string of length $n$ in Chomsky normal form has exactly $2n-1$ steps.
    Therefore one could, in principle, enumerate all such derivations and
    check whether one yields $x$.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Eliminate $\varepsilon$ and unit productions to obtain a
        CNF grammar $G_1$ equivalent to $G$ on non‑empty strings.
      \item Enumerate all derivations in $G_1$ with at most $2|x|-1$
        steps.
      \item Accept if any derivation produces $x$.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item In CNF, every derivation tree of a string of length $n$ has
        $2n-1$ internal nodes, so the number of steps is bounded.
      \item This yields a finite but astronomically large search space.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    This algorithm is of theoretical interest only: it is exponential in
    the size of $x$ and not suitable for parsing.  Real parsers use
    dynamic programming or Earley‑like techniques.
  \end{talert}
  \note{Mention that enumerating derivations corresponds to brute‑force
    backtracking.  It is ineffective in practice but illustrates that
    membership is decidable.}
  \label{fr:6.3-03}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 4: Efficient membership — CYK algorithm
\begin{frame}[t]{Efficient membership — CYK algorithm}
  \begin{tblock}{Core idea}
    The Cocke–Younger–Kasami (CYK) algorithm is a dynamic programming
    method for deciding membership.  By converting the grammar to
    Chomsky normal form and filling a triangular table, we can check
    whether the start variable derives the entire string in cubic time.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Convert the grammar to CNF.
      \item Create an $n\times n$ table, where $n = |x|$.
      \item For substrings of length 1, mark variables that derive the
        corresponding terminal.
      \item For each length $\ell = 2$ to $n$, fill table entries
        $T[i,i+\ell-1]$ by checking all splits $k$ and productions
        $A \rightarrow BC$ with $B \in T[i,i+k-1]$ and $C \in
        T[i+k,i+\ell-1]$.
      \item Accept if the start variable appears in $T[1,n]$.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item The algorithm runs in $O(n^3 |G|)$ time, where $|G|$ is the
        size of the grammar.
      \item It requires the grammar to be in CNF; preprocessing steps
        remove $\varepsilon$ and unit productions.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    Ensure that the grammar is in CNF; otherwise the algorithm must be
    adapted.  The CYK table only determines membership, not the
    structure of the parse tree, although with backpointers one can
    recover a parse.
  \end{talert}
  \note{Draw a small CYK table on the board for a short string.  Point
    out that the algorithm systematically explores all ways of splitting
    substrings.}
  \label{fr:6.3-04}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 5: Earley’s algorithm — high level
\begin{frame}[t]{Earley’s algorithm — high level}
  \begin{tblock}{Core idea}
    Earley’s algorithm generalises CYK to arbitrary context‑free
    grammars (not just CNF) and is particularly efficient for
    unambiguous or nearly deterministic grammars.  It incrementally
    builds sets of “dotted items” representing partial recognitions of
    productions, using three operations: prediction, scanning, and
    completion.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item For each input position $i$, maintain a set $S_i$ of items
        $[A \rightarrow \alpha \cdot \beta, j]$ meaning “from position
        $j$, we have recognised $\alpha$ and expect $\beta$ next.”
      \item \textbf{Prediction}: if the dot is before a nonterminal $B$,
        add items for each production of $B$.
      \item \textbf{Scanning}: if the dot is before a terminal $\sigma$
        matching the next input symbol, advance the dot.
      \item \textbf{Completion}: when an item reaches the end of its
        production, use it to advance dots in items that predicted this
        nonterminal.
      \item Accept if an item $[S \rightarrow \alpha \cdot,0]$ exists in
        $S_n$ after reading the entire input.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Earley’s algorithm runs in $O(n^3)$ time in the worst case
        but often performs better for unambiguous grammars.
      \item It does not require the grammar to be in CNF.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    The algorithm maintains sets of items; careful implementation is
    required to avoid redundant work.  Unlike CYK, Earley can handle
    nullable productions directly.
  \end{talert}
  \note{For those interested in compiler design, note that Earley’s
    algorithm underlies some general‑purpose parsers.  Students need
    only grasp the high‑level mechanics here.}
  \label{fr:6.3-05}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 6: Emptiness problem for CFLs
\begin{frame}[t]{Emptiness problem for CFLs}
  \begin{tblock}{Core idea}
    The emptiness problem asks whether a given context‑free grammar
    generates any string at all.  The answer is determined by
    identifying productive and reachable variables.  A grammar’s
    language is nonempty iff there is a derivation tree whose leaves are
    terminals and whose internal nodes are reachable from the start
    symbol.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Mark variables that produce a terminal string (productive
        variables).
      \item Remove productions that involve unproductive variables.
      \item Mark variables reachable from the start symbol.
      \item The language is nonempty if a productive reachable variable
        derives a terminal string.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Compute the set of productive variables by repeatedly
        marking variables $A$ for which there is a production
        $A \rightarrow \alpha$ where all symbols of $\alpha$ are either
        terminals or already marked variables.
      \item Compute the set of reachable variables by traversing the
        dependency graph from $S$.
      \item If no productive reachable variable remains, the language is
        empty; otherwise it is nonempty.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    Handle $\varepsilon$‑productions carefully; variables deriving the
    empty string are productive.  Remove useless variables before
    deciding emptiness to avoid false negatives.
  \end{talert}
  \note{Relate this procedure to earlier discussions of simplifying
    grammars and eliminating useless symbols (Section 4.4).}
  \label{fr:6.3-06}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 7: Infiniteness problem for CFLs
\begin{frame}[t]{Infiniteness problem for CFLs}
  \begin{tblock}{Core idea}
    A context‑free language is infinite iff its grammar contains a cycle
    through productive variables that can increase the length of derived
    strings.  Detecting infiniteness reduces to finding such cycles in
    the dependency graph.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Simplify the grammar by removing useless productions and
        converting to Chomsky normal form.
      \item Build a directed graph whose nodes are variables and where
        $A \to B$ if $A$ has a production that includes $B$.
      \item Identify cycles among productive variables that can
        generate at least one terminal symbol on each pass.
      \item If such a cycle is reachable from the start variable and
        leads to increasing string lengths, the language is infinite.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Not every cycle implies infiniteness: the cycle must allow
        the derivation of longer terminal strings (e.g., a self‑embedding
        production).
      \item If no such cycle exists, the language is finite, and one
        can enumerate all possible derivations of bounded length.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    Beware of cycles that generate only the empty string or that
    recycle variables without producing terminals; these do not witness
    infiniteness.  Ensure the cycle is both productive and length‑increasing.
  \end{talert}
  \note{Link this discussion back to the pumping lemma: infinite CFLs
    exhibit self‑embedding patterns that allow pumping.}
  \label{fr:6.3-07}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 8: Intersection non‑emptiness and inclusion problems
\begin{frame}[t]{Intersection non‑emptiness and inclusion}
  \begin{tblock}{Core idea}
    Two more natural questions are: (a) given CFGs $G_1$ and $G_2$, is
    $L(G_1) \cap L(G_2) \ne \emptyset$?  (b) given CFGs $G_1$ and $G_2$,
    does $L(G_1)$ lie within $L(G_2)$?  For regular languages these
    questions are decidable using closure properties and emptiness tests.
    For context‑free languages, however, general algorithms do not exist.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Describe the problems formally.
      \item Explain why the finite‑automaton construction (cross
        product) fails for PDAs.
      \item Indicate that there is no context‑free grammar that
        recognises the intersection or difference in general.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item For intersection non‑emptiness: there is no algorithm that
        always determines whether two CFGs share a common string (this
        problem is undecidable).
      \item For inclusion: there is no algorithm that always decides
        whether $L(G_1) \subseteq L(G_2)$ (also undecidable).
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    Do not attempt to construct a product PDA of two arbitrary PDAs; as
    seen in Section 6.2, one stack cannot simulate two.  Understanding
    these undecidability results requires machinery beyond this course,
    but recognising the limitations is important.
  \end{talert}
  \note{Forewarn students that not all natural questions about CFLs
    have algorithmic solutions.  Later courses in computability will
    explore undecidability proofs in detail.}
  \label{fr:6.3-08}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 9: Contrast with regular languages
\begin{frame}[t]{Contrast with regular languages}
  \begin{tblock}{Core idea}
    For regular languages, we have algorithms for membership,
    emptiness, finiteness, intersection non‑emptiness, and inclusion
    because finite automata are closed under all Boolean operations and
    have simple decision procedures.  For context‑free languages, only
    membership, emptiness, and infiniteness remain decidable in general.
  \end{tblock}
  \begin{tblock}{Roadmap}
    \begin{enumerate}
      \item Recall that the cross‑product of two FAs solves intersection
        and difference problems.
      \item Highlight that CFLs are not closed under intersection or
        complement, complicating analogous algorithms.
      \item Summarise which problems transfer from the regular to the
        context‑free setting and which do not.
    \end{enumerate}
  \end{tblock}
  \begin{tblock}{Key formal steps}
    \begin{itemize}
      \item Membership: decidable for regular languages via DFA
        simulation; for CFLs via CYK or Earley.
      \item Emptiness: decidable for both classes by eliminating
        unproductive states or variables.
      \item Infiniteness: decidable for both by detecting cycles.
      \item Intersection non‑emptiness/inclusion: decidable for regular
        languages, undecidable for CFLs.
    \end{itemize}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    Avoid applying FA‑style constructions blindly to PDAs.  Always
    verify whether the closure property you rely on actually holds for
    CFLs.
  \end{talert}
  \note{This slide summarises the boundary between regular and
    context‑free decision problems.  It helps motivate why more
    expressive formalisms can have undecidable properties.}
  \label{fr:6.3-09}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 10: Practical guidance for decision problems
\begin{frame}[t]{Practical guidance for decision problems}
  \begin{tblock}{Core idea}
    Different decision problems call for different techniques.  The
    following checklist summarises when to use each method:
  \end{tblock}
  \begin{tblock}{Checklist}
    \begin{itemize}
      \item \textbf{Membership:} use CYK or Earley; convert the grammar
        to CNF for CYK and build a dynamic‑programming table.
      \item \textbf{Nonemptiness:} mark productive and reachable
        variables; the language is nonempty iff a productive variable is
        reachable from $S$.
      \item \textbf{Infiniteness:} detect length‑increasing cycles on
        productive variables.
      \item \textbf{Intersecting with a regular filter:} apply
        Theorem 6.13 to form a product PDA with the regular FA.
      \item \textbf{General intersection/inclusion:} recognise that no
        general algorithm exists; look for other techniques (e.g., pumps
        or reductions) to argue non‑membership or provide counterexamples.
    \end{itemize}
  \end{tblock}
  \note{Encourage students to choose the right tool for the problem at
    hand.  Not every problem requires building a full parser; sometimes
    simple marking suffices.}
  \label{fr:6.3-10}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 11: Recap — Decidable and undecidable problems
\begin{frame}[t]{Recap: Decidable vs. undecidable problems}
  \begin{tfacetbox}{Recap}
    \begin{itemize}
      \item \textbf{Decidable} for CFLs: membership (via CYK/Earley),
        emptiness (productive and reachable variables), infiniteness
        (length‑increasing cycles), and intersection with a regular
        language (via product PDA).
      \item \textbf{Undecidable} for CFLs: intersection non‑emptiness
        ($L(G_1) \cap L(G_2) \ne \emptyset$ in general), language
        inclusion ($L(G_1) \subseteq L(G_2)$), and equivalence.
      \item These undecidability results demonstrate the limits of what
        we can algorithmically determine about arbitrary context‑free
        languages.
    \end{itemize}
  \end{tfacetbox}
  \note{Use this slide as a concise summary.  Stress that proof of
    undecidability is beyond this chapter, but knowledge of which
    problems are undecidable helps avoid futile attempts to solve them.}
  \label{fr:6.3-11}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 12: Example — Membership check
\begin{frame}[t]{Example: Membership check with CYK}
  \begin{texample}{Checking $a b c$ in a simple grammar}
    Consider the grammar with productions $S \rightarrow AB \mid BC$, $A
    \rightarrow aA \mid a$, $B \rightarrow b$, and $C \rightarrow c$.  Does
    this grammar generate the string $abc$?  Convert the grammar to
    CNF and apply the CYK algorithm:
    \begin{itemize}
      \item In CNF, the productions become $S \rightarrow AB \mid BC$, $A
        \rightarrow a$, $B \rightarrow b$, $C \rightarrow c$ (since the
        original $A \rightarrow aA$ production is eliminated by standard
        procedures).
      \item The CYK table $T[1,3]$ records which variables derive each
        substring: $T[1,1]=\{A\}$ for $a$, $T[2,2]=\{B\}$ for $b$,
        $T[3,3]=\{C\}$ for $c$.
      \item For substrings of length 2, $T[1,2]$ contains no
        variables since no production has right side $A B$ or $A C$.
        $T[2,3]$ contains no variables since $B C$ is not a production.
      \item For the full string, $T[1,3]$ contains $S$ because
        $S \rightarrow B C$ and $B \in T[2,2]$, $C \in T[3,3]$.
    \end{itemize}
    Hence $abc \in L(S)$.
  \end{texample}
  \note{Work through the CYK table carefully.  This example shows how
    the algorithm builds up possible variables and concludes membership.}
  \label{fr:6.3-12}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 13: Example — Detecting infiniteness
\begin{frame}[t]{Example: Detecting infiniteness via cycles}
  \begin{texample}{A grammar with a productive cycle}
    Let $G$ have productions $S \rightarrow A C \mid a$, $A \rightarrow a A
    \mid a$, and $C \rightarrow c$.  The variable $A$ has a production
    $A \rightarrow aA$ that creates a self‑embedding cycle.  This cycle
    is productive: each iteration of the cycle appends an $a$ to the
    string, yielding arbitrarily long $a$‑blocks.  Since $S$ derives
    $a^n c$ for all $n \ge 1$, $L(G)$ is infinite.
  \end{texample}
  \begin{talert}{Pitfalls / Gotchas}
    Compare with a grammar where $A \rightarrow \varepsilon A$; such a
    cycle does not add length and therefore does not imply infiniteness.
  \end{talert}
  \note{Encourage students to draw the dependency graph and locate
    cycles.  Emphasise the distinction between cycles that add symbols
    and those that merely rearrange nonterminals.}
  \label{fr:6.3-13}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 14: Spot the pitfall — Pumping lemma vs. membership
\begin{frame}[t]{Spot the pitfall: Pumping lemma vs. membership}
  \begin{tblock}{Core idea}
    The pumping lemma for context‑free languages is a “negative tool”:
    it proves that certain languages are not context‑free by showing
    that they violate the pumping property.  It is “not” a membership
    algorithm.  One cannot feed a particular string into the pumping
    lemma to determine whether it is generated by a given grammar.
  \end{tblock}
  \begin{tblock}{Exercises}
    \begin{enumerate}
      \item Explain why the pumping lemma cannot be used to decide
        whether a specific string belongs to a given CFL.
      \item Give an example where a string belongs to a context‑free
        language but fails a naive pumping test for some decomposition.
      \item Describe the proper use of the pumping lemma (to show
        non‑context‑freeness) and contrast it with membership
        algorithms.
    \end{enumerate}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    Students sometimes attempt to “apply” the pumping lemma to a
    particular string by choosing a convenient decomposition.  The
    quantifiers in the lemma require that \emph{all} long enough
    strings and \emph{all} valid decompositions pump.  Failure to
    appreciate this leads to incorrect reasoning.
  \end{talert}
  \note{Reinforce the idea that membership is a decidable problem with
    positive algorithms, whereas the pumping lemma is a nonconstructive
    tool used for negative results.}
  \label{fr:6.3-14}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 15: Exercise – Decision problem practice
\begin{frame}[t]{Exercise: Decision problem practice}
  \begin{tblock}{Core idea}
    Test your understanding of the decision problems discussed.  For
    each item below, state whether the problem is decidable and
    briefly justify your answer.
  \end{tblock}
  \begin{tblock}{Exercises}
    \begin{enumerate}
      \item Given a CFG $G$ and a string $x$, decide whether $x \in
        L(G)$.
      \item Given a CFG $G$, decide whether $L(G) = \emptyset$.
      \item Given a CFG $G$, decide whether $L(G)$ is infinite.
      \item Given CFGs $G_1$ and $G_2$, decide whether $L(G_1) \cap
        L(G_2) \ne \emptyset$.
      \item Given CFGs $G_1$ and $G_2$, decide whether $L(G_1)
        \subseteq L(G_2)$.
      \item Given a CFG $G$ and a regular language $R$, decide whether
        $L(G) \cap R = \emptyset$.
    \end{enumerate}
  \end{tblock}
  \begin{talert}{Pitfalls / Gotchas}
    Remember that decidability results often depend on closure
    properties.  For the intersection problem in item (4), refer back
    to Section 6.2 to understand why no general algorithm exists.
  \end{talert}
  \note{Use this exercise to summarise which problems are within reach
    of algorithms and which are not.  Encourage students to cite the
    appropriate theorems.}
  \label{fr:6.3-15}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 16: CYK table schematic
\begin{frame}[t]{CYK table schematic}
  \begin{tblock}{Core idea}
    A visual representation of the CYK dynamic programming table helps
    to solidify understanding.  Each cell corresponds to a substring
    of the input and contains the set of variables that can derive
    that substring.
  \end{tblock}
  \begin{center}
    \includegraphics[width=.7\linewidth]{sec6-3-cyk-skeleton.png} % TODO: minimal table schematic
  \end{center}
  \note{Replace this placeholder with a triangular table labelled with
    positions 1 to $n$ along the top and annotate a few cells with
    variables.  Highlight how substrings of increasing length build upon
    shorter substrings.}
  \label{fr:6.3-16}
\end{frame}

%-----------------------------------------------------------------------------
% Frame 17: Section 6.3 summary
\begin{frame}[t]{Summary of Section 6.3}
  \begin{tblock}{Core idea}
    Decision problems for context‑free languages cover membership,
    emptiness, infiniteness, and beyond.  Efficient algorithms exist
    for membership (CYK, Earley) and for emptiness and infiniteness via
    grammar analysis.  Intersection and inclusion problems, however,
    illustrate the limitations of context‑free languages: they are
    undecidable in general.
  \end{tblock}
  \begin{tblock}{Roadmap recap}
    \begin{enumerate}
      \item Membership: CYK and Earley offer polynomial‑time parsing.
      \item Nonemptiness: productive and reachable variables detect
        whether any string is generated.
      \item Infiniteness: length‑increasing cycles reveal unbounded
        growth.
      \item Intersection with regular languages: decidable via
        product PDA.
      \item Intersection/inclusion for general CFLs: undecidable.
    \end{enumerate}
  \end{tblock}
  \note{Encourage students to synthesise the results of Sections 6.2
    and 6.3.  Understanding these decision problems lays the groundwork
    for more advanced study in formal language theory and compiler
    construction.}
  \label{fr:6.3-17}
\end{frame}